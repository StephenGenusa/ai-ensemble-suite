{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>\"# AI Ensemble Suite\\n\\nWelcome to the documentation for AI Ensemble Suite.\" </p>"},{"location":"llama-cpp-python-thread-safety-from-Sonnet37/","title":"Thread Safety of llama-cpp-python","text":"<p>llama-cpp-python is generally not fully thread-safe in its core operations. Here's what you should know:</p>"},{"location":"llama-cpp-python-thread-safety-from-Sonnet37/#thread-safety-considerations","title":"Thread Safety Considerations","text":"<ol> <li> <p>Core model operations: The main inference operations are not designed to be called concurrently from multiple threads on the same model instance.</p> </li> <li> <p>Model instances: Different model instances can be used in separate threads, but a single model instance should not be shared across threads for concurrent inference.</p> </li> <li> <p>Batch processing: The library does have some internal threading capabilities for batch processing, but this is controlled within the library itself.</p> </li> <li> <p>Global state: Some components of the underlying llama.cpp maintain global state that can cause issues in multi-threaded environments.</p> </li> </ol>"},{"location":"llama-cpp-python-thread-safety-from-Sonnet37/#safe-usage-patterns","title":"Safe Usage Patterns","text":"<ul> <li>Create separate model instances for separate threads</li> <li>Use locks/mutexes if you must share a model instance across threads</li> <li>Consider using a queue-based architecture where a dedicated thread handles all model interactions</li> </ul> <p>If you need concurrent processing, it's generally better to use a process-based approach (like multiple workers) rather than attempting to share the model across threads.</p>"},{"location":"api/aggregation/","title":"Aggregation API","text":"<p>API reference for aggregation methods. </p> <p>Aggregation strategies for ai-ensemble-suite.</p>"},{"location":"api/aggregation/#ai_ensemble_suite.aggregation.AdaptiveSelection","title":"<code>AdaptiveSelection</code>","text":"<p>               Bases: <code>BaseAggregator</code></p> <p>Adaptive Selection aggregation strategy.</p> <p>Dynamically selects and executes another aggregation strategy based on analysis of the query, context, phase outputs, or an explicit selector model.</p> Source code in <code>src\\ai_ensemble_suite\\aggregation\\adaptive_selection.py</code> <pre><code>class AdaptiveSelection(BaseAggregator):\n    \"\"\"Adaptive Selection aggregation strategy.\n\n    Dynamically selects and executes another aggregation strategy based on\n    analysis of the query, context, phase outputs, or an explicit selector model.\n    \"\"\"\n\n    # Override __init__ to accept model_manager\n    def __init__(\n        self,\n        config_manager: \"ConfigManager\",\n        strategy_name: str, # Should be 'adaptive_selection'\n        model_manager: Optional[\"ModelManager\"] = None, # Required for selector model\n        strategy_config_override: Optional[Dict[str, Any]] = None\n    ) -&gt; None:\n        \"\"\"Initialize the AdaptiveSelection aggregator.\"\"\"\n        super().__init__(config_manager, strategy_name, model_manager, strategy_config_override)\n        if strategy_name != \"adaptive_selection\":\n             logger.warning(f\"AdaptiveSelection initialized with unexpected strategy name '{strategy_name}'.\")\n\n\n    async def aggregate(\n        self,\n        outputs: Dict[str, Dict[str, Any]],\n        context: Dict[str, Any],\n        trace_collector: Optional[TraceCollector] = None\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Select the most appropriate strategy and execute it.\n\n        Args:\n            outputs: Dictionary mapping phase names to their outputs.\n            context: Context information from collaboration phases (should include 'query').\n            trace_collector: Optional trace collector for gathering execution details.\n\n        Returns:\n            Dictionary containing the result from the executed sub-strategy, potentially\n            augmented with information about which strategy was selected.\n\n        Raises:\n            AggregationError: If selection or execution of the sub-strategy fails.\n        \"\"\"\n        start_time = time.time()\n        logger.debug(f\"Starting Adaptive Selection aggregation...\")\n\n        if not outputs:\n             raise AggregationError(\"No phase outputs provided for Adaptive Selection.\")\n\n        try:\n            # Get the configuration for available strategies FROM THIS strategy's config\n            # The 'strategies' key should contain definitions like { \"name\": { \"config_details\" } }\n            available_strategies_config = self._config.get(\"strategies\", {})\n\n            # Provide default strategies if none are defined in config\n            if not available_strategies_config or not isinstance(available_strategies_config, dict):\n                logger.warning(\"No valid 'strategies' defined in AdaptiveSelection config, using defaults.\")\n                available_strategies_config = {\n                    \"sequential_refinement\": {\n                        \"description\": \"Uses the output of the final phase in a sequence.\",\n                        \"conditions\": [\"sequential\", \"refinement\", \"conversation\", \"final\"],\n                        # Example config for this strategy if run directly\n                        \"final_phase\": context.get(\"phase_sequence\", [])[-1] if context.get(\"phase_sequence\") else None,\n                    },\n                    \"confidence_based\": {\n                        \"description\": \"Selects the output with the highest confidence score.\",\n                        \"conditions\": [\"factual\", \"objective\", \"specific\", \"high confidence\"],\n                        \"threshold\": 0.7, # Example config\n                    },\n                    \"weighted_voting\": {\n                        \"description\": \"Combines outputs based on pre-defined weights.\",\n                        \"conditions\": [\"opinion\", \"subjective\", \"creative\", \"multiple options\"],\n                        \"weights\": {}, # Example config (should ideally be populated)\n                    },\n                     \"multidimensional_voting\": {\n                        \"description\": \"Evaluates outputs on multiple dimensions and selects highest weighted score.\",\n                        \"conditions\": [\"complex query\", \"multi-faceted\", \"evaluation needed\"],\n                        \"dimensions\": [\"accuracy\", \"clarity\", \"completeness\"], # Example\n                        \"dimension_weights\": {}, # Example\n                    },\n                    \"ensemble_fusion\": {\n                         \"description\": \"Uses a model to fuse multiple outputs into one.\",\n                         \"conditions\": [\"integration\", \"synthesis\", \"combine perspectives\"],\n                         # Needs fusion_model specified here or via context\n                    }\n                }\n\n            # Get contextual info like query type, if available\n            query = context.get(\"query\", \"\")\n            query_type = context.get(\"query_type\", \"\") # Often determined earlier\n\n            # --- Select the best strategy ---\n            selected_strategy_name = await self._select_strategy(\n                outputs, context, available_strategies_config, query, query_type, trace_collector\n            )\n\n            logger.info(f\"Adaptive Selection chose strategy: '{selected_strategy_name}'\")\n\n            # Get the specific configuration for the selected strategy\n            # This config will be passed as an override to the selected aggregator instance\n            strategy_config = available_strategies_config.get(selected_strategy_name, {})\n            if not strategy_config:\n                 logger.warning(f\"No configuration found for selected strategy '{selected_strategy_name}', using empty config.\")\n                 strategy_config = {}\n\n             # Ensure the strategy name is part of its config for consistency\n            strategy_config['strategy'] = selected_strategy_name\n\n\n            # --- Instantiate and Execute the Selected Strategy ---\n            # Get the class corresponding to the selected strategy name\n            aggregator_class = STRATEGY_CLASS_MAP.get(selected_strategy_name)\n\n            if aggregator_class is None:\n                logger.error(f\"Cannot execute selected strategy '{selected_strategy_name}': Class not found in registry. Falling back to default.\")\n                aggregator_class = DEFAULT_STRATEGY_CLASS\n                selected_strategy_name = DEFAULT_STRATEGY_NAME\n                # Get default config? For now, use empty, BaseAggregator will load defaults.\n                strategy_config = {\"strategy\": selected_strategy_name}\n\n\n            # Create the aggregator instance, passing:\n            # - The *original* config_manager (for template access etc.)\n            # - The selected strategy name\n            # - The model_manager (needed by some strategies)\n            # - The *specific configuration* for the selected strategy as an override\n            try:\n                aggregator_instance = aggregator_class(\n                    config_manager=self._config_manager, # Original manager\n                    strategy_name=selected_strategy_name,\n                    model_manager=self._model_manager,    # Pass model manager\n                    strategy_config_override=strategy_config # Pass specific config\n                )\n            except Exception as e:\n                 logger.error(f\"Failed to instantiate aggregator for strategy '{selected_strategy_name}': {e}\", exc_info=True)\n                 raise AggregationError(f\"Failed to create instance for strategy '{selected_strategy_name}': {e}\")\n\n\n            # Execute the chosen strategy's aggregate method\n            result = await aggregator_instance.aggregate(outputs, context, trace_collector)\n\n            # Add adaptive selection information to the final result\n            result[\"selected_strategy\"] = selected_strategy_name\n\n\n            execution_time = time.time() - start_time\n            logger.info(\n                f\"Adaptive Selection completed using '{selected_strategy_name}' in {execution_time:.2f}s.\"\n            )\n\n            # Add trace for AdaptiveSelection itself, including the sub-result\n            if trace_collector:\n                trace_collector.add_aggregation_trace(\n                    # Name of THIS strategy\n                    strategy_name=self._strategy_name, # \"adaptive_selection\"\n                    inputs={\n                        \"phase_output_keys\": list(outputs.keys()),\n                        \"context_keys\": list(context.keys()),\n                        \"available_strategies\": list(available_strategies_config.keys()),\n                        \"query\": query[:100] + \"...\" if len(query) &gt; 100 else query,\n                        \"query_type\": query_type\n                    },\n                    # Output includes the result from the sub-strategy + selection info\n                    output={\n                        \"_selected_strategy\": selected_strategy_name,\n                        **result # Unpack the sub-strategy's result here\n                    },\n                    execution_time=execution_time,\n                    parameters={ # Parameters of THIS strategy\n                        # Store the config structure used for selection\n                        \"strategy_definitions\": available_strategies_config,\n                        \"selector_model\": self._config.get(\"selector_model\")\n                    }\n                )\n\n            return result\n\n        except AggregationError as e:\n             logger.error(f\"Adaptive Selection aggregation failed: {str(e)}\")\n             raise # Re-raise known aggregation errors\n        except Exception as e:\n            logger.error(f\"Unexpected error during Adaptive Selection aggregation: {str(e)}\", exc_info=True)\n            # Wrap unexpected errors\n            raise AggregationError(f\"Adaptive Selection aggregation failed unexpectedly: {str(e)}\")\n\n    async def _select_strategy(\n        self,\n        outputs: Dict[str, Dict[str, Any]],\n        context: Dict[str, Any],\n        available_strategies: Dict[str, Dict[str, Any]], # Configs for potential strategies\n        query: str,\n        query_type: str,\n        trace_collector: Optional[TraceCollector] = None\n    ) -&gt; str:\n        \"\"\"Select the most appropriate aggregation strategy based on various factors.\n\n        Args:\n            outputs: Outputs from collaboration phases.\n            context: Context dictionary.\n            available_strategies: Dictionary mapping strategy names to their configurations/metadata.\n            query: The original user query.\n            query_type: Pre-determined type/category of the query (if available).\n            trace_collector: Optional trace collector.\n\n        Returns:\n            The name (string) of the selected strategy. Falls back to a default if selection fails.\n        \"\"\"\n        logger.debug(\"Selecting aggregation strategy...\")\n\n        strategies_to_consider = list(available_strategies.keys())\n        if not strategies_to_consider:\n             logger.warning(\"No available strategies defined for selection. Falling back to default.\")\n             return DEFAULT_STRATEGY_NAME\n\n        # --- Selection Logic (Priority Order) ---\n\n        # 1. Check for explicit strategy preference in context\n        preferred_strategy = context.get(\"preferred_strategy\")\n        if preferred_strategy and isinstance(preferred_strategy, str) and preferred_strategy in strategies_to_consider:\n            logger.debug(f\"Using preferred strategy from context: '{preferred_strategy}'\")\n            return preferred_strategy\n\n        # 2. Use dedicated strategy selector model if configured\n        selector_model_id = self._config.get(\"selector_model\")\n        if selector_model_id:\n            if self._model_manager is None:\n                logger.warning(\"Selector model specified, but ModelManager not available. Cannot use selector model.\")\n            else:\n                logger.debug(f\"Using selector model '{selector_model_id}' to choose strategy.\")\n                try:\n                      # Format strategy options for the selector prompt\n                      strategy_options_prompt = \"\"\n                      for name, config in available_strategies.items():\n                           description = config.get(\"description\", f\"Strategy: {name}\")\n                           conditions = config.get(\"conditions\", [])\n                           conditions_str = \", \".join(conditions) if conditions else \"General purpose\"\n                           strategy_options_prompt += f\"- {name}: {description} (Best for: {conditions_str})\\n\"\n\n                      # Create the prompt for the selector model\n                      selector_prompt_template = self._config.get(\"selector_prompt_template\", \"adaptive_selector_default\")\n                      default_selector_prompt = f\"\"\"Given the user query and available aggregation strategies, select the *single best* strategy name.\n\nUSER QUERY:\n{{query}}\n\nAVAILABLE STRATEGIES:\n{{strategy_options}}\n\nAnalyze the query and choose the strategy name from the list above that is most suitable.\nRespond with ONLY the strategy name (e.g., 'confidence_based', 'ensemble_fusion').\"\"\"\n\n                      selector_prompt = \"\"\n                      try:\n                           context = {\"query\": query, \"strategy_options\": strategy_options_prompt}\n                           selector_prompt = self._config_manager.render_prompt(selector_prompt_template, context)\n                      except (ConfigurationError, KeyError):\n                            logger.warning(f\"Selector template '{selector_prompt_template}' failed, using default.\")\n                            selector_prompt = default_selector_prompt.format(query=query, strategy_options=strategy_options_prompt)\n\n\n                      # Run the selector model inference\n                      selector_result_raw = await self._model_manager.run_inference(\n                          model_id=selector_model_id,\n                          prompt=selector_prompt,\n                          temperature=0.1, # Low temp for deterministic selection\n                          max_tokens=50    # Expect short response (name only)\n                      )\n\n                      # Add trace for the selector model call\n                      if trace_collector:\n                           trace_collector.add_model_trace(\n                               model_id=selector_model_id,\n                               input_prompt=selector_prompt,\n                               output=selector_result_raw,\n                               execution_time=selector_result_raw.get(\"total_time\", 0),\n                               parameters={\"role\": \"strategy_selector\"}\n                           )\n\n                      # Extract the selected strategy name from the model's response\n                      selected_name_raw = selector_result_raw.get(\"text\", \"\").strip().lower()\n                      # Clean up potential extra text\n                      selected_name_clean = re.split(r'[\\s:,\\.\\-]+', selected_name_raw)[0]\n\n\n                      # Validate if the selected name is one of the available strategies\n                      if selected_name_clean in strategies_to_consider:\n                          logger.debug(f\"Selector model chose strategy: '{selected_name_clean}'\")\n                          return selected_name_clean\n                      else:\n                          logger.warning(f\"Selector model returned invalid strategy name: '{selected_name_raw}'. Ignoring.\")\n\n                except ModelError as e:\n                    logger.error(f\"Strategy selector model '{selector_model_id}' failed: {str(e)}\")\n                except Exception as e:\n                    logger.error(f\"Unexpected error using strategy selector model: {str(e)}\", exc_info=True)\n\n\n        # 3. Use query type matching based on 'conditions' in strategy config\n        if query_type:\n            logger.debug(f\"Trying strategy selection based on query_type: '{query_type}'\")\n            query_type_lower = query_type.lower()\n            for name, config in available_strategies.items():\n                 conditions = config.get(\"conditions\", [])\n                 if isinstance(conditions, list):\n                      for cond in conditions:\n                           if isinstance(cond, str) and cond.lower() in query_type_lower:\n                                logger.debug(f\"Matched query_type '{query_type}' to condition '{cond}' for strategy '{name}'\")\n                                return name\n\n        # 4. Check phase outputs for keywords matching 'conditions' (Less reliable)\n        # Combine text from all phase outputs\n        # all_output_text = \" \".join([self._extract_output(o_data) for o_data in outputs.values()])\n        # if len(all_output_text) &gt; 50: # Only if there's substantial text\n        #     all_output_text_lower = all_output_text.lower()\n        #     match_counts: Dict[str, int] = {}\n        #     for name, config in available_strategies.items():\n        #         conditions = config.get(\"conditions\", [])\n        #         count = 0\n        #         if isinstance(conditions, list):\n        #             for cond in conditions:\n        #                 if isinstance(cond, str) and cond.lower() in all_output_text_lower:\n        #                      count += 1\n        #         if count &gt; 0: match_counts[name] = count\n        #\n        #     if match_counts:\n        #         # Select strategy with the most condition matches in the text\n        #         best_match_strategy = max(match_counts, key=match_counts.get)\n        #         logger.debug(f\"Selected strategy '{best_match_strategy}' based on keyword matches in outputs \"\n        #                      f\"(Matches: {match_counts[best_match_strategy]})\")\n        #         return best_match_strategy\n\n\n        # 5. Fallback: Use weighted selection if weights are defined (less common for adaptive)\n        # Or simply fallback to the default strategy if no specific selection criteria met.\n\n        # --- Final Fallback ---\n        logger.debug(f\"No specific strategy selected based on criteria. Falling back to default: '{DEFAULT_STRATEGY_NAME}'\")\n        # Ensure the default is actually in the available list, otherwise pick the first available one\n        if DEFAULT_STRATEGY_NAME in strategies_to_consider:\n             return DEFAULT_STRATEGY_NAME\n        elif strategies_to_consider:\n             first_available = strategies_to_consider[0]\n             logger.warning(f\"Default strategy '{DEFAULT_STRATEGY_NAME}' not available, using first available: '{first_available}'\")\n             return first_available\n        else:\n             # This case should be caught earlier, but as a safety net:\n             raise AggregationError(\"Cannot select strategy: No available strategies defined and no default applicable.\")\n</code></pre>"},{"location":"api/aggregation/#ai_ensemble_suite.aggregation.AdaptiveSelection.__init__","title":"<code>__init__(config_manager, strategy_name, model_manager=None, strategy_config_override=None)</code>","text":"<p>Initialize the AdaptiveSelection aggregator.</p> Source code in <code>src\\ai_ensemble_suite\\aggregation\\adaptive_selection.py</code> <pre><code>def __init__(\n    self,\n    config_manager: \"ConfigManager\",\n    strategy_name: str, # Should be 'adaptive_selection'\n    model_manager: Optional[\"ModelManager\"] = None, # Required for selector model\n    strategy_config_override: Optional[Dict[str, Any]] = None\n) -&gt; None:\n    \"\"\"Initialize the AdaptiveSelection aggregator.\"\"\"\n    super().__init__(config_manager, strategy_name, model_manager, strategy_config_override)\n    if strategy_name != \"adaptive_selection\":\n         logger.warning(f\"AdaptiveSelection initialized with unexpected strategy name '{strategy_name}'.\")\n</code></pre>"},{"location":"api/aggregation/#ai_ensemble_suite.aggregation.AdaptiveSelection.aggregate","title":"<code>aggregate(outputs, context, trace_collector=None)</code>  <code>async</code>","text":"<p>Select the most appropriate strategy and execute it.</p> <p>Parameters:</p> Name Type Description Default <code>outputs</code> <code>Dict[str, Dict[str, Any]]</code> <p>Dictionary mapping phase names to their outputs.</p> required <code>context</code> <code>Dict[str, Any]</code> <p>Context information from collaboration phases (should include 'query').</p> required <code>trace_collector</code> <code>Optional[TraceCollector]</code> <p>Optional trace collector for gathering execution details.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary containing the result from the executed sub-strategy, potentially</p> <code>Dict[str, Any]</code> <p>augmented with information about which strategy was selected.</p> <p>Raises:</p> Type Description <code>AggregationError</code> <p>If selection or execution of the sub-strategy fails.</p> Source code in <code>src\\ai_ensemble_suite\\aggregation\\adaptive_selection.py</code> <pre><code>async def aggregate(\n    self,\n    outputs: Dict[str, Dict[str, Any]],\n    context: Dict[str, Any],\n    trace_collector: Optional[TraceCollector] = None\n) -&gt; Dict[str, Any]:\n    \"\"\"Select the most appropriate strategy and execute it.\n\n    Args:\n        outputs: Dictionary mapping phase names to their outputs.\n        context: Context information from collaboration phases (should include 'query').\n        trace_collector: Optional trace collector for gathering execution details.\n\n    Returns:\n        Dictionary containing the result from the executed sub-strategy, potentially\n        augmented with information about which strategy was selected.\n\n    Raises:\n        AggregationError: If selection or execution of the sub-strategy fails.\n    \"\"\"\n    start_time = time.time()\n    logger.debug(f\"Starting Adaptive Selection aggregation...\")\n\n    if not outputs:\n         raise AggregationError(\"No phase outputs provided for Adaptive Selection.\")\n\n    try:\n        # Get the configuration for available strategies FROM THIS strategy's config\n        # The 'strategies' key should contain definitions like { \"name\": { \"config_details\" } }\n        available_strategies_config = self._config.get(\"strategies\", {})\n\n        # Provide default strategies if none are defined in config\n        if not available_strategies_config or not isinstance(available_strategies_config, dict):\n            logger.warning(\"No valid 'strategies' defined in AdaptiveSelection config, using defaults.\")\n            available_strategies_config = {\n                \"sequential_refinement\": {\n                    \"description\": \"Uses the output of the final phase in a sequence.\",\n                    \"conditions\": [\"sequential\", \"refinement\", \"conversation\", \"final\"],\n                    # Example config for this strategy if run directly\n                    \"final_phase\": context.get(\"phase_sequence\", [])[-1] if context.get(\"phase_sequence\") else None,\n                },\n                \"confidence_based\": {\n                    \"description\": \"Selects the output with the highest confidence score.\",\n                    \"conditions\": [\"factual\", \"objective\", \"specific\", \"high confidence\"],\n                    \"threshold\": 0.7, # Example config\n                },\n                \"weighted_voting\": {\n                    \"description\": \"Combines outputs based on pre-defined weights.\",\n                    \"conditions\": [\"opinion\", \"subjective\", \"creative\", \"multiple options\"],\n                    \"weights\": {}, # Example config (should ideally be populated)\n                },\n                 \"multidimensional_voting\": {\n                    \"description\": \"Evaluates outputs on multiple dimensions and selects highest weighted score.\",\n                    \"conditions\": [\"complex query\", \"multi-faceted\", \"evaluation needed\"],\n                    \"dimensions\": [\"accuracy\", \"clarity\", \"completeness\"], # Example\n                    \"dimension_weights\": {}, # Example\n                },\n                \"ensemble_fusion\": {\n                     \"description\": \"Uses a model to fuse multiple outputs into one.\",\n                     \"conditions\": [\"integration\", \"synthesis\", \"combine perspectives\"],\n                     # Needs fusion_model specified here or via context\n                }\n            }\n\n        # Get contextual info like query type, if available\n        query = context.get(\"query\", \"\")\n        query_type = context.get(\"query_type\", \"\") # Often determined earlier\n\n        # --- Select the best strategy ---\n        selected_strategy_name = await self._select_strategy(\n            outputs, context, available_strategies_config, query, query_type, trace_collector\n        )\n\n        logger.info(f\"Adaptive Selection chose strategy: '{selected_strategy_name}'\")\n\n        # Get the specific configuration for the selected strategy\n        # This config will be passed as an override to the selected aggregator instance\n        strategy_config = available_strategies_config.get(selected_strategy_name, {})\n        if not strategy_config:\n             logger.warning(f\"No configuration found for selected strategy '{selected_strategy_name}', using empty config.\")\n             strategy_config = {}\n\n         # Ensure the strategy name is part of its config for consistency\n        strategy_config['strategy'] = selected_strategy_name\n\n\n        # --- Instantiate and Execute the Selected Strategy ---\n        # Get the class corresponding to the selected strategy name\n        aggregator_class = STRATEGY_CLASS_MAP.get(selected_strategy_name)\n\n        if aggregator_class is None:\n            logger.error(f\"Cannot execute selected strategy '{selected_strategy_name}': Class not found in registry. Falling back to default.\")\n            aggregator_class = DEFAULT_STRATEGY_CLASS\n            selected_strategy_name = DEFAULT_STRATEGY_NAME\n            # Get default config? For now, use empty, BaseAggregator will load defaults.\n            strategy_config = {\"strategy\": selected_strategy_name}\n\n\n        # Create the aggregator instance, passing:\n        # - The *original* config_manager (for template access etc.)\n        # - The selected strategy name\n        # - The model_manager (needed by some strategies)\n        # - The *specific configuration* for the selected strategy as an override\n        try:\n            aggregator_instance = aggregator_class(\n                config_manager=self._config_manager, # Original manager\n                strategy_name=selected_strategy_name,\n                model_manager=self._model_manager,    # Pass model manager\n                strategy_config_override=strategy_config # Pass specific config\n            )\n        except Exception as e:\n             logger.error(f\"Failed to instantiate aggregator for strategy '{selected_strategy_name}': {e}\", exc_info=True)\n             raise AggregationError(f\"Failed to create instance for strategy '{selected_strategy_name}': {e}\")\n\n\n        # Execute the chosen strategy's aggregate method\n        result = await aggregator_instance.aggregate(outputs, context, trace_collector)\n\n        # Add adaptive selection information to the final result\n        result[\"selected_strategy\"] = selected_strategy_name\n\n\n        execution_time = time.time() - start_time\n        logger.info(\n            f\"Adaptive Selection completed using '{selected_strategy_name}' in {execution_time:.2f}s.\"\n        )\n\n        # Add trace for AdaptiveSelection itself, including the sub-result\n        if trace_collector:\n            trace_collector.add_aggregation_trace(\n                # Name of THIS strategy\n                strategy_name=self._strategy_name, # \"adaptive_selection\"\n                inputs={\n                    \"phase_output_keys\": list(outputs.keys()),\n                    \"context_keys\": list(context.keys()),\n                    \"available_strategies\": list(available_strategies_config.keys()),\n                    \"query\": query[:100] + \"...\" if len(query) &gt; 100 else query,\n                    \"query_type\": query_type\n                },\n                # Output includes the result from the sub-strategy + selection info\n                output={\n                    \"_selected_strategy\": selected_strategy_name,\n                    **result # Unpack the sub-strategy's result here\n                },\n                execution_time=execution_time,\n                parameters={ # Parameters of THIS strategy\n                    # Store the config structure used for selection\n                    \"strategy_definitions\": available_strategies_config,\n                    \"selector_model\": self._config.get(\"selector_model\")\n                }\n            )\n\n        return result\n\n    except AggregationError as e:\n         logger.error(f\"Adaptive Selection aggregation failed: {str(e)}\")\n         raise # Re-raise known aggregation errors\n    except Exception as e:\n        logger.error(f\"Unexpected error during Adaptive Selection aggregation: {str(e)}\", exc_info=True)\n        # Wrap unexpected errors\n        raise AggregationError(f\"Adaptive Selection aggregation failed unexpectedly: {str(e)}\")\n</code></pre>"},{"location":"api/aggregation/#ai_ensemble_suite.aggregation.BaseAggregator","title":"<code>BaseAggregator</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for aggregation strategies.</p> <p>Defines the interface for aggregators and provides common functionality like configuration handling and default confidence calculation.</p> Source code in <code>src\\ai_ensemble_suite\\aggregation\\base.py</code> <pre><code>class BaseAggregator(ABC):\n    \"\"\"Abstract base class for aggregation strategies.\n\n    Defines the interface for aggregators and provides common functionality\n    like configuration handling and default confidence calculation.\n    \"\"\"\n\n    def __init__(\n        self,\n        config_manager: \"ConfigManager\",\n        strategy_name: str,\n        model_manager: Optional[\"ModelManager\"] = None, # Added model_manager\n        strategy_config_override: Optional[Dict[str, Any]] = None # Added override\n    ) -&gt; None:\n        \"\"\"Initialize the aggregator.\n\n        Args:\n            config_manager: The main ConfigManager instance.\n            strategy_name: The name of the strategy for configuration lookup.\n            model_manager: Optional ModelManager instance, required by some strategies.\n            strategy_config_override: Optional dictionary to directly use as the\n                                      strategy's config, bypassing config_manager lookup.\n                                      Used by AdaptiveSelection.\n\n        Raises:\n            ConfigurationError: If strategy configuration loading fails (and no override).\n            TypeError: If config_manager is not provided.\n        \"\"\"\n        if config_manager is None:\n             raise TypeError(\"ConfigManager instance is required for BaseAggregator.\")\n\n        self._config_manager = config_manager\n        self._strategy_name = strategy_name\n        self._model_manager = model_manager # Store the model manager\n\n        # Load strategy configuration\n        if strategy_config_override is not None:\n             logger.debug(f\"Using provided config override for strategy '{strategy_name}'.\")\n             self._config = copy.deepcopy(strategy_config_override)\n             # Ensure 'strategy' key exists in override for consistency? Maybe not needed.\n             self._config.setdefault(\"strategy\", strategy_name)\n        else:\n            try:\n                # Fetch config using the strategy name\n                self._config = self._config_manager.get_aggregation_config(strategy_name)\n            except ConfigurationError as e:\n                # Re-raise with more context\n                raise ConfigurationError(\n                    f\"Failed to load configuration for aggregation strategy '{strategy_name}': {str(e)}\"\n                ) from e\n\n        logger.debug(f\"Initialized aggregation strategy '{strategy_name}'\")\n\n    @abstractmethod\n    async def aggregate(\n        self,\n        outputs: Dict[str, Dict[str, Any]],\n        context: Dict[str, Any],\n        trace_collector: Optional[TraceCollector] = None\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Aggregate the outputs from collaboration phases.\n\n        Args:\n            outputs: Dictionary mapping phase names to their full output dictionaries.\n                     Each value typically contains 'output', 'confidence', etc.\n            context: Dictionary containing context information from the ensemble run,\n                     potentially including the original query, phase sequence, etc.\n            trace_collector: Optional trace collector for gathering execution details.\n\n        Returns:\n            A dictionary containing at least:\n                response: The final aggregated output as a string.\n                confidence: A float score (0.0-1.0) representing the confidence\n                            in the aggregated response.\n            May contain additional strategy-specific information (e.g., 'best_phase').\n\n        Raises:\n            AggregationError: If the aggregation process fails significantly.\n        \"\"\"\n        pass\n\n    def _extract_output(self, phase_output_data: Any) -&gt; str:\n        \"\"\"Extract the primary output text content from a phase's result data.\n\n        Args:\n            phase_output_data: The data structure returned by a collaboration phase.\n\n        Returns:\n            The extracted output text as a string, or an empty string if none found.\n        \"\"\"\n        if isinstance(phase_output_data, str):\n            return phase_output_data.strip()\n        elif isinstance(phase_output_data, dict):\n            # Try common keys in preferred order\n            # Added 'result' based on potential model outputs\n            for field in [\"output\", \"response\", \"text\", \"result\", \"content\", \"final_output\", \"synthesis\", \"critique\"]:\n                content = phase_output_data.get(field)\n                if isinstance(content, str):\n                    return content.strip()\n            # Fallback for nested results (like AsyncThinking might have 'outputs': {'model_id': {'text': ...}})\n            outputs_dict = phase_output_data.get(\"outputs\")\n            if isinstance(outputs_dict, dict) and outputs_dict:\n                 # Try to get the first output's text, somewhat arbitrary\n                 first_output_key = next(iter(outputs_dict.keys()), None)\n                 if first_output_key:\n                      first_output_data = outputs_dict[first_output_key]\n                      # Check if the nested output data itself is a dict with text\n                      if isinstance(first_output_data, dict):\n                           # Check common keys again within the nested structure\n                           for field in [\"text\", \"output\", \"response\", \"result\"]:\n                                content = first_output_data.get(field)\n                                if isinstance(content, str):\n                                     logger.debug(f\"Extracted text from nested output under key '{first_output_key}'-&gt;'{field}'\")\n                                     return content.strip()\n                      # Check if the nested output data is just the string itself\n                      elif isinstance(first_output_data, str):\n                          logger.debug(f\"Extracted text directly from nested output under key '{first_output_key}'\")\n                          return first_output_data.strip()\n\n        # If no common key found or input is not dict/str, try converting non-complex types\n        try:\n            # Avoid converting large dicts/lists/bytes directly, might indicate wrong field access or hide errors\n            if isinstance(phase_output_data, (dict, list, bytes)):\n                 # Check if it's a simple dict/list with a single string value (less common)\n                 if len(phase_output_data) == 1:\n                      single_value = None\n                      if isinstance(phase_output_data, dict):\n                           single_value = next(iter(phase_output_data.values()), None)\n                      elif isinstance(phase_output_data, list):\n                           single_value = phase_output_data[0]\n                      if isinstance(single_value, str):\n                           return single_value.strip()\n\n                 # Otherwise, assume complex structure we can't reliably stringify\n                 logger.debug(f\"Cannot reliably extract text from complex data structure: {type(phase_output_data)}\")\n                 return \"\"\n             # Check if it's a simple type that can be stringified safely (int, float, bool, NoneType)\n            elif phase_output_data is None:\n                return \"\" # Explicitly handle None\n            elif isinstance(phase_output_data, (int, float, bool)): # Add other simple types as needed\n                 return str(phase_output_data).strip()\n            else: # Unknown potentially complex type\n                 logger.warning(f\"Cannot extract text from phase output of unexpected type {type(phase_output_data)}. Returning empty string.\")\n                 return \"\"\n        except Exception as e:\n            logger.debug(f\"Could not convert phase output of type {type(phase_output_data)} to string: {e}\")\n            return \"\" # Return empty string if conversion fails or is inappropriate\n\n    def get_config(self) -&gt; Dict[str, Any]:\n        \"\"\"Get the configuration specific to this aggregation strategy.\n\n        Returns:\n            A deep copy of the strategy's configuration dictionary.\n        \"\"\"\n        return copy.deepcopy(self._config)\n\n    def get_name(self) -&gt; str:\n        \"\"\"Get the name of this aggregation strategy.\n\n        Returns:\n            The strategy name string.\n        \"\"\"\n        return self._strategy_name\n\n    # Kept this method, but note that model requirements might be needed for some strategies\n    # Consider moving model requirement logic elsewhere or making it dynamic?\n    def get_required_models(self) -&gt; Set[str]:\n        \"\"\"Get the set of models specifically required BY THE AGGREGATOR itself.\n\n        Note: This usually applies only to model-based aggregators like EnsembleFusion\n              or evaluation models used in MultidimensionalVoting/AdaptiveSelection.\n\n        Returns:\n            Set of model IDs required directly by this aggregation strategy.\n        \"\"\"\n        # Default: Most aggregators don't inherently require specific models themselves.\n        # Strategies needing models should override this or get models from config.\n        required = set()\n        # Example for model-based aggregators (they should override this):\n        if \"fusion_model\" in self._config:\n             required.add(self._config[\"fusion_model\"])\n        if \"evaluator_model\" in self._config:\n             required.add(self._config[\"evaluator_model\"])\n        if \"selector_model\" in self._config: # For AdaptiveSelection\n             required.add(self._config[\"selector_model\"])\n        return required\n\n\n    def get_final_phase(self) -&gt; Optional[str]:\n        \"\"\"Get the name of the phase considered 'final' by this strategy's config.\n\n        Used by strategies like SequentialRefinement or as a fallback.\n\n        Returns:\n            The name of the final phase string, or None if not specified.\n        \"\"\"\n        # Get from the strategy-specific config\n        return self._config.get(\"final_phase\")\n\n    def _calculate_confidence(\n        self,\n        phase_outputs: Dict[str, Dict[str, Any]]\n    ) -&gt; float:\n        \"\"\"Calculate an overall confidence score based on input phase outputs.\n\n        Provides a default implementation that averages confidence scores found\n        in the phase outputs. Subclasses can override this for more specific logic.\n\n        Args:\n            phase_outputs: Dictionary mapping phase names to their outputs.\n\n        Returns:\n            A confidence score between 0.0 and 1.0.\n        \"\"\"\n        confidence_scores = []\n\n        for phase_name, output_data in phase_outputs.items():\n            # Check if the output is a dict and contains a 'confidence' key\n            if isinstance(output_data, dict) and \"confidence\" in output_data:\n                confidence_value = output_data[\"confidence\"]\n                score = None\n                if isinstance(confidence_value, (int, float)):\n                    # Direct numeric confidence score\n                    score = float(confidence_value)\n                elif isinstance(confidence_value, dict):\n                    # Nested dictionary, look for a 'combined' score first\n                    score = confidence_value.get(\"combined\")\n                    # Fallback: average all numeric values in the confidence dict? Risky.\n                    # Or maybe use a specific metric like 'token_prob'?\n                    if score is None: score = confidence_value.get(\"token_prob\") # Example fallback\n                # Ensure score is a valid number\n                if isinstance(score, (int, float)):\n                     # Clamp score to valid range [0, 1]\n                     clamped_score = min(max(float(score), 0.0), 1.0)\n                     confidence_scores.append(clamped_score)\n                # else:\n                #     logger.debug(f\"Non-numeric or missing 'combined' confidence found in phase '{phase_name}'.\")\n\n        # Calculate average confidence or return default if no scores available\n        if confidence_scores:\n            average_confidence = sum(confidence_scores) / len(confidence_scores)\n            logger.debug(f\"Calculated average confidence from {len(confidence_scores)} phases: {average_confidence:.3f}\")\n            return average_confidence\n        else:\n            logger.warning(\"No valid confidence scores found in any phase outputs, returning default confidence 0.7\")\n            return 0.7  # Default medium-high confidence\n</code></pre>"},{"location":"api/aggregation/#ai_ensemble_suite.aggregation.BaseAggregator.__init__","title":"<code>__init__(config_manager, strategy_name, model_manager=None, strategy_config_override=None)</code>","text":"<p>Initialize the aggregator.</p> <p>Parameters:</p> Name Type Description Default <code>config_manager</code> <code>ConfigManager</code> <p>The main ConfigManager instance.</p> required <code>strategy_name</code> <code>str</code> <p>The name of the strategy for configuration lookup.</p> required <code>model_manager</code> <code>Optional[ModelManager]</code> <p>Optional ModelManager instance, required by some strategies.</p> <code>None</code> <code>strategy_config_override</code> <code>Optional[Dict[str, Any]]</code> <p>Optional dictionary to directly use as the                       strategy's config, bypassing config_manager lookup.                       Used by AdaptiveSelection.</p> <code>None</code> <p>Raises:</p> Type Description <code>ConfigurationError</code> <p>If strategy configuration loading fails (and no override).</p> <code>TypeError</code> <p>If config_manager is not provided.</p> Source code in <code>src\\ai_ensemble_suite\\aggregation\\base.py</code> <pre><code>def __init__(\n    self,\n    config_manager: \"ConfigManager\",\n    strategy_name: str,\n    model_manager: Optional[\"ModelManager\"] = None, # Added model_manager\n    strategy_config_override: Optional[Dict[str, Any]] = None # Added override\n) -&gt; None:\n    \"\"\"Initialize the aggregator.\n\n    Args:\n        config_manager: The main ConfigManager instance.\n        strategy_name: The name of the strategy for configuration lookup.\n        model_manager: Optional ModelManager instance, required by some strategies.\n        strategy_config_override: Optional dictionary to directly use as the\n                                  strategy's config, bypassing config_manager lookup.\n                                  Used by AdaptiveSelection.\n\n    Raises:\n        ConfigurationError: If strategy configuration loading fails (and no override).\n        TypeError: If config_manager is not provided.\n    \"\"\"\n    if config_manager is None:\n         raise TypeError(\"ConfigManager instance is required for BaseAggregator.\")\n\n    self._config_manager = config_manager\n    self._strategy_name = strategy_name\n    self._model_manager = model_manager # Store the model manager\n\n    # Load strategy configuration\n    if strategy_config_override is not None:\n         logger.debug(f\"Using provided config override for strategy '{strategy_name}'.\")\n         self._config = copy.deepcopy(strategy_config_override)\n         # Ensure 'strategy' key exists in override for consistency? Maybe not needed.\n         self._config.setdefault(\"strategy\", strategy_name)\n    else:\n        try:\n            # Fetch config using the strategy name\n            self._config = self._config_manager.get_aggregation_config(strategy_name)\n        except ConfigurationError as e:\n            # Re-raise with more context\n            raise ConfigurationError(\n                f\"Failed to load configuration for aggregation strategy '{strategy_name}': {str(e)}\"\n            ) from e\n\n    logger.debug(f\"Initialized aggregation strategy '{strategy_name}'\")\n</code></pre>"},{"location":"api/aggregation/#ai_ensemble_suite.aggregation.BaseAggregator.aggregate","title":"<code>aggregate(outputs, context, trace_collector=None)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Aggregate the outputs from collaboration phases.</p> <p>Parameters:</p> Name Type Description Default <code>outputs</code> <code>Dict[str, Dict[str, Any]]</code> <p>Dictionary mapping phase names to their full output dictionaries.      Each value typically contains 'output', 'confidence', etc.</p> required <code>context</code> <code>Dict[str, Any]</code> <p>Dictionary containing context information from the ensemble run,      potentially including the original query, phase sequence, etc.</p> required <code>trace_collector</code> <code>Optional[TraceCollector]</code> <p>Optional trace collector for gathering execution details.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>A dictionary containing at least: response: The final aggregated output as a string. confidence: A float score (0.0-1.0) representing the confidence             in the aggregated response.</p> <code>Dict[str, Any]</code> <p>May contain additional strategy-specific information (e.g., 'best_phase').</p> <p>Raises:</p> Type Description <code>AggregationError</code> <p>If the aggregation process fails significantly.</p> Source code in <code>src\\ai_ensemble_suite\\aggregation\\base.py</code> <pre><code>@abstractmethod\nasync def aggregate(\n    self,\n    outputs: Dict[str, Dict[str, Any]],\n    context: Dict[str, Any],\n    trace_collector: Optional[TraceCollector] = None\n) -&gt; Dict[str, Any]:\n    \"\"\"Aggregate the outputs from collaboration phases.\n\n    Args:\n        outputs: Dictionary mapping phase names to their full output dictionaries.\n                 Each value typically contains 'output', 'confidence', etc.\n        context: Dictionary containing context information from the ensemble run,\n                 potentially including the original query, phase sequence, etc.\n        trace_collector: Optional trace collector for gathering execution details.\n\n    Returns:\n        A dictionary containing at least:\n            response: The final aggregated output as a string.\n            confidence: A float score (0.0-1.0) representing the confidence\n                        in the aggregated response.\n        May contain additional strategy-specific information (e.g., 'best_phase').\n\n    Raises:\n        AggregationError: If the aggregation process fails significantly.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/aggregation/#ai_ensemble_suite.aggregation.BaseAggregator.get_config","title":"<code>get_config()</code>","text":"<p>Get the configuration specific to this aggregation strategy.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>A deep copy of the strategy's configuration dictionary.</p> Source code in <code>src\\ai_ensemble_suite\\aggregation\\base.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Get the configuration specific to this aggregation strategy.\n\n    Returns:\n        A deep copy of the strategy's configuration dictionary.\n    \"\"\"\n    return copy.deepcopy(self._config)\n</code></pre>"},{"location":"api/aggregation/#ai_ensemble_suite.aggregation.BaseAggregator.get_final_phase","title":"<code>get_final_phase()</code>","text":"<p>Get the name of the phase considered 'final' by this strategy's config.</p> <p>Used by strategies like SequentialRefinement or as a fallback.</p> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>The name of the final phase string, or None if not specified.</p> Source code in <code>src\\ai_ensemble_suite\\aggregation\\base.py</code> <pre><code>def get_final_phase(self) -&gt; Optional[str]:\n    \"\"\"Get the name of the phase considered 'final' by this strategy's config.\n\n    Used by strategies like SequentialRefinement or as a fallback.\n\n    Returns:\n        The name of the final phase string, or None if not specified.\n    \"\"\"\n    # Get from the strategy-specific config\n    return self._config.get(\"final_phase\")\n</code></pre>"},{"location":"api/aggregation/#ai_ensemble_suite.aggregation.BaseAggregator.get_name","title":"<code>get_name()</code>","text":"<p>Get the name of this aggregation strategy.</p> <p>Returns:</p> Type Description <code>str</code> <p>The strategy name string.</p> Source code in <code>src\\ai_ensemble_suite\\aggregation\\base.py</code> <pre><code>def get_name(self) -&gt; str:\n    \"\"\"Get the name of this aggregation strategy.\n\n    Returns:\n        The strategy name string.\n    \"\"\"\n    return self._strategy_name\n</code></pre>"},{"location":"api/aggregation/#ai_ensemble_suite.aggregation.BaseAggregator.get_required_models","title":"<code>get_required_models()</code>","text":"<p>Get the set of models specifically required BY THE AGGREGATOR itself.</p> This usually applies only to model-based aggregators like EnsembleFusion <p>or evaluation models used in MultidimensionalVoting/AdaptiveSelection.</p> <p>Returns:</p> Type Description <code>Set[str]</code> <p>Set of model IDs required directly by this aggregation strategy.</p> Source code in <code>src\\ai_ensemble_suite\\aggregation\\base.py</code> <pre><code>def get_required_models(self) -&gt; Set[str]:\n    \"\"\"Get the set of models specifically required BY THE AGGREGATOR itself.\n\n    Note: This usually applies only to model-based aggregators like EnsembleFusion\n          or evaluation models used in MultidimensionalVoting/AdaptiveSelection.\n\n    Returns:\n        Set of model IDs required directly by this aggregation strategy.\n    \"\"\"\n    # Default: Most aggregators don't inherently require specific models themselves.\n    # Strategies needing models should override this or get models from config.\n    required = set()\n    # Example for model-based aggregators (they should override this):\n    if \"fusion_model\" in self._config:\n         required.add(self._config[\"fusion_model\"])\n    if \"evaluator_model\" in self._config:\n         required.add(self._config[\"evaluator_model\"])\n    if \"selector_model\" in self._config: # For AdaptiveSelection\n         required.add(self._config[\"selector_model\"])\n    return required\n</code></pre>"},{"location":"api/aggregation/#ai_ensemble_suite.aggregation.ConfidenceBased","title":"<code>ConfidenceBased</code>","text":"<p>               Bases: <code>BaseAggregator</code></p> <p>Confidence-Based aggregation strategy.</p> <p>Selects the output from the collaboration phase that has the highest associated confidence score, potentially applying a minimum threshold.</p> Source code in <code>src\\ai_ensemble_suite\\aggregation\\confidence_based.py</code> <pre><code>class ConfidenceBased(BaseAggregator):\n    \"\"\"Confidence-Based aggregation strategy.\n\n    Selects the output from the collaboration phase that has the highest\n    associated confidence score, potentially applying a minimum threshold.\n    \"\"\"\n\n    # Override __init__ to accept optional model_manager and override\n    # Although this strategy doesn't use model_manager, maintain signature consistency\n    def __init__(\n        self,\n        config_manager: \"ConfigManager\",\n        strategy_name: str,\n        model_manager: Optional[\"ModelManager\"] = None,\n        strategy_config_override: Optional[Dict[str, Any]] = None\n    ) -&gt; None:\n        \"\"\"Initialize the ConfidenceBased aggregator.\"\"\"\n        super().__init__(config_manager, strategy_name, model_manager, strategy_config_override)\n        # No specific init needed\n\n\n    async def aggregate(\n        self,\n        outputs: Dict[str, Dict[str, Any]],\n        context: Dict[str, Any],\n        trace_collector: Optional[TraceCollector] = None\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Aggregate outputs by selecting the one with the highest confidence score.\n\n        Args:\n            outputs: Dictionary mapping phase names to their output data.\n            context: Context information from collaboration phases.\n            trace_collector: Optional trace collector.\n\n        Returns:\n            Dictionary containing:\n                response: The text output from the highest confidence phase.\n                confidence: The confidence score of the selected output.\n                source_phase: The name of the phase providing the selected output.\n                confidence_scores: Dictionary mapping all phase names to their scores.\n\n        Raises:\n            AggregationError: If no outputs with valid confidence scores are available.\n        \"\"\"\n        start_time = time.time()\n        logger.debug(f\"Starting Confidence-Based aggregation for strategy '{self._strategy_name}'\")\n\n        if not outputs:\n             raise AggregationError(\"No phase outputs provided for Confidence-Based aggregation.\")\n\n        try:\n            # Get confidence threshold from strategy configuration\n            threshold = self._config.get(\"threshold\", 0.0) # Default is 0.0 (consider all)\n            if not isinstance(threshold, (int, float)) or not 0.0 &lt;= threshold &lt;= 1.0:\n                 logger.warning(f\"Invalid confidence threshold '{threshold}', using 0.0.\")\n                 threshold = 0.0\n\n            # --- Extract confidence scores and outputs ---\n            scored_outputs: List[Tuple[str, str, float]] = [] # (phase_name, output_text, confidence_score)\n\n            all_phase_confidences: Dict[str, float] = {} # Store all scores for tracing/result\n\n            for phase_name, phase_output_data in outputs.items():\n                output_text = self._extract_output(phase_output_data)\n                # Skip phases where text extraction failed\n                if not output_text:\n                    logger.debug(f\"Could not extract text from phase '{phase_name}', skipping.\")\n                    all_phase_confidences[phase_name] = -1.0 # Indicate missing text\n                    continue\n\n                confidence = -1.0 # Default to invalid score\n                if isinstance(phase_output_data, dict) and \"confidence\" in phase_output_data:\n                    conf_val = phase_output_data[\"confidence\"]\n                    if isinstance(conf_val, (int, float)):\n                        confidence = float(conf_val)\n                    elif isinstance(conf_val, dict) and \"combined\" in conf_val:\n                        # Prioritize 'combined' score if available\n                        combined_score = conf_val.get(\"combined\")\n                        if isinstance(combined_score, (int, float)):\n                             confidence = float(combined_score)\n\n                # Ensure confidence is within [0, 1] range\n                if isinstance(confidence, (int, float)) and 0.0 &lt;= confidence &lt;= 1.0:\n                     all_phase_confidences[phase_name] = confidence\n                     # Add to list if above the configured threshold\n                     if confidence &gt;= threshold:\n                         scored_outputs.append((phase_name, output_text, confidence))\n                         logger.debug(f\"Phase '{phase_name}' added with confidence {confidence:.3f} (&gt;= threshold {threshold:.3f})\")\n                     else:\n                          logger.debug(f\"Phase '{phase_name}' skipped, confidence {confidence:.3f} &lt; threshold {threshold:.3f}\")\n                else:\n                    # Log phases with missing or invalid confidence\n                    logger.debug(f\"Phase '{phase_name}' has missing or invalid confidence score.\")\n                    all_phase_confidences[phase_name] = -1.0 # Mark as invalid\n\n\n            # --- Determine the best output ---\n            if not scored_outputs:\n                # No outputs met the threshold (or none had valid confidence)\n                logger.warning(f\"No outputs met the confidence threshold &gt;= {threshold:.3f}. \"\n                               f\"Falling back to highest available score (ignoring threshold) or default.\")\n\n                # Fallback 1: Find highest score among *all* phases with valid scores\n                valid_scores = {name: score for name, score in all_phase_confidences.items() if score &gt;= 0.0}\n                if valid_scores:\n                     best_fallback_phase = max(valid_scores, key=valid_scores.get)\n                     best_fallback_score = valid_scores[best_fallback_phase]\n                     best_fallback_text = self._extract_output(outputs[best_fallback_phase])\n                     logger.info(f\"Using fallback phase '{best_fallback_phase}' with confidence {best_fallback_score:.3f}.\")\n                     scored_outputs.append((best_fallback_phase, best_fallback_text, best_fallback_score))\n                else:\n                    # Fallback 2: If NO valid scores at all, error out\n                    raise AggregationError(\"No outputs with valid confidence scores available for aggregation.\")\n\n            # Sort the valid (above threshold or best fallback) outputs by confidence (descending)\n            scored_outputs.sort(key=lambda x: x[2], reverse=True)\n\n            # Get the highest confidence output from the filtered/sorted list\n            best_phase_name, best_output_text, best_confidence = scored_outputs[0]\n\n\n            execution_time = time.time() - start_time\n            logger.info(\n                f\"Confidence-Based aggregation completed in {execution_time:.2f}s. \"\n                f\"Selected phase: '{best_phase_name}' (Confidence: {best_confidence:.3f})\",\n                 extra={ \"threshold\": threshold }\n            )\n\n            # Prepare final result dictionary\n            aggregation_result = {\n                \"response\": best_output_text,\n                \"confidence\": best_confidence,\n                \"source_phase\": best_phase_name,\n                # Include map of all phase confidences (even invalid ones marked as -1)\n                \"confidence_scores\": all_phase_confidences\n            }\n\n            # Add trace if collector is provided\n            if trace_collector:\n                trace_collector.add_aggregation_trace(\n                    strategy_name=self._strategy_name,\n                    inputs={\n                        \"phase_output_keys\": list(outputs.keys()),\n                        \"context_keys\": list(context.keys()),\n                    },\n                    output=aggregation_result, # Contains response, confidence, source, scores map\n                    execution_time=execution_time,\n                    parameters={ # Parameters used by this strategy\n                        \"threshold\": threshold,\n                        # Maybe trace the sorted list for debugging? Could be large.\n                        # \"sorted_scored_outputs\": [(p, c) for p, _, c in scored_outputs]\n                    }\n                )\n\n            return aggregation_result\n\n        except AggregationError as e:\n             logger.error(f\"Confidence-based aggregation failed: {str(e)}\")\n             raise # Re-raise known aggregation errors\n        except Exception as e:\n            logger.error(f\"Unexpected error during Confidence-Based aggregation: {str(e)}\", exc_info=True)\n            # Wrap unexpected errors\n            raise AggregationError(f\"Confidence-Based aggregation failed unexpectedly: {str(e)}\")\n</code></pre>"},{"location":"api/aggregation/#ai_ensemble_suite.aggregation.ConfidenceBased.__init__","title":"<code>__init__(config_manager, strategy_name, model_manager=None, strategy_config_override=None)</code>","text":"<p>Initialize the ConfidenceBased aggregator.</p> Source code in <code>src\\ai_ensemble_suite\\aggregation\\confidence_based.py</code> <pre><code>def __init__(\n    self,\n    config_manager: \"ConfigManager\",\n    strategy_name: str,\n    model_manager: Optional[\"ModelManager\"] = None,\n    strategy_config_override: Optional[Dict[str, Any]] = None\n) -&gt; None:\n    \"\"\"Initialize the ConfidenceBased aggregator.\"\"\"\n    super().__init__(config_manager, strategy_name, model_manager, strategy_config_override)\n</code></pre>"},{"location":"api/aggregation/#ai_ensemble_suite.aggregation.ConfidenceBased.aggregate","title":"<code>aggregate(outputs, context, trace_collector=None)</code>  <code>async</code>","text":"<p>Aggregate outputs by selecting the one with the highest confidence score.</p> <p>Parameters:</p> Name Type Description Default <code>outputs</code> <code>Dict[str, Dict[str, Any]]</code> <p>Dictionary mapping phase names to their output data.</p> required <code>context</code> <code>Dict[str, Any]</code> <p>Context information from collaboration phases.</p> required <code>trace_collector</code> <code>Optional[TraceCollector]</code> <p>Optional trace collector.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary containing: response: The text output from the highest confidence phase. confidence: The confidence score of the selected output. source_phase: The name of the phase providing the selected output. confidence_scores: Dictionary mapping all phase names to their scores.</p> <p>Raises:</p> Type Description <code>AggregationError</code> <p>If no outputs with valid confidence scores are available.</p> Source code in <code>src\\ai_ensemble_suite\\aggregation\\confidence_based.py</code> <pre><code>async def aggregate(\n    self,\n    outputs: Dict[str, Dict[str, Any]],\n    context: Dict[str, Any],\n    trace_collector: Optional[TraceCollector] = None\n) -&gt; Dict[str, Any]:\n    \"\"\"Aggregate outputs by selecting the one with the highest confidence score.\n\n    Args:\n        outputs: Dictionary mapping phase names to their output data.\n        context: Context information from collaboration phases.\n        trace_collector: Optional trace collector.\n\n    Returns:\n        Dictionary containing:\n            response: The text output from the highest confidence phase.\n            confidence: The confidence score of the selected output.\n            source_phase: The name of the phase providing the selected output.\n            confidence_scores: Dictionary mapping all phase names to their scores.\n\n    Raises:\n        AggregationError: If no outputs with valid confidence scores are available.\n    \"\"\"\n    start_time = time.time()\n    logger.debug(f\"Starting Confidence-Based aggregation for strategy '{self._strategy_name}'\")\n\n    if not outputs:\n         raise AggregationError(\"No phase outputs provided for Confidence-Based aggregation.\")\n\n    try:\n        # Get confidence threshold from strategy configuration\n        threshold = self._config.get(\"threshold\", 0.0) # Default is 0.0 (consider all)\n        if not isinstance(threshold, (int, float)) or not 0.0 &lt;= threshold &lt;= 1.0:\n             logger.warning(f\"Invalid confidence threshold '{threshold}', using 0.0.\")\n             threshold = 0.0\n\n        # --- Extract confidence scores and outputs ---\n        scored_outputs: List[Tuple[str, str, float]] = [] # (phase_name, output_text, confidence_score)\n\n        all_phase_confidences: Dict[str, float] = {} # Store all scores for tracing/result\n\n        for phase_name, phase_output_data in outputs.items():\n            output_text = self._extract_output(phase_output_data)\n            # Skip phases where text extraction failed\n            if not output_text:\n                logger.debug(f\"Could not extract text from phase '{phase_name}', skipping.\")\n                all_phase_confidences[phase_name] = -1.0 # Indicate missing text\n                continue\n\n            confidence = -1.0 # Default to invalid score\n            if isinstance(phase_output_data, dict) and \"confidence\" in phase_output_data:\n                conf_val = phase_output_data[\"confidence\"]\n                if isinstance(conf_val, (int, float)):\n                    confidence = float(conf_val)\n                elif isinstance(conf_val, dict) and \"combined\" in conf_val:\n                    # Prioritize 'combined' score if available\n                    combined_score = conf_val.get(\"combined\")\n                    if isinstance(combined_score, (int, float)):\n                         confidence = float(combined_score)\n\n            # Ensure confidence is within [0, 1] range\n            if isinstance(confidence, (int, float)) and 0.0 &lt;= confidence &lt;= 1.0:\n                 all_phase_confidences[phase_name] = confidence\n                 # Add to list if above the configured threshold\n                 if confidence &gt;= threshold:\n                     scored_outputs.append((phase_name, output_text, confidence))\n                     logger.debug(f\"Phase '{phase_name}' added with confidence {confidence:.3f} (&gt;= threshold {threshold:.3f})\")\n                 else:\n                      logger.debug(f\"Phase '{phase_name}' skipped, confidence {confidence:.3f} &lt; threshold {threshold:.3f}\")\n            else:\n                # Log phases with missing or invalid confidence\n                logger.debug(f\"Phase '{phase_name}' has missing or invalid confidence score.\")\n                all_phase_confidences[phase_name] = -1.0 # Mark as invalid\n\n\n        # --- Determine the best output ---\n        if not scored_outputs:\n            # No outputs met the threshold (or none had valid confidence)\n            logger.warning(f\"No outputs met the confidence threshold &gt;= {threshold:.3f}. \"\n                           f\"Falling back to highest available score (ignoring threshold) or default.\")\n\n            # Fallback 1: Find highest score among *all* phases with valid scores\n            valid_scores = {name: score for name, score in all_phase_confidences.items() if score &gt;= 0.0}\n            if valid_scores:\n                 best_fallback_phase = max(valid_scores, key=valid_scores.get)\n                 best_fallback_score = valid_scores[best_fallback_phase]\n                 best_fallback_text = self._extract_output(outputs[best_fallback_phase])\n                 logger.info(f\"Using fallback phase '{best_fallback_phase}' with confidence {best_fallback_score:.3f}.\")\n                 scored_outputs.append((best_fallback_phase, best_fallback_text, best_fallback_score))\n            else:\n                # Fallback 2: If NO valid scores at all, error out\n                raise AggregationError(\"No outputs with valid confidence scores available for aggregation.\")\n\n        # Sort the valid (above threshold or best fallback) outputs by confidence (descending)\n        scored_outputs.sort(key=lambda x: x[2], reverse=True)\n\n        # Get the highest confidence output from the filtered/sorted list\n        best_phase_name, best_output_text, best_confidence = scored_outputs[0]\n\n\n        execution_time = time.time() - start_time\n        logger.info(\n            f\"Confidence-Based aggregation completed in {execution_time:.2f}s. \"\n            f\"Selected phase: '{best_phase_name}' (Confidence: {best_confidence:.3f})\",\n             extra={ \"threshold\": threshold }\n        )\n\n        # Prepare final result dictionary\n        aggregation_result = {\n            \"response\": best_output_text,\n            \"confidence\": best_confidence,\n            \"source_phase\": best_phase_name,\n            # Include map of all phase confidences (even invalid ones marked as -1)\n            \"confidence_scores\": all_phase_confidences\n        }\n\n        # Add trace if collector is provided\n        if trace_collector:\n            trace_collector.add_aggregation_trace(\n                strategy_name=self._strategy_name,\n                inputs={\n                    \"phase_output_keys\": list(outputs.keys()),\n                    \"context_keys\": list(context.keys()),\n                },\n                output=aggregation_result, # Contains response, confidence, source, scores map\n                execution_time=execution_time,\n                parameters={ # Parameters used by this strategy\n                    \"threshold\": threshold,\n                    # Maybe trace the sorted list for debugging? Could be large.\n                    # \"sorted_scored_outputs\": [(p, c) for p, _, c in scored_outputs]\n                }\n            )\n\n        return aggregation_result\n\n    except AggregationError as e:\n         logger.error(f\"Confidence-based aggregation failed: {str(e)}\")\n         raise # Re-raise known aggregation errors\n    except Exception as e:\n        logger.error(f\"Unexpected error during Confidence-Based aggregation: {str(e)}\", exc_info=True)\n        # Wrap unexpected errors\n        raise AggregationError(f\"Confidence-Based aggregation failed unexpectedly: {str(e)}\")\n</code></pre>"},{"location":"api/aggregation/#ai_ensemble_suite.aggregation.EnsembleFusion","title":"<code>EnsembleFusion</code>","text":"<p>               Bases: <code>BaseAggregator</code></p> <p>Ensemble Fusion aggregation strategy.</p> <p>Uses a designated 'fusion' model to synthesize multiple phase outputs into a single, coherent response, aiming to integrate the best aspects of each input.</p> Source code in <code>src\\ai_ensemble_suite\\aggregation\\ensemble_fusion.py</code> <pre><code>class EnsembleFusion(BaseAggregator):\n    \"\"\"Ensemble Fusion aggregation strategy.\n\n    Uses a designated 'fusion' model to synthesize multiple phase outputs into\n    a single, coherent response, aiming to integrate the best aspects of each input.\n    \"\"\"\n\n    def __init__(\n        self,\n        config_manager: \"ConfigManager\",\n        strategy_name: str,\n        model_manager: Optional[\"ModelManager\"] = None,\n        strategy_config_override: Optional[Dict[str, Any]] = None\n    ) -&gt; None:\n        \"\"\"Initialize the EnsembleFusion aggregator.\"\"\"\n        super().__init__(config_manager, strategy_name, model_manager, strategy_config_override)\n        if self._model_manager is None:\n             logger.warning(f\"EnsembleFusion strategy '{self._strategy_name}' initialized without a ModelManager. Aggregation will likely fail.\")\n\n    async def aggregate(\n        self,\n        outputs: Dict[str, Dict[str, Any]],\n        context: Dict[str, Any],\n        trace_collector: Optional[TraceCollector] = None\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Aggregate phase outputs by using a specified model to fuse them.\"\"\"\n        start_time = time.time()\n        logger.debug(f\"Starting Ensemble Fusion aggregation using strategy '{self._strategy_name}'\")\n\n        if self._model_manager is None:\n            raise AggregationError(\"ModelManager is required for Ensemble Fusion but not available.\")\n\n        if not outputs:\n             raise AggregationError(\"No phase outputs provided for Ensemble Fusion.\")\n\n        try:\n            # Determine the Fusion Model\n            fusion_model_id = self._config.get(\"fusion_model\") # Key checked by schema\n            fusion_model_id = context.get(\"fusion_model\", fusion_model_id) # Allow context override\n\n            if not fusion_model_id:\n                 # Schema validation should require fusion_model, so this is unlikely\n                 logger.error(\"Configuration Error: 'fusion_model' is required but missing.\")\n                 raise AggregationError(\"Missing required 'fusion_model' configuration for Ensemble Fusion.\")\n\n            # Ensure the selected fusion model actually exists\n            try:\n                 fusion_model_instance = self._model_manager.get_model(fusion_model_id)\n                 logger.info(f\"Using fusion model: {fusion_model_id}\")\n            except ModelError:\n                 logger.error(f\"Specified fusion model '{fusion_model_id}' not found in ModelManager.\")\n                 raise AggregationError(f\"Fusion model '{fusion_model_id}' not available.\")\n\n\n            # Prepare Inputs for Fusion\n            outputs_to_fuse: Dict[str, str] = {}\n            for phase_name, phase_output_data in outputs.items():\n                extracted_text = self._extract_output(phase_output_data)\n                if extracted_text:\n                    outputs_to_fuse[phase_name] = extracted_text\n                else:\n                    logger.warning(f\"Could not extract usable text from phase '{phase_name}' for fusion input.\")\n\n            if not outputs_to_fuse:\n                 # Handle case where only one valid input exists - fusion might still be useful\n                 if len(outputs) == 1:\n                      logger.warning(\"Only one valid output found, using it directly instead of fusion.\")\n                      single_phase_name = list(outputs.keys())[0]\n                      single_output_data = outputs[single_phase_name]\n                      single_text = self._extract_output(single_output_data)\n                      single_confidence = self._calculate_confidence({single_phase_name: single_output_data})\n                      return {\n                           \"response\": single_text,\n                           \"confidence\": single_confidence,\n                           \"fusion_model\": None, # Indicate no fusion occurred\n                           \"source_outputs\": {single_phase_name: single_text}\n                      }\n                 else:\n                      raise AggregationError(\"No valid outputs found to provide as input for fusion.\")\n\n            # Format the collected outputs into a single string\n            fusion_input_str = self._format_fusion_input(outputs_to_fuse, context)\n\n            # Prepare and Run Fusion Prompt\n            fusion_template_name = self._config.get(\"fusion_template\", \"ensemble_fusion\") # Checked by schema\n\n            # Robust default template (in case formatting fails)\n            default_fusion_template = f\"\"\"Synthesize the following outputs into a single, high-quality response to the original query. Focus on integrating the best aspects, ensuring accuracy, clarity, and coherence. Avoid simply listing the inputs; create a unified final answer.\n\nORIGINAL QUERY:\n{{query}}\n\nINPUTS TO FUSE:\n{{fusion_input}}\n\nBased on the query and the provided inputs, generate a comprehensive and well-structured final response:\"\"\"\n\n            query_context = context.get(\"query\", \"N/A\")\n            if query_context == \"N/A\":\n                 logger.warning(\"Query not found in context for fusion prompt.\")\n\n            fusion_prompt = \"\"\n            try:\n                context_dict = {\"query\": query_context, \"fusion_input\": fusion_input_str}\n                fusion_prompt = self.render_template(fusion_template_name, context_dict)\n            except (ConfigurationError, ValidationError) as e:\n                 logger.warning(f\"Error formatting fusion template '{fusion_template_name}': {e}. Using default.\")\n                 try:\n                      fusion_prompt = default_fusion_template.format(query=query_context, fusion_input=fusion_input_str)\n                 except KeyError: # Catch issues with the default template itself\n                       logger.error(\"Failed to format even the default fusion template.\", exc_info=True)\n                       raise AggregationError(\"Failed to create fusion prompt.\") from e\n            except Exception as e:\n                 logger.error(f\"Unexpected error formatting fusion prompt: {e}\", exc_info=True)\n                 # Fallback to default on unexpected errors\n                 try:\n                      fusion_prompt = default_fusion_template.format(query=query_context, fusion_input=fusion_input_str)\n                 except KeyError:\n                       logger.error(\"Failed to format even the default fusion template on unexpected error.\", exc_info=True)\n                       raise AggregationError(\"Failed to create fusion prompt.\") from e\n\n\n            # Run the fusion model\n            logger.debug(f\"Sending fusion prompt to model '{fusion_model_id}'. Prompt length: {len(fusion_prompt)}\")\n            fusion_params = {\n                 \"temperature\": self._config.get(\"fusion_temperature\", 0.6),\n                 \"max_tokens\": self._config.get(\"fusion_max_tokens\", 2048),\n                 \"top_p\": self._config.get(\"fusion_top_p\"),\n                 \"top_k\": self._config.get(\"fusion_top_k\"),\n                 \"repeat_penalty\": self._config.get(\"fusion_repeat_penalty\")\n            }\n            fusion_params_filtered = {k: v for k, v in fusion_params.items() if v is not None}\n            logger.debug(f\"Fusion inference parameters: {fusion_params_filtered}\")\n\n            fusion_result_raw = await self._model_manager.run_inference(\n                model_id=fusion_model_id,\n                prompt=fusion_prompt,\n                compute_confidence=True,\n                **fusion_params_filtered\n            )\n            logger.debug(\"Received fusion result from model.\")\n\n\n            # Process Fusion Result\n            fused_output_text = fusion_result_raw.get(\"text\", \"\")\n            if not fused_output_text:\n                 logger.warning(f\"Fusion model '{fusion_model_id}' returned empty text.\")\n                 # Consider fallback - maybe return best input based on confidence?\n                 # For now, raise error if fusion genuinely failed.\n                 raise AggregationError(f\"Fusion model '{fusion_model_id}' failed to generate output.\")\n\n            # Determine confidence score\n            confidence = 0.0\n            fusion_model_confidence_data = fusion_result_raw.get(\"confidence\")\n\n            if isinstance(fusion_model_confidence_data, dict):\n                 combined_conf = fusion_model_confidence_data.get(\"combined\")\n                 if isinstance(combined_conf, (int, float)) and not math.isnan(combined_conf):\n                      confidence = float(combined_conf)\n                 else:\n                      numeric_scores = [v for v in fusion_model_confidence_data.values() if isinstance(v, (int, float)) and not math.isnan(v)]\n                      confidence = sum(numeric_scores) / len(numeric_scores) if numeric_scores else 0.0\n            elif isinstance(fusion_model_confidence_data, (int, float)) and not math.isnan(fusion_model_confidence_data):\n                 confidence = float(fusion_model_confidence_data)\n\n            confidence = max(0.0, min(1.0, confidence)) # Clamp [0, 1]\n\n            # Fallback confidence calculation if model confidence is low/zero\n            if confidence &lt;= 0.01:\n                 logger.debug(\"Fusion model did not provide significant confidence, calculating average from input phases.\")\n                 # Use helper to calculate avg confidence of phases that contributed to the input\n                 input_phase_names = list(outputs_to_fuse.keys())\n                 # Filter original outputs dict to only include those used for fusion\n                 fusion_source_outputs = {k: v for k, v in outputs.items() if k in input_phase_names}\n                 average_input_confidence = self._calculate_confidence(fusion_source_outputs)\n                 confidence = average_input_confidence\n                 logger.info(f\"Using average input confidence as fallback: {confidence:.3f}\")\n\n            execution_time = time.time() - start_time\n            logger.info(\n                f\"Ensemble Fusion aggregation completed in {execution_time:.2f}s using model '{fusion_model_id}'. \"\n                f\"Input phases fused: {len(outputs_to_fuse)}.\",\n                extra={\"final_confidence\": confidence}\n            )\n\n            # Prepare final result\n            aggregation_result = {\n                \"response\": fused_output_text,\n                \"confidence\": confidence,\n                \"fusion_model\": fusion_model_id,\n                \"source_outputs\": outputs_to_fuse\n            }\n\n            # Add trace\n            if trace_collector:\n                 model_exec_time = fusion_result_raw.get(\"total_time\", fusion_result_raw.get(\"generation_time\", 0))\n                 # Add trace for the fusion model call itself\n                 trace_collector.add_model_trace(\n                      model_id=fusion_model_id,\n                      input_prompt=fusion_prompt,\n                      output=fusion_result_raw,\n                      execution_time=model_exec_time,\n                      parameters={\"role\": \"fusion_aggregator\", **fusion_params_filtered}\n                 )\n                 # Add trace for the aggregation step\n                 trace_collector.add_aggregation_trace(\n                    strategy_name=self._strategy_name,\n                    inputs={\n                        \"phase_output_keys\": list(outputs.keys()),\n                        \"context_keys\": list(context.keys()),\n                    },\n                    output=aggregation_result,\n                    execution_time=execution_time,\n                    parameters={\n                        \"fusion_model_used\": fusion_model_id,\n                        \"fusion_template\": fusion_template_name,\n                        **fusion_params_filtered\n                    }\n                 )\n\n            return aggregation_result\n\n        except AggregationError as e:\n             logger.error(f\"Ensemble Fusion aggregation failed: {str(e)}\")\n             raise\n        except ModelError as e:\n             logger.error(f\"Model error during Ensemble Fusion: {str(e)}\")\n             raise AggregationError(f\"Ensemble Fusion failed due to model error: {str(e)}\") from e\n        except Exception as e:\n            logger.error(f\"Unexpected error during Ensemble Fusion aggregation: {str(e)}\", exc_info=True)\n            raise AggregationError(f\"Ensemble Fusion aggregation failed unexpectedly: {str(e)}\")\n\n\n    def _format_fusion_input(\n        self,\n        outputs_to_fuse: Dict[str, str],\n        context: Dict[str, Any]\n    ) -&gt; str:\n        \"\"\"Formats the extracted phase outputs into a structured string for the fusion prompt.\"\"\"\n        formatted_input_parts = []\n        # Determine order: Use context sequence if available, else dict order\n        phase_sequence = context.get(\"phase_sequence\", list(outputs_to_fuse.keys()))\n        ordered_phases = [p for p in phase_sequence if p in outputs_to_fuse]\n        remaining_phases = [p for p in outputs_to_fuse if p not in ordered_phases]\n        final_phase_order = ordered_phases + remaining_phases\n\n        for i, phase_name in enumerate(final_phase_order):\n             output_text = outputs_to_fuse[phase_name]\n             # Use clear separators and headers\n             header = f\"--- Input {i+1} (from Phase: {phase_name}) ---\"\n             formatted_input_parts.append(f\"{header}\\n{output_text}\")\n\n        # Join with double newline for separation\n        return \"\\n\\n\".join(formatted_input_parts)\n</code></pre>"},{"location":"api/aggregation/#ai_ensemble_suite.aggregation.EnsembleFusion.__init__","title":"<code>__init__(config_manager, strategy_name, model_manager=None, strategy_config_override=None)</code>","text":"<p>Initialize the EnsembleFusion aggregator.</p> Source code in <code>src\\ai_ensemble_suite\\aggregation\\ensemble_fusion.py</code> <pre><code>def __init__(\n    self,\n    config_manager: \"ConfigManager\",\n    strategy_name: str,\n    model_manager: Optional[\"ModelManager\"] = None,\n    strategy_config_override: Optional[Dict[str, Any]] = None\n) -&gt; None:\n    \"\"\"Initialize the EnsembleFusion aggregator.\"\"\"\n    super().__init__(config_manager, strategy_name, model_manager, strategy_config_override)\n    if self._model_manager is None:\n         logger.warning(f\"EnsembleFusion strategy '{self._strategy_name}' initialized without a ModelManager. Aggregation will likely fail.\")\n</code></pre>"},{"location":"api/aggregation/#ai_ensemble_suite.aggregation.EnsembleFusion.aggregate","title":"<code>aggregate(outputs, context, trace_collector=None)</code>  <code>async</code>","text":"<p>Aggregate phase outputs by using a specified model to fuse them.</p> Source code in <code>src\\ai_ensemble_suite\\aggregation\\ensemble_fusion.py</code> <pre><code>    async def aggregate(\n        self,\n        outputs: Dict[str, Dict[str, Any]],\n        context: Dict[str, Any],\n        trace_collector: Optional[TraceCollector] = None\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Aggregate phase outputs by using a specified model to fuse them.\"\"\"\n        start_time = time.time()\n        logger.debug(f\"Starting Ensemble Fusion aggregation using strategy '{self._strategy_name}'\")\n\n        if self._model_manager is None:\n            raise AggregationError(\"ModelManager is required for Ensemble Fusion but not available.\")\n\n        if not outputs:\n             raise AggregationError(\"No phase outputs provided for Ensemble Fusion.\")\n\n        try:\n            # Determine the Fusion Model\n            fusion_model_id = self._config.get(\"fusion_model\") # Key checked by schema\n            fusion_model_id = context.get(\"fusion_model\", fusion_model_id) # Allow context override\n\n            if not fusion_model_id:\n                 # Schema validation should require fusion_model, so this is unlikely\n                 logger.error(\"Configuration Error: 'fusion_model' is required but missing.\")\n                 raise AggregationError(\"Missing required 'fusion_model' configuration for Ensemble Fusion.\")\n\n            # Ensure the selected fusion model actually exists\n            try:\n                 fusion_model_instance = self._model_manager.get_model(fusion_model_id)\n                 logger.info(f\"Using fusion model: {fusion_model_id}\")\n            except ModelError:\n                 logger.error(f\"Specified fusion model '{fusion_model_id}' not found in ModelManager.\")\n                 raise AggregationError(f\"Fusion model '{fusion_model_id}' not available.\")\n\n\n            # Prepare Inputs for Fusion\n            outputs_to_fuse: Dict[str, str] = {}\n            for phase_name, phase_output_data in outputs.items():\n                extracted_text = self._extract_output(phase_output_data)\n                if extracted_text:\n                    outputs_to_fuse[phase_name] = extracted_text\n                else:\n                    logger.warning(f\"Could not extract usable text from phase '{phase_name}' for fusion input.\")\n\n            if not outputs_to_fuse:\n                 # Handle case where only one valid input exists - fusion might still be useful\n                 if len(outputs) == 1:\n                      logger.warning(\"Only one valid output found, using it directly instead of fusion.\")\n                      single_phase_name = list(outputs.keys())[0]\n                      single_output_data = outputs[single_phase_name]\n                      single_text = self._extract_output(single_output_data)\n                      single_confidence = self._calculate_confidence({single_phase_name: single_output_data})\n                      return {\n                           \"response\": single_text,\n                           \"confidence\": single_confidence,\n                           \"fusion_model\": None, # Indicate no fusion occurred\n                           \"source_outputs\": {single_phase_name: single_text}\n                      }\n                 else:\n                      raise AggregationError(\"No valid outputs found to provide as input for fusion.\")\n\n            # Format the collected outputs into a single string\n            fusion_input_str = self._format_fusion_input(outputs_to_fuse, context)\n\n            # Prepare and Run Fusion Prompt\n            fusion_template_name = self._config.get(\"fusion_template\", \"ensemble_fusion\") # Checked by schema\n\n            # Robust default template (in case formatting fails)\n            default_fusion_template = f\"\"\"Synthesize the following outputs into a single, high-quality response to the original query. Focus on integrating the best aspects, ensuring accuracy, clarity, and coherence. Avoid simply listing the inputs; create a unified final answer.\n\nORIGINAL QUERY:\n{{query}}\n\nINPUTS TO FUSE:\n{{fusion_input}}\n\nBased on the query and the provided inputs, generate a comprehensive and well-structured final response:\"\"\"\n\n            query_context = context.get(\"query\", \"N/A\")\n            if query_context == \"N/A\":\n                 logger.warning(\"Query not found in context for fusion prompt.\")\n\n            fusion_prompt = \"\"\n            try:\n                context_dict = {\"query\": query_context, \"fusion_input\": fusion_input_str}\n                fusion_prompt = self.render_template(fusion_template_name, context_dict)\n            except (ConfigurationError, ValidationError) as e:\n                 logger.warning(f\"Error formatting fusion template '{fusion_template_name}': {e}. Using default.\")\n                 try:\n                      fusion_prompt = default_fusion_template.format(query=query_context, fusion_input=fusion_input_str)\n                 except KeyError: # Catch issues with the default template itself\n                       logger.error(\"Failed to format even the default fusion template.\", exc_info=True)\n                       raise AggregationError(\"Failed to create fusion prompt.\") from e\n            except Exception as e:\n                 logger.error(f\"Unexpected error formatting fusion prompt: {e}\", exc_info=True)\n                 # Fallback to default on unexpected errors\n                 try:\n                      fusion_prompt = default_fusion_template.format(query=query_context, fusion_input=fusion_input_str)\n                 except KeyError:\n                       logger.error(\"Failed to format even the default fusion template on unexpected error.\", exc_info=True)\n                       raise AggregationError(\"Failed to create fusion prompt.\") from e\n\n\n            # Run the fusion model\n            logger.debug(f\"Sending fusion prompt to model '{fusion_model_id}'. Prompt length: {len(fusion_prompt)}\")\n            fusion_params = {\n                 \"temperature\": self._config.get(\"fusion_temperature\", 0.6),\n                 \"max_tokens\": self._config.get(\"fusion_max_tokens\", 2048),\n                 \"top_p\": self._config.get(\"fusion_top_p\"),\n                 \"top_k\": self._config.get(\"fusion_top_k\"),\n                 \"repeat_penalty\": self._config.get(\"fusion_repeat_penalty\")\n            }\n            fusion_params_filtered = {k: v for k, v in fusion_params.items() if v is not None}\n            logger.debug(f\"Fusion inference parameters: {fusion_params_filtered}\")\n\n            fusion_result_raw = await self._model_manager.run_inference(\n                model_id=fusion_model_id,\n                prompt=fusion_prompt,\n                compute_confidence=True,\n                **fusion_params_filtered\n            )\n            logger.debug(\"Received fusion result from model.\")\n\n\n            # Process Fusion Result\n            fused_output_text = fusion_result_raw.get(\"text\", \"\")\n            if not fused_output_text:\n                 logger.warning(f\"Fusion model '{fusion_model_id}' returned empty text.\")\n                 # Consider fallback - maybe return best input based on confidence?\n                 # For now, raise error if fusion genuinely failed.\n                 raise AggregationError(f\"Fusion model '{fusion_model_id}' failed to generate output.\")\n\n            # Determine confidence score\n            confidence = 0.0\n            fusion_model_confidence_data = fusion_result_raw.get(\"confidence\")\n\n            if isinstance(fusion_model_confidence_data, dict):\n                 combined_conf = fusion_model_confidence_data.get(\"combined\")\n                 if isinstance(combined_conf, (int, float)) and not math.isnan(combined_conf):\n                      confidence = float(combined_conf)\n                 else:\n                      numeric_scores = [v for v in fusion_model_confidence_data.values() if isinstance(v, (int, float)) and not math.isnan(v)]\n                      confidence = sum(numeric_scores) / len(numeric_scores) if numeric_scores else 0.0\n            elif isinstance(fusion_model_confidence_data, (int, float)) and not math.isnan(fusion_model_confidence_data):\n                 confidence = float(fusion_model_confidence_data)\n\n            confidence = max(0.0, min(1.0, confidence)) # Clamp [0, 1]\n\n            # Fallback confidence calculation if model confidence is low/zero\n            if confidence &lt;= 0.01:\n                 logger.debug(\"Fusion model did not provide significant confidence, calculating average from input phases.\")\n                 # Use helper to calculate avg confidence of phases that contributed to the input\n                 input_phase_names = list(outputs_to_fuse.keys())\n                 # Filter original outputs dict to only include those used for fusion\n                 fusion_source_outputs = {k: v for k, v in outputs.items() if k in input_phase_names}\n                 average_input_confidence = self._calculate_confidence(fusion_source_outputs)\n                 confidence = average_input_confidence\n                 logger.info(f\"Using average input confidence as fallback: {confidence:.3f}\")\n\n            execution_time = time.time() - start_time\n            logger.info(\n                f\"Ensemble Fusion aggregation completed in {execution_time:.2f}s using model '{fusion_model_id}'. \"\n                f\"Input phases fused: {len(outputs_to_fuse)}.\",\n                extra={\"final_confidence\": confidence}\n            )\n\n            # Prepare final result\n            aggregation_result = {\n                \"response\": fused_output_text,\n                \"confidence\": confidence,\n                \"fusion_model\": fusion_model_id,\n                \"source_outputs\": outputs_to_fuse\n            }\n\n            # Add trace\n            if trace_collector:\n                 model_exec_time = fusion_result_raw.get(\"total_time\", fusion_result_raw.get(\"generation_time\", 0))\n                 # Add trace for the fusion model call itself\n                 trace_collector.add_model_trace(\n                      model_id=fusion_model_id,\n                      input_prompt=fusion_prompt,\n                      output=fusion_result_raw,\n                      execution_time=model_exec_time,\n                      parameters={\"role\": \"fusion_aggregator\", **fusion_params_filtered}\n                 )\n                 # Add trace for the aggregation step\n                 trace_collector.add_aggregation_trace(\n                    strategy_name=self._strategy_name,\n                    inputs={\n                        \"phase_output_keys\": list(outputs.keys()),\n                        \"context_keys\": list(context.keys()),\n                    },\n                    output=aggregation_result,\n                    execution_time=execution_time,\n                    parameters={\n                        \"fusion_model_used\": fusion_model_id,\n                        \"fusion_template\": fusion_template_name,\n                        **fusion_params_filtered\n                    }\n                 )\n\n            return aggregation_result\n\n        except AggregationError as e:\n             logger.error(f\"Ensemble Fusion aggregation failed: {str(e)}\")\n             raise\n        except ModelError as e:\n             logger.error(f\"Model error during Ensemble Fusion: {str(e)}\")\n             raise AggregationError(f\"Ensemble Fusion failed due to model error: {str(e)}\") from e\n        except Exception as e:\n            logger.error(f\"Unexpected error during Ensemble Fusion aggregation: {str(e)}\", exc_info=True)\n            raise AggregationError(f\"Ensemble Fusion aggregation failed unexpectedly: {str(e)}\")\n</code></pre>"},{"location":"api/aggregation/#ai_ensemble_suite.aggregation.MultidimensionalVoting","title":"<code>MultidimensionalVoting</code>","text":"<p>               Bases: <code>BaseAggregator</code></p> <p>Multi-dimensional Voting aggregation strategy.</p> <p>Evaluates phase outputs along multiple configured dimensions (e.g., accuracy, clarity) using scores potentially generated by an evaluator model or extracted from context, then selects the output with the highest weighted score.</p> Source code in <code>src\\ai_ensemble_suite\\aggregation\\multidimensional_voting.py</code> <pre><code>class MultidimensionalVoting(BaseAggregator):\n    \"\"\"Multi-dimensional Voting aggregation strategy.\n\n    Evaluates phase outputs along multiple configured dimensions (e.g., accuracy,\n    clarity) using scores potentially generated by an evaluator model or extracted\n    from context, then selects the output with the highest weighted score.\n    \"\"\"\n\n    # Override __init__ to accept model_manager\n    def __init__(\n        self,\n        config_manager: \"ConfigManager\",\n        strategy_name: str,\n        model_manager: Optional[\"ModelManager\"] = None, # Required for evaluation\n        strategy_config_override: Optional[Dict[str, Any]] = None\n    ) -&gt; None:\n        \"\"\"Initialize the MultidimensionalVoting aggregator.\"\"\"\n        super().__init__(config_manager, strategy_name, model_manager, strategy_config_override)\n        # No specific initialization needed here currently\n\n\n    async def aggregate(\n        self,\n        outputs: Dict[str, Dict[str, Any]],\n        context: Dict[str, Any], # Context received here\n        trace_collector: Optional[TraceCollector] = None\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Aggregate outputs by evaluating multiple dimensions and voting.\n\n        Args:\n            outputs: Dictionary mapping phase names to their outputs.\n            context: Context information from collaboration phases (includes 'query').\n            trace_collector: Optional trace collector for gathering execution details.\n\n        Returns:\n            Dictionary containing:\n                response: The aggregated output (from the best phase) as a string.\n                confidence: The weighted score of the best phase as confidence.\n                best_phase: The name of the phase selected as best.\n                dimension_scores: Scores for each phase across dimensions (0.0-1.0).\n                weighted_scores: Final weighted score for each phase (0.0-1.0).\n\n        Raises:\n            AggregationError: If aggregation fails (e.g., no outputs, evaluation fails).\n        \"\"\"\n        start_time = time.time()\n        logger.debug(f\"Starting Multidimensional Voting aggregation for strategy '{self._strategy_name}'\")\n\n        if not outputs:\n             raise AggregationError(\"No phase outputs provided for Multidimensional Voting aggregation.\")\n\n        try:\n            # Get dimensions from strategy configuration\n            dimensions = self._config.get(\"dimensions\")\n            # Default dimensions if not specified or invalid\n            if not isinstance(dimensions, list) or not dimensions:\n                default_dims = [\"accuracy\", \"clarity\", \"completeness\", \"reasoning\"]\n                logger.warning(f\"Invalid or empty 'dimensions' in config, using defaults: {default_dims}\")\n                dimensions = default_dims\n\n            # Get dimension weights from strategy configuration\n            dimension_weights = self._config.get(\"dimension_weights\", {})\n            if not isinstance(dimension_weights, dict):\n                 logger.warning(\"Invalid 'dimension_weights' in config (must be dict), using equal weights.\")\n                 dimension_weights = {}\n\n            # Ensure all specified dimensions have a weight, defaulting to equal if needed\n            normalized_weights: Dict[str, float] = {}\n            dimensions_with_weight: List[str] = []\n            total_weight_specified = 0.0\n            for dim in dimensions:\n                weight = dimension_weights.get(dim, -1.0) # Use -1 to detect unspecified\n                if isinstance(weight, (int, float)) and weight &gt;= 0:\n                     normalized_weights[dim] = float(weight)\n                     dimensions_with_weight.append(dim)\n                     total_weight_specified += float(weight)\n                # Else: Skip dimensions with invalid or negative weights defined\n\n            # If no valid weights specified or sum is zero, use equal weights for *all* original dimensions\n            if not dimensions_with_weight or total_weight_specified &lt;= 0:\n                logger.debug(\"Using default equal weights for all dimensions.\")\n                equal_weight = 1.0 / len(dimensions) if dimensions else 0\n                normalized_weights = {dim: equal_weight for dim in dimensions}\n                dimensions_with_weight = dimensions\n                total_weight_after_default = sum(normalized_weights.values()) # Should be close to 1.0\n                logger.debug(f\"Default equal weights applied: {normalized_weights} (Sum: {total_weight_after_default})\")\n            # If weights were specified and sum &gt; 0, normalize them if they don't sum to 1\n            elif not math.isclose(total_weight_specified, 1.0):\n                 logger.debug(f\"Normalizing specified dimension weights from sum {total_weight_specified}\")\n                 normalized_weights = {dim: w / total_weight_specified for dim, w in normalized_weights.items()}\n                 total_weight_after_normalize = sum(normalized_weights.values())\n                 logger.debug(f\"Normalized weights: {normalized_weights} (Sum: {total_weight_after_normalize})\")\n\n\n            # --- Get scores for each phase and dimension ---\n            dimension_scores: Dict[str, Dict[str, float]] = {} # { phase_name: { dimension: score } }\n\n            # 1. Look for pre-evaluated scores within phase outputs\n            for phase_name, phase_output_data in outputs.items():\n                 if not isinstance(phase_output_data, dict):\n                      logger.debug(f\"Skipping phase '{phase_name}' output as it's not a dictionary.\")\n                      continue # Skip non-dict outputs\n\n                 phase_scores: Dict[str, float] = {}\n\n                 # Check common locations for scores (evaluations dict, specific keys, text parsing)\n                 # Prioritize 'evaluations' dict if present\n                 evals_dict = phase_output_data.get(\"evaluations\")\n                 if isinstance(evals_dict, dict):\n                      for dim in dimensions_with_weight: # Only look for relevant dimensions\n                           if dim in evals_dict and isinstance(evals_dict[dim], (int, float)):\n                                phase_scores[dim] = max(0.0, min(1.0, float(evals_dict[dim]))) # Normalize/clamp 0-1\n\n                 # Look for direct keys like 'accuracy_score' if not found above\n                 for dim in dimensions_with_weight:\n                      if dim not in phase_scores: # Only if not already found in 'evaluations'\n                           dim_key_score = f\"{dim}_score\"\n                           dim_key_rating = f\"{dim}_rating\" # Another common pattern\n                           score_val = None\n                           if dim_key_score in phase_output_data and isinstance(phase_output_data[dim_key_score], (int, float)):\n                                score_val = phase_output_data[dim_key_score]\n                           elif dim_key_rating in phase_output_data and isinstance(phase_output_data[dim_key_rating], (int, float)):\n                                score_val = phase_output_data[dim_key_rating]\n\n                           if score_val is not None:\n                                # Assume scores/ratings are 0-1 or 0-10, 1-10 etc. Normalize best guess.\n                                if score_val &gt; 1.0: score_val = score_val / 10.0 # Assume out of 10\n                                phase_scores[dim] = max(0.0, min(1.0, float(score_val)))\n\n\n                 # Attempt regex extraction from output text if scores still missing for some dims\n                 output_text_content = self._extract_output(phase_output_data)\n                 if output_text_content:\n                      for dim in dimensions_with_weight:\n                           if dim not in phase_scores: # Only parse if score not found yet\n                                # Regex looks for \"dimension: score/10\" patterns, more robustly\n                                score_pattern = rf'{re.escape(dim)}\\s*[:\\-]?\\s*score\\s*[:\\-]?\\s*(\\d+(?:\\.\\d+)?)\\s*/\\s*10'\n                                match = re.search(score_pattern, output_text_content, re.IGNORECASE)\n                                if not match: # Try alternate pattern like \"dimension: 8\" (assume out of 10)\n                                     score_pattern_alt = rf'{re.escape(dim)}\\s*[:\\-]?\\s*(\\d+(?:\\.\\d+)?)(?!\\s*/)' # Number not followed by /\n                                     match = re.search(score_pattern_alt, output_text_content, re.IGNORECASE)\n\n                                if match:\n                                    try:\n                                         score_val = float(match.group(1))\n                                         # Normalize if it seems to be out of 10\n                                         if score_val &gt; 1.0 or '/10' in match.group(0).lower():\n                                              score_val = score_val / 10.0\n                                         phase_scores[dim] = max(0.0, min(1.0, score_val)) # Normalize 0-1\n                                    except ValueError:\n                                         logger.warning(f\"Could not parse score for dim '{dim}' in phase '{phase_name}' output via regex.\")\n\n                 # Store scores found for this phase if any relevant ones were found\n                 if any(dim in phase_scores for dim in dimensions_with_weight):\n                      dimension_scores[phase_name] = phase_scores\n\n            logger.debug(f\"Initial dimension scores extracted: {dimension_scores}\")\n\n\n            # 2. If scores incomplete or missing, use evaluator model if configured\n            evaluator_model_id = self._config.get(\"evaluator_model\")\n            evaluator_model_id = context.get(\"evaluator_model\", evaluator_model_id) # Allow context override\n\n            # Determine if evaluation is needed\n            phases_needing_scores = set(outputs.keys()) - set(dimension_scores.keys())\n            phases_with_incomplete_scores = {\n                 name for name, scores in dimension_scores.items()\n                 if not all(dim in scores for dim in dimensions_with_weight)\n            }\n            needs_evaluation = bool(phases_needing_scores or phases_with_incomplete_scores)\n\n            if needs_evaluation and evaluator_model_id:\n                 if self._model_manager is None:\n                      logger.warning(\"Evaluator model specified, but ModelManager not available to aggregator. Cannot perform model-based evaluation.\")\n                 else:\n                      logger.info(f\"Performing dimension evaluation using model: {evaluator_model_id}\")\n                      try:\n                           # *** FIX: Pass context to _evaluate_phases ***\n                           # Pass only the dimensions that have weights assigned\n                           evaluated_scores = await self._evaluate_phases(\n                               outputs, dimensions_with_weight, evaluator_model_id, context, trace_collector\n                           )\n                           # Merge evaluated scores, filling gaps. Evaluated scores take precedence? Let's say yes.\n                           for phase_name, scores in evaluated_scores.items():\n                               if phase_name not in dimension_scores:\n                                    dimension_scores[phase_name] = {}\n                               # Update existing dict, overwriting with newly evaluated scores\n                               dimension_scores[phase_name].update(scores)\n                               logger.debug(f\"Updated/Added evaluated scores for phase '{phase_name}': {scores}\")\n\n                      except ModelError as e:\n                            logger.error(f\"Evaluation model '{evaluator_model_id}' failed: {e}. Continuing without its evaluation.\")\n                            # Continue without evaluated scores if evaluation fails\n                      except Exception as e:\n                           logger.error(f\"Unexpected error during phase evaluation using model '{evaluator_model_id}': {str(e)}\", exc_info=True)\n                           # Continue without evaluated scores\n\n\n            # 3. Fallback: Use confidence scores if dimension scores are STILL missing/incomplete\n            phases_still_needing_scores = set(outputs.keys()) - set(dimension_scores.keys())\n            phases_still_incomplete = {\n                 name for name, scores in dimension_scores.items()\n                 if not all(dim in scores for dim in dimensions_with_weight)\n            }\n\n            if phases_still_needing_scores or phases_still_incomplete:\n                 logger.debug(\"Dimension scores still incomplete after potential evaluation, using phase confidence scores as fallback.\")\n                 for phase_name, phase_output_data in outputs.items():\n                      # Check if scores are needed for this phase\n                      needs_score_fill = phase_name in phases_still_needing_scores or \\\n                                         (phase_name in phases_still_incomplete and isinstance(dimension_scores.get(phase_name), dict))\n\n                      if needs_score_fill:\n                           confidence = 0.0\n                           if isinstance(phase_output_data, dict) and \"confidence\" in phase_output_data:\n                                conf_val = phase_output_data[\"confidence\"]\n                                if isinstance(conf_val, (int, float)):\n                                     confidence = max(0.0, min(1.0, float(conf_val)))\n                                elif isinstance(conf_val, dict):\n                                     combined_conf = conf_val.get(\"combined\")\n                                     if isinstance(combined_conf, (int, float)):\n                                         confidence = max(0.0, min(1.0, float(combined_conf)))\n\n                           if confidence &gt; 0:\n                                if phase_name not in dimension_scores:\n                                     dimension_scores[phase_name] = {}\n                                # Fill in missing scores for dimensions_with_weight using confidence\n                                for dim in dimensions_with_weight:\n                                     if dim not in dimension_scores[phase_name]:\n                                          dimension_scores[phase_name][dim] = confidence\n                                logger.debug(f\"Using confidence {confidence:.3f} to fill missing dimension scores for phase '{phase_name}'.\")\n\n\n            # 4. Final Fallback: Assign default scores (e.g., 0.5) if still no scores for some phases\n            phases_finally_needing_scores = set(outputs.keys()) - set(dimension_scores.keys())\n            phases_finally_incomplete = {\n                 name for name, scores in dimension_scores.items()\n                 if not all(dim in scores for dim in dimensions_with_weight)\n            }\n\n            if phases_finally_needing_scores or phases_finally_incomplete:\n                 default_fallback_score = 0.5 # Use a neutral default\n                 logger.warning(f\"No scores or confidence available for some phases/dimensions, assigning default score ({default_fallback_score}).\")\n                 for phase_name in outputs:\n                      if phase_name not in dimension_scores:\n                          dimension_scores[phase_name] = {dim: default_fallback_score for dim in dimensions_with_weight}\n                      elif phase_name in phases_finally_incomplete: # Already has a dict, fill missing\n                           for dim in dimensions_with_weight:\n                                dimension_scores[phase_name].setdefault(dim, default_fallback_score)\n\n\n            # --- Calculate weighted scores for each phase ---\n            weighted_scores: Dict[str, float] = {}\n            if not dimension_scores: # Check if somehow still empty\n                 raise AggregationError(\"Could not determine dimension scores for any phase.\")\n\n            for phase_name, scores in dimension_scores.items():\n                 if phase_name not in outputs: continue # Skip if phase wasn't in original outputs\n\n                 current_weighted_score = 0.0\n                 current_total_weight = 0.0 # Track weight used *for this phase*\n\n                 # Calculate weighted sum using the normalized weights\n                 for dim, score in scores.items():\n                     if dim in normalized_weights: # Use only dimensions with defined, normalized weights\n                          weight = normalized_weights[dim]\n                          if weight &gt; 0: # Skip dimensions with zero or negative weight\n                               current_weighted_score += score * weight\n                               current_total_weight += weight\n                          # Else: Ignore dimensions without weight or with zero weight\n\n                 # Normalize score by total weight actually used for this phase's scoring\n                 # This handles cases where some dimensions might be missing scores or weights\n                 if current_total_weight &gt; 0:\n                       weighted_scores[phase_name] = current_weighted_score / current_total_weight\n                 elif scores: # If weights were zero/missing but scores existed, use simple average of available scores\n                       logger.warning(f\"Total applicable weight for phase '{phase_name}' was zero, using average score.\")\n                       valid_scores = [s for s in scores.values() if isinstance(s, (int, float))]\n                       weighted_scores[phase_name] = sum(valid_scores) / len(valid_scores) if valid_scores else 0.0\n                 else: # No scores and no applicable weights\n                      logger.warning(f\"No scores or applicable weights for phase '{phase_name}', assigning default weighted score 0.5.\")\n                      weighted_scores[phase_name] = 0.5\n\n            logger.debug(f\"Calculated final weighted scores: {weighted_scores}\")\n\n\n            # --- Select the best phase ---\n            if not weighted_scores:\n                raise AggregationError(\"No phases remaining after scoring for aggregation.\")\n\n            # Select the phase with the highest weighted score\n            # Use max(), handling potential ties by taking the first one found (dict order dependent in &lt;3.7)\n            # For more robust tie-breaking, could use confidence or other factors.\n            best_phase_name = max(weighted_scores, key=weighted_scores.get)\n            best_score = weighted_scores[best_phase_name] # This is the final normalized weighted score\n\n            # Extract the output content from the best phase\n            best_output_content = self._extract_output(outputs[best_phase_name])\n\n\n            execution_time = time.time() - start_time\n            logger.info(\n                f\"Multidimensional Voting aggregation completed in {execution_time:.2f}s. \"\n                f\"Best phase: '{best_phase_name}' (Weighted Score: {best_score:.3f})\",\n                extra={ \"dimensions_used\": list(normalized_weights.keys()) }\n            )\n\n            # Prepare final result dictionary\n            aggregation_result = {\n                \"response\": best_output_content,\n                \"confidence\": best_score, # Use the final weighted score as confidence\n                \"best_phase\": best_phase_name,\n                 # Include detailed scores for dimensions that had weights\n                \"dimension_scores\": dimension_scores,\n                \"weighted_scores\": weighted_scores # Include final weighted scores per phase\n            }\n\n            # Add trace if collector is provided\n            if trace_collector:\n                trace_collector.add_aggregation_trace(\n                    strategy_name=self._strategy_name,\n                    inputs={\n                        \"phase_output_keys\": list(outputs.keys()),\n                        \"context_keys\": list(context.keys()),\n                        \"configured_dimensions\": dimensions, # Original list from config\n                        \"configured_weights\": dimension_weights, # Original weights from config\n                    },\n                    output=aggregation_result,\n                    execution_time=execution_time,\n                    parameters={ # Parameters influencing the outcome\n                        \"effective_dimensions\": dimensions_with_weight, # Dimensions actually used\n                        \"normalized_weights\": normalized_weights, # Weights used in calculation\n                        \"evaluator_model\": evaluator_model_id\n                    }\n                )\n\n            return aggregation_result\n\n        except AggregationError as e:\n             logger.error(f\"Multidimensional voting aggregation failed: {str(e)}\")\n             raise # Re-raise known aggregation errors\n        except Exception as e:\n            logger.error(f\"Unexpected error during Multidimensional Voting aggregation: {str(e)}\", exc_info=True)\n            # Wrap unexpected errors in AggregationError\n            raise AggregationError(f\"Multidimensional Voting aggregation failed unexpectedly: {str(e)}\")\n\n    async def _evaluate_phases(\n            self,\n            outputs: Dict[str, Dict[str, Any]],\n            dimensions: List[str],\n            evaluator_model_id: str,\n            context: Dict[str, Any],  # Added missing context parameter\n            trace_collector: Optional[TraceCollector] = None\n    ) -&gt; Dict[str, Dict[str, float]]:\n        \"\"\"Evaluate phase outputs using a specified model along multiple dimensions.\n\n        Args:\n            outputs: Dictionary mapping phase names to their output data.\n            dimensions: List of dimension names (strings) to evaluate.\n            evaluator_model_id: ID of the model to use for evaluation.\n            context: Context dictionary (needed for the query).\n            trace_collector: Optional trace collector.\n\n        Returns:\n            Dictionary mapping phase names to their evaluated dimension scores (0.0-1.0).\n            Returns empty dict if evaluation fails significantly.\n\n        Raises:\n            ModelError: If the evaluation model fails.\n            AggregationError: If required components like ModelManager are missing.\n        \"\"\"\n        if self._model_manager is None:\n             raise AggregationError(\"ModelManager is required for evaluation but not available.\")\n\n        logger.debug(f\"Evaluating {len(outputs)} phases using model '{evaluator_model_id}' on dimensions: {dimensions}\")\n\n        # Format outputs for the evaluation prompt\n        formatted_outputs_for_prompt = \"\"\n        valid_outputs_to_eval = {} # Keep track of which outputs we could format\n        for phase_name, phase_output_data in outputs.items():\n            output_text = self._extract_output(phase_output_data)\n            if output_text: # Only include phases with extractable text\n                formatted_outputs_for_prompt += f\"\\n\\n## Output from Phase: {phase_name}\\n\\n{output_text}\\n\"\n                valid_outputs_to_eval[phase_name] = output_text\n            else:\n                 logger.warning(f\"Could not extract text from output of phase '{phase_name}' for evaluation.\")\n\n        if not formatted_outputs_for_prompt:\n             logger.warning(\"No valid outputs found to format for evaluation prompt.\")\n             return {}\n\n        # Create dimensions string for the prompt\n        dimensions_str = \", \".join(dimensions)\n\n        # Get evaluation prompt template from config (or use a robust default)\n        evaluation_template_name = self._config.get(\"evaluation_template\", \"multidimensional_evaluation\")\n\n        # Default template asking for scores 0-10\n        default_eval_template = f\"\"\"Please evaluate the following outputs based on the user query and the specified dimensions.\n\nUSER QUERY:\n{{query}}\n\nDIMENSIONS TO EVALUATE: {dimensions_str}\n\nFor each output below, provide a score from 0 to 10 for each dimension, where 0 is worst and 10 is best.\n\nOUTPUTS TO EVALUATE:\n{formatted_outputs_for_prompt}\n\nProvide your evaluation clearly for each phase, like this example:\n\n## Evaluation for Phase: [phase_name_1]\n- {dimensions[0]}: [score]/10\n- {dimensions[1]}: [score]/10\n...\n\n## Evaluation for Phase: [phase_name_2]\n- {dimensions[0]}: [score]/10\n...\n(End your response after evaluating all phases)\n\"\"\"\n        # Try formatting using ConfigManager, fallback to default if template missing/fails\n        evaluation_prompt = \"\"\n        try:\n             # *** FIX: Use context parameter to get query ***\n             query_context = context.get(\"query\", \"N/A\") # Default if query missing in context\n             context_dict = {\n                 \"query\": query_context,\n                 \"outputs_section\": formatted_outputs_for_prompt, # Keep the variable name simple\n                 \"dimensions_list\": dimensions_str\n            }\n             evaluation_prompt = self._config_manager.render_template(evaluation_template_name, context_dict)\n\n        except ConfigurationError:\n             logger.warning(f\"Evaluation template '{evaluation_template_name}' not found, using default.\")\n             # *** FIX: Use context parameter to get query for default template ***\n             query_context = context.get(\"query\", \"N/A\")\n             # Format the default template string directly\n             evaluation_prompt = default_eval_template.format(query=query_context)\n        except (KeyError, ValidationError) as e: # Catch missing keys or other format errors\n             logger.warning(f\"Error formatting template '{evaluation_template_name}': {e}. Using default.\")\n             # *** FIX: Use context parameter to get query for default template ***\n             query_context = context.get(\"query\", \"N/A\")\n             evaluation_prompt = default_eval_template.format(query=query_context)\n        except Exception as e:\n             logger.error(f\"Unexpected error formatting evaluation prompt: {e}\", exc_info=True)\n             # *** FIX: Use context parameter to get query for default template ***\n             query_context = context.get(\"query\", \"N/A\")\n             evaluation_prompt = default_eval_template.format(query=query_context)\n\n\n        # Run evaluator model\n        try:\n            logger.debug(f\"Sending evaluation prompt to model '{evaluator_model_id}'.\")\n            # Use model manager's run_inference\n            evaluation_result_raw = await self._model_manager.run_inference(\n                model_id=evaluator_model_id,\n                prompt=evaluation_prompt,\n                # Add params suitable for evaluation (e.g., lower temp)\n                temperature=self._config.get(\"evaluation_temperature\", 0.2), # More deterministic eval\n                max_tokens=self._config.get(\"evaluation_max_tokens\", 1024) # Allow enough space\n            )\n            logger.debug(\"Received evaluation result from model.\")\n\n            # Add trace for the evaluation model call\n            if trace_collector:\n                trace_collector.add_model_trace(\n                    model_id=evaluator_model_id,\n                    input_prompt=evaluation_prompt, # Can be large\n                    output=evaluation_result_raw, # Store raw eval output\n                    execution_time=evaluation_result_raw.get(\"total_time\", 0),\n                    parameters={\"role\": \"dimension_evaluator\", \"evaluated_dimensions\": dimensions}\n                )\n\n            evaluation_text = evaluation_result_raw.get(\"text\", \"\")\n            if not evaluation_text:\n                 logger.warning(f\"Evaluation model '{evaluator_model_id}' returned empty text.\")\n                 return {}\n\n\n            # --- Parse scores from the evaluation text ---\n            parsed_dimension_scores: Dict[str, Dict[str, float]] = {}\n            # Iterate through the phases we expected to evaluate\n            for phase_name in valid_outputs_to_eval.keys():\n                phase_scores: Dict[str, float] = {}\n                # Look for the section corresponding to this phase's evaluation\n                # Make regex more robust: allows for variations in header, non-greedy match\n                phase_pattern = rf\"^\\s*(?:#+)?\\s*(?:Evaluation for Phase|Output from Phase|Phase):\\s*{re.escape(phase_name)}\\s*$(.*?)(?:^\\s*(?:#+)?\\s*(?:Evaluation for Phase|Output from Phase|Phase):|\\Z)\"\n                phase_match = re.search(phase_pattern, evaluation_text, re.IGNORECASE | re.MULTILINE | re.DOTALL)\n\n                if phase_match:\n                    phase_section_text = phase_match.group(1)\n                    logger.debug(f\"Found evaluation section for phase '{phase_name}'.\")\n\n                    # Extract scores for each dimension within this section\n                    for dim in dimensions:\n                         # Regex: dimension name, optional colon/hyphen, space, score / 10\n                         score_pattern = rf'^\\s*[\\*\\-]?\\s*{re.escape(dim)}\\s*[:\\-]?\\s*(\\d+(?:\\.\\d+)?)\\s*/\\s*10'\n                         score_match = re.search(score_pattern, phase_section_text, re.IGNORECASE | re.MULTILINE)\n                         if not score_match: # Try alt pattern without /10\n                             score_pattern_alt = rf'^\\s*[\\*\\-]?\\s*{re.escape(dim)}\\s*[:\\-]?\\s*(\\d+(?:\\.\\d+)?)(?![\\d\\.]|\\s*/)'\n                             score_match = re.search(score_pattern_alt, phase_section_text, re.IGNORECASE | re.MULTILINE)\n\n                         if score_match:\n                             try:\n                                  score_val = float(score_match.group(1))\n                                   # Normalize if likely out of 10\n                                  if score_val &gt; 1.0 or '/10' in score_match.group(0).lower():\n                                       score_val /= 10.0\n                                  phase_scores[dim] = max(0.0, min(1.0, score_val)) # Clamp score [0, 1]\n                                  logger.debug(f\"Parsed score for '{phase_name}' -&gt; '{dim}': {phase_scores[dim]:.2f}\")\n                             except ValueError:\n                                 logger.warning(f\"Could not parse score '{score_match.group(1)}' for dim '{dim}' in phase '{phase_name}'.\")\n                         # else:\n                         #     logger.debug(f\"Score pattern not found for dim '{dim}' in phase '{phase_name}' section.\")\n\n                # else:\n                #     logger.debug(f\"Evaluation section pattern not found for phase '{phase_name}'.\")\n\n\n                # Add scores found for this phase to the result if any were parsed\n                if phase_scores:\n                    parsed_dimension_scores[phase_name] = phase_scores\n\n            if not parsed_dimension_scores:\n                 logger.warning(\"Failed to parse any scores from the evaluation model's output.\")\n\n            return parsed_dimension_scores\n\n        except ModelError as e:\n             # Specific error from the model run\n             logger.error(f\"Evaluation model '{evaluator_model_id}' failed during inference: {str(e)}\")\n             raise # Re-raise ModelError to be caught by the main aggregate method\n        except Exception as e:\n            # Unexpected errors during the evaluation process\n            logger.error(f\"Unexpected error during phase evaluation process: {str(e)}\", exc_info=True)\n            # Raise as AggregationError to indicate evaluation step failed\n            raise AggregationError(f\"Unexpected error during evaluation: {str(e)}\")\n</code></pre>"},{"location":"api/aggregation/#ai_ensemble_suite.aggregation.MultidimensionalVoting.__init__","title":"<code>__init__(config_manager, strategy_name, model_manager=None, strategy_config_override=None)</code>","text":"<p>Initialize the MultidimensionalVoting aggregator.</p> Source code in <code>src\\ai_ensemble_suite\\aggregation\\multidimensional_voting.py</code> <pre><code>def __init__(\n    self,\n    config_manager: \"ConfigManager\",\n    strategy_name: str,\n    model_manager: Optional[\"ModelManager\"] = None, # Required for evaluation\n    strategy_config_override: Optional[Dict[str, Any]] = None\n) -&gt; None:\n    \"\"\"Initialize the MultidimensionalVoting aggregator.\"\"\"\n    super().__init__(config_manager, strategy_name, model_manager, strategy_config_override)\n</code></pre>"},{"location":"api/aggregation/#ai_ensemble_suite.aggregation.MultidimensionalVoting.aggregate","title":"<code>aggregate(outputs, context, trace_collector=None)</code>  <code>async</code>","text":"<p>Aggregate outputs by evaluating multiple dimensions and voting.</p> <p>Parameters:</p> Name Type Description Default <code>outputs</code> <code>Dict[str, Dict[str, Any]]</code> <p>Dictionary mapping phase names to their outputs.</p> required <code>context</code> <code>Dict[str, Any]</code> <p>Context information from collaboration phases (includes 'query').</p> required <code>trace_collector</code> <code>Optional[TraceCollector]</code> <p>Optional trace collector for gathering execution details.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary containing: response: The aggregated output (from the best phase) as a string. confidence: The weighted score of the best phase as confidence. best_phase: The name of the phase selected as best. dimension_scores: Scores for each phase across dimensions (0.0-1.0). weighted_scores: Final weighted score for each phase (0.0-1.0).</p> <p>Raises:</p> Type Description <code>AggregationError</code> <p>If aggregation fails (e.g., no outputs, evaluation fails).</p> Source code in <code>src\\ai_ensemble_suite\\aggregation\\multidimensional_voting.py</code> <pre><code>async def aggregate(\n    self,\n    outputs: Dict[str, Dict[str, Any]],\n    context: Dict[str, Any], # Context received here\n    trace_collector: Optional[TraceCollector] = None\n) -&gt; Dict[str, Any]:\n    \"\"\"Aggregate outputs by evaluating multiple dimensions and voting.\n\n    Args:\n        outputs: Dictionary mapping phase names to their outputs.\n        context: Context information from collaboration phases (includes 'query').\n        trace_collector: Optional trace collector for gathering execution details.\n\n    Returns:\n        Dictionary containing:\n            response: The aggregated output (from the best phase) as a string.\n            confidence: The weighted score of the best phase as confidence.\n            best_phase: The name of the phase selected as best.\n            dimension_scores: Scores for each phase across dimensions (0.0-1.0).\n            weighted_scores: Final weighted score for each phase (0.0-1.0).\n\n    Raises:\n        AggregationError: If aggregation fails (e.g., no outputs, evaluation fails).\n    \"\"\"\n    start_time = time.time()\n    logger.debug(f\"Starting Multidimensional Voting aggregation for strategy '{self._strategy_name}'\")\n\n    if not outputs:\n         raise AggregationError(\"No phase outputs provided for Multidimensional Voting aggregation.\")\n\n    try:\n        # Get dimensions from strategy configuration\n        dimensions = self._config.get(\"dimensions\")\n        # Default dimensions if not specified or invalid\n        if not isinstance(dimensions, list) or not dimensions:\n            default_dims = [\"accuracy\", \"clarity\", \"completeness\", \"reasoning\"]\n            logger.warning(f\"Invalid or empty 'dimensions' in config, using defaults: {default_dims}\")\n            dimensions = default_dims\n\n        # Get dimension weights from strategy configuration\n        dimension_weights = self._config.get(\"dimension_weights\", {})\n        if not isinstance(dimension_weights, dict):\n             logger.warning(\"Invalid 'dimension_weights' in config (must be dict), using equal weights.\")\n             dimension_weights = {}\n\n        # Ensure all specified dimensions have a weight, defaulting to equal if needed\n        normalized_weights: Dict[str, float] = {}\n        dimensions_with_weight: List[str] = []\n        total_weight_specified = 0.0\n        for dim in dimensions:\n            weight = dimension_weights.get(dim, -1.0) # Use -1 to detect unspecified\n            if isinstance(weight, (int, float)) and weight &gt;= 0:\n                 normalized_weights[dim] = float(weight)\n                 dimensions_with_weight.append(dim)\n                 total_weight_specified += float(weight)\n            # Else: Skip dimensions with invalid or negative weights defined\n\n        # If no valid weights specified or sum is zero, use equal weights for *all* original dimensions\n        if not dimensions_with_weight or total_weight_specified &lt;= 0:\n            logger.debug(\"Using default equal weights for all dimensions.\")\n            equal_weight = 1.0 / len(dimensions) if dimensions else 0\n            normalized_weights = {dim: equal_weight for dim in dimensions}\n            dimensions_with_weight = dimensions\n            total_weight_after_default = sum(normalized_weights.values()) # Should be close to 1.0\n            logger.debug(f\"Default equal weights applied: {normalized_weights} (Sum: {total_weight_after_default})\")\n        # If weights were specified and sum &gt; 0, normalize them if they don't sum to 1\n        elif not math.isclose(total_weight_specified, 1.0):\n             logger.debug(f\"Normalizing specified dimension weights from sum {total_weight_specified}\")\n             normalized_weights = {dim: w / total_weight_specified for dim, w in normalized_weights.items()}\n             total_weight_after_normalize = sum(normalized_weights.values())\n             logger.debug(f\"Normalized weights: {normalized_weights} (Sum: {total_weight_after_normalize})\")\n\n\n        # --- Get scores for each phase and dimension ---\n        dimension_scores: Dict[str, Dict[str, float]] = {} # { phase_name: { dimension: score } }\n\n        # 1. Look for pre-evaluated scores within phase outputs\n        for phase_name, phase_output_data in outputs.items():\n             if not isinstance(phase_output_data, dict):\n                  logger.debug(f\"Skipping phase '{phase_name}' output as it's not a dictionary.\")\n                  continue # Skip non-dict outputs\n\n             phase_scores: Dict[str, float] = {}\n\n             # Check common locations for scores (evaluations dict, specific keys, text parsing)\n             # Prioritize 'evaluations' dict if present\n             evals_dict = phase_output_data.get(\"evaluations\")\n             if isinstance(evals_dict, dict):\n                  for dim in dimensions_with_weight: # Only look for relevant dimensions\n                       if dim in evals_dict and isinstance(evals_dict[dim], (int, float)):\n                            phase_scores[dim] = max(0.0, min(1.0, float(evals_dict[dim]))) # Normalize/clamp 0-1\n\n             # Look for direct keys like 'accuracy_score' if not found above\n             for dim in dimensions_with_weight:\n                  if dim not in phase_scores: # Only if not already found in 'evaluations'\n                       dim_key_score = f\"{dim}_score\"\n                       dim_key_rating = f\"{dim}_rating\" # Another common pattern\n                       score_val = None\n                       if dim_key_score in phase_output_data and isinstance(phase_output_data[dim_key_score], (int, float)):\n                            score_val = phase_output_data[dim_key_score]\n                       elif dim_key_rating in phase_output_data and isinstance(phase_output_data[dim_key_rating], (int, float)):\n                            score_val = phase_output_data[dim_key_rating]\n\n                       if score_val is not None:\n                            # Assume scores/ratings are 0-1 or 0-10, 1-10 etc. Normalize best guess.\n                            if score_val &gt; 1.0: score_val = score_val / 10.0 # Assume out of 10\n                            phase_scores[dim] = max(0.0, min(1.0, float(score_val)))\n\n\n             # Attempt regex extraction from output text if scores still missing for some dims\n             output_text_content = self._extract_output(phase_output_data)\n             if output_text_content:\n                  for dim in dimensions_with_weight:\n                       if dim not in phase_scores: # Only parse if score not found yet\n                            # Regex looks for \"dimension: score/10\" patterns, more robustly\n                            score_pattern = rf'{re.escape(dim)}\\s*[:\\-]?\\s*score\\s*[:\\-]?\\s*(\\d+(?:\\.\\d+)?)\\s*/\\s*10'\n                            match = re.search(score_pattern, output_text_content, re.IGNORECASE)\n                            if not match: # Try alternate pattern like \"dimension: 8\" (assume out of 10)\n                                 score_pattern_alt = rf'{re.escape(dim)}\\s*[:\\-]?\\s*(\\d+(?:\\.\\d+)?)(?!\\s*/)' # Number not followed by /\n                                 match = re.search(score_pattern_alt, output_text_content, re.IGNORECASE)\n\n                            if match:\n                                try:\n                                     score_val = float(match.group(1))\n                                     # Normalize if it seems to be out of 10\n                                     if score_val &gt; 1.0 or '/10' in match.group(0).lower():\n                                          score_val = score_val / 10.0\n                                     phase_scores[dim] = max(0.0, min(1.0, score_val)) # Normalize 0-1\n                                except ValueError:\n                                     logger.warning(f\"Could not parse score for dim '{dim}' in phase '{phase_name}' output via regex.\")\n\n             # Store scores found for this phase if any relevant ones were found\n             if any(dim in phase_scores for dim in dimensions_with_weight):\n                  dimension_scores[phase_name] = phase_scores\n\n        logger.debug(f\"Initial dimension scores extracted: {dimension_scores}\")\n\n\n        # 2. If scores incomplete or missing, use evaluator model if configured\n        evaluator_model_id = self._config.get(\"evaluator_model\")\n        evaluator_model_id = context.get(\"evaluator_model\", evaluator_model_id) # Allow context override\n\n        # Determine if evaluation is needed\n        phases_needing_scores = set(outputs.keys()) - set(dimension_scores.keys())\n        phases_with_incomplete_scores = {\n             name for name, scores in dimension_scores.items()\n             if not all(dim in scores for dim in dimensions_with_weight)\n        }\n        needs_evaluation = bool(phases_needing_scores or phases_with_incomplete_scores)\n\n        if needs_evaluation and evaluator_model_id:\n             if self._model_manager is None:\n                  logger.warning(\"Evaluator model specified, but ModelManager not available to aggregator. Cannot perform model-based evaluation.\")\n             else:\n                  logger.info(f\"Performing dimension evaluation using model: {evaluator_model_id}\")\n                  try:\n                       # *** FIX: Pass context to _evaluate_phases ***\n                       # Pass only the dimensions that have weights assigned\n                       evaluated_scores = await self._evaluate_phases(\n                           outputs, dimensions_with_weight, evaluator_model_id, context, trace_collector\n                       )\n                       # Merge evaluated scores, filling gaps. Evaluated scores take precedence? Let's say yes.\n                       for phase_name, scores in evaluated_scores.items():\n                           if phase_name not in dimension_scores:\n                                dimension_scores[phase_name] = {}\n                           # Update existing dict, overwriting with newly evaluated scores\n                           dimension_scores[phase_name].update(scores)\n                           logger.debug(f\"Updated/Added evaluated scores for phase '{phase_name}': {scores}\")\n\n                  except ModelError as e:\n                        logger.error(f\"Evaluation model '{evaluator_model_id}' failed: {e}. Continuing without its evaluation.\")\n                        # Continue without evaluated scores if evaluation fails\n                  except Exception as e:\n                       logger.error(f\"Unexpected error during phase evaluation using model '{evaluator_model_id}': {str(e)}\", exc_info=True)\n                       # Continue without evaluated scores\n\n\n        # 3. Fallback: Use confidence scores if dimension scores are STILL missing/incomplete\n        phases_still_needing_scores = set(outputs.keys()) - set(dimension_scores.keys())\n        phases_still_incomplete = {\n             name for name, scores in dimension_scores.items()\n             if not all(dim in scores for dim in dimensions_with_weight)\n        }\n\n        if phases_still_needing_scores or phases_still_incomplete:\n             logger.debug(\"Dimension scores still incomplete after potential evaluation, using phase confidence scores as fallback.\")\n             for phase_name, phase_output_data in outputs.items():\n                  # Check if scores are needed for this phase\n                  needs_score_fill = phase_name in phases_still_needing_scores or \\\n                                     (phase_name in phases_still_incomplete and isinstance(dimension_scores.get(phase_name), dict))\n\n                  if needs_score_fill:\n                       confidence = 0.0\n                       if isinstance(phase_output_data, dict) and \"confidence\" in phase_output_data:\n                            conf_val = phase_output_data[\"confidence\"]\n                            if isinstance(conf_val, (int, float)):\n                                 confidence = max(0.0, min(1.0, float(conf_val)))\n                            elif isinstance(conf_val, dict):\n                                 combined_conf = conf_val.get(\"combined\")\n                                 if isinstance(combined_conf, (int, float)):\n                                     confidence = max(0.0, min(1.0, float(combined_conf)))\n\n                       if confidence &gt; 0:\n                            if phase_name not in dimension_scores:\n                                 dimension_scores[phase_name] = {}\n                            # Fill in missing scores for dimensions_with_weight using confidence\n                            for dim in dimensions_with_weight:\n                                 if dim not in dimension_scores[phase_name]:\n                                      dimension_scores[phase_name][dim] = confidence\n                            logger.debug(f\"Using confidence {confidence:.3f} to fill missing dimension scores for phase '{phase_name}'.\")\n\n\n        # 4. Final Fallback: Assign default scores (e.g., 0.5) if still no scores for some phases\n        phases_finally_needing_scores = set(outputs.keys()) - set(dimension_scores.keys())\n        phases_finally_incomplete = {\n             name for name, scores in dimension_scores.items()\n             if not all(dim in scores for dim in dimensions_with_weight)\n        }\n\n        if phases_finally_needing_scores or phases_finally_incomplete:\n             default_fallback_score = 0.5 # Use a neutral default\n             logger.warning(f\"No scores or confidence available for some phases/dimensions, assigning default score ({default_fallback_score}).\")\n             for phase_name in outputs:\n                  if phase_name not in dimension_scores:\n                      dimension_scores[phase_name] = {dim: default_fallback_score for dim in dimensions_with_weight}\n                  elif phase_name in phases_finally_incomplete: # Already has a dict, fill missing\n                       for dim in dimensions_with_weight:\n                            dimension_scores[phase_name].setdefault(dim, default_fallback_score)\n\n\n        # --- Calculate weighted scores for each phase ---\n        weighted_scores: Dict[str, float] = {}\n        if not dimension_scores: # Check if somehow still empty\n             raise AggregationError(\"Could not determine dimension scores for any phase.\")\n\n        for phase_name, scores in dimension_scores.items():\n             if phase_name not in outputs: continue # Skip if phase wasn't in original outputs\n\n             current_weighted_score = 0.0\n             current_total_weight = 0.0 # Track weight used *for this phase*\n\n             # Calculate weighted sum using the normalized weights\n             for dim, score in scores.items():\n                 if dim in normalized_weights: # Use only dimensions with defined, normalized weights\n                      weight = normalized_weights[dim]\n                      if weight &gt; 0: # Skip dimensions with zero or negative weight\n                           current_weighted_score += score * weight\n                           current_total_weight += weight\n                      # Else: Ignore dimensions without weight or with zero weight\n\n             # Normalize score by total weight actually used for this phase's scoring\n             # This handles cases where some dimensions might be missing scores or weights\n             if current_total_weight &gt; 0:\n                   weighted_scores[phase_name] = current_weighted_score / current_total_weight\n             elif scores: # If weights were zero/missing but scores existed, use simple average of available scores\n                   logger.warning(f\"Total applicable weight for phase '{phase_name}' was zero, using average score.\")\n                   valid_scores = [s for s in scores.values() if isinstance(s, (int, float))]\n                   weighted_scores[phase_name] = sum(valid_scores) / len(valid_scores) if valid_scores else 0.0\n             else: # No scores and no applicable weights\n                  logger.warning(f\"No scores or applicable weights for phase '{phase_name}', assigning default weighted score 0.5.\")\n                  weighted_scores[phase_name] = 0.5\n\n        logger.debug(f\"Calculated final weighted scores: {weighted_scores}\")\n\n\n        # --- Select the best phase ---\n        if not weighted_scores:\n            raise AggregationError(\"No phases remaining after scoring for aggregation.\")\n\n        # Select the phase with the highest weighted score\n        # Use max(), handling potential ties by taking the first one found (dict order dependent in &lt;3.7)\n        # For more robust tie-breaking, could use confidence or other factors.\n        best_phase_name = max(weighted_scores, key=weighted_scores.get)\n        best_score = weighted_scores[best_phase_name] # This is the final normalized weighted score\n\n        # Extract the output content from the best phase\n        best_output_content = self._extract_output(outputs[best_phase_name])\n\n\n        execution_time = time.time() - start_time\n        logger.info(\n            f\"Multidimensional Voting aggregation completed in {execution_time:.2f}s. \"\n            f\"Best phase: '{best_phase_name}' (Weighted Score: {best_score:.3f})\",\n            extra={ \"dimensions_used\": list(normalized_weights.keys()) }\n        )\n\n        # Prepare final result dictionary\n        aggregation_result = {\n            \"response\": best_output_content,\n            \"confidence\": best_score, # Use the final weighted score as confidence\n            \"best_phase\": best_phase_name,\n             # Include detailed scores for dimensions that had weights\n            \"dimension_scores\": dimension_scores,\n            \"weighted_scores\": weighted_scores # Include final weighted scores per phase\n        }\n\n        # Add trace if collector is provided\n        if trace_collector:\n            trace_collector.add_aggregation_trace(\n                strategy_name=self._strategy_name,\n                inputs={\n                    \"phase_output_keys\": list(outputs.keys()),\n                    \"context_keys\": list(context.keys()),\n                    \"configured_dimensions\": dimensions, # Original list from config\n                    \"configured_weights\": dimension_weights, # Original weights from config\n                },\n                output=aggregation_result,\n                execution_time=execution_time,\n                parameters={ # Parameters influencing the outcome\n                    \"effective_dimensions\": dimensions_with_weight, # Dimensions actually used\n                    \"normalized_weights\": normalized_weights, # Weights used in calculation\n                    \"evaluator_model\": evaluator_model_id\n                }\n            )\n\n        return aggregation_result\n\n    except AggregationError as e:\n         logger.error(f\"Multidimensional voting aggregation failed: {str(e)}\")\n         raise # Re-raise known aggregation errors\n    except Exception as e:\n        logger.error(f\"Unexpected error during Multidimensional Voting aggregation: {str(e)}\", exc_info=True)\n        # Wrap unexpected errors in AggregationError\n        raise AggregationError(f\"Multidimensional Voting aggregation failed unexpectedly: {str(e)}\")\n</code></pre>"},{"location":"api/aggregation/#ai_ensemble_suite.aggregation.SequentialRefinement","title":"<code>SequentialRefinement</code>","text":"<p>               Bases: <code>BaseAggregator</code></p> <p>Sequential Refinement aggregation strategy.</p> <p>Assumes phases run in a sequence where later phases refine earlier ones. This strategy simply selects the output of the designated 'final' phase.</p> Source code in <code>src\\ai_ensemble_suite\\aggregation\\sequential_refinement.py</code> <pre><code>class SequentialRefinement(BaseAggregator):\n    \"\"\"Sequential Refinement aggregation strategy.\n\n    Assumes phases run in a sequence where later phases refine earlier ones.\n    This strategy simply selects the output of the designated 'final' phase.\n    \"\"\"\n\n    # Override __init__ for consistency, though model_manager isn't used here\n    def __init__(\n        self,\n        config_manager: \"ConfigManager\",\n        strategy_name: str,\n        model_manager: Optional[\"ModelManager\"] = None,\n        strategy_config_override: Optional[Dict[str, Any]] = None\n    ) -&gt; None:\n        \"\"\"Initialize the SequentialRefinement aggregator.\"\"\"\n        super().__init__(config_manager, strategy_name, model_manager, strategy_config_override)\n        # No specific init needed\n\n\n    async def aggregate(\n        self,\n        outputs: Dict[str, Dict[str, Any]],\n        context: Dict[str, Any],\n        trace_collector: Optional[TraceCollector] = None\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Aggregate by selecting the output from the designated final phase.\n\n        Args:\n            outputs: Dictionary mapping phase names to their output data.\n            context: Context information (used to infer final phase if not configured).\n            trace_collector: Optional trace collector.\n\n        Returns:\n            Dictionary containing:\n                response: The text output from the final phase.\n                confidence: The confidence score associated with the final phase output.\n                final_phase: The name of the phase selected as final.\n\n        Raises:\n            AggregationError: If no final phase can be determined or its output is missing.\n        \"\"\"\n        start_time = time.time()\n        logger.debug(f\"Starting Sequential Refinement aggregation for strategy '{self._strategy_name}'\")\n\n        if not outputs:\n             raise AggregationError(\"No phase outputs provided for Sequential Refinement aggregation.\")\n\n        try:\n            final_phase_name: Optional[str] = None\n\n            # --- Determine the Final Phase ---\n            # 1. Check strategy configuration for 'final_phase'\n            configured_final_phase = self.get_final_phase() # Method uses self._config\n            if configured_final_phase and configured_final_phase in outputs:\n                logger.debug(f\"Using configured final_phase: '{configured_final_phase}'\")\n                final_phase_name = configured_final_phase\n\n            # 2. Infer from 'sequence' key in strategy config if final_phase not set/found\n            if not final_phase_name:\n                sequence = self._config.get(\"sequence\", [])\n                if isinstance(sequence, list) and sequence:\n                    potential_final = sequence[-1]\n                    if potential_final in outputs:\n                        logger.debug(f\"Inferred final phase from config sequence: '{potential_final}'\")\n                        final_phase_name = potential_final\n\n            # 3. Infer from 'phase_sequence' in the main context if still not found\n            if not final_phase_name:\n                context_sequence = context.get(\"phase_sequence\", [])\n                if isinstance(context_sequence, list) and context_sequence:\n                    potential_final = context_sequence[-1]\n                    if potential_final in outputs:\n                        logger.debug(f\"Inferred final phase from context sequence: '{potential_final}'\")\n                        final_phase_name = potential_final\n\n            # 4. Fallback: Look for phases named like 'refinement', 'integration', 'final' etc.\n            if not final_phase_name:\n                 candidate_suffixes = [\"refinement\", \"integration\", \"final\", \"synthesis\", \"summary\", \"committee\"]\n                 # Check in reverse order of phase execution if possible\n                 phases_to_check = reversed(context.get(\"phase_sequence\", list(outputs.keys())))\n                 for phase_name_to_check in phases_to_check:\n                      if phase_name_to_check in outputs:\n                           lower_name = phase_name_to_check.lower()\n                           if any(suffix in lower_name for suffix in candidate_suffixes):\n                                logger.debug(f\"Using phase '{phase_name_to_check}' as likely final phase based on name.\")\n                                final_phase_name = phase_name_to_check\n                                break\n\n            # 5. Last Resort: If still no final phase determined, use the last available output based on context sequence or dict order\n            if not final_phase_name:\n                phases_in_order = context.get(\"phase_sequence\", list(outputs.keys()))\n                # Find the last phase in the order that actually exists in outputs\n                for potential_final in reversed(phases_in_order):\n                     if potential_final in outputs:\n                          logger.warning(f\"Could not determine specific final phase, using last available phase: '{potential_final}'\")\n                          final_phase_name = potential_final\n                          break\n                # If even that fails (e.g., sequence empty and outputs empty, though caught earlier)\n                if not final_phase_name:\n                     raise AggregationError(\"Cannot determine final phase: No outputs available or sequence context missing.\")\n\n\n            # --- Extract Output and Confidence from Final Phase ---\n            logger.info(f\"Selected final phase for aggregation: '{final_phase_name}'\")\n            final_output_data = outputs.get(final_phase_name)\n\n            if final_output_data is None:\n                # Should not happen if logic above is correct, but safety check\n                raise AggregationError(f\"Selected final phase '{final_phase_name}' not found in outputs dictionary.\")\n\n            # Extract the response text using the utility method\n            final_response_text = self._extract_output(final_output_data)\n            # Note: _extract_output returns \"\" if extraction fails\n\n            # Get confidence score from the final phase's output data\n            confidence = 0.0 # Default if not found\n            if isinstance(final_output_data, dict) and \"confidence\" in final_output_data:\n                conf_val = final_output_data[\"confidence\"]\n                if isinstance(conf_val, (int, float)):\n                    confidence = float(conf_val)\n                elif isinstance(conf_val, dict) and \"combined\" in conf_val:\n                    # Prioritize 'combined' score\n                    combined_score = conf_val.get(\"combined\")\n                    if isinstance(combined_score, (int, float)):\n                        confidence = float(combined_score)\n\n            # Ensure confidence is valid [0, 1] range\n            confidence = min(max(confidence, 0.0), 1.0)\n\n            # If confidence is still very low/zero, maybe use the average from all phases?\n            if confidence &lt; 0.1: # Threshold check\n                 average_confidence = self._calculate_confidence(outputs)\n                 if average_confidence &gt; confidence:\n                      logger.debug(f\"Final phase confidence ({confidence:.3f}) is low, using average confidence ({average_confidence:.3f}) instead.\")\n                      confidence = average_confidence\n\n\n            execution_time = time.time() - start_time\n            logger.info(\n                f\"Sequential Refinement aggregation completed in {execution_time:.2f}s. \"\n                f\"Using output from phase: '{final_phase_name}'\",\n                 extra={ \"final_confidence\": confidence }\n            )\n\n            # Prepare final result dictionary\n            aggregation_result = {\n                \"response\": final_response_text,\n                \"confidence\": confidence,\n                \"final_phase\": final_phase_name\n            }\n\n            # Add trace if collector is provided\n            if trace_collector:\n                trace_collector.add_aggregation_trace(\n                    strategy_name=self._strategy_name,\n                    inputs={\n                        \"phase_output_keys\": list(outputs.keys()),\n                        \"context_keys\": list(context.keys()),\n                        \"determined_final_phase\": final_phase_name\n                    },\n                    output=aggregation_result, # Response, confidence, final_phase name\n                    execution_time=execution_time,\n                    parameters={\"final_phase_configured\": configured_final_phase} # Specific params\n                )\n\n            return aggregation_result\n\n        except AggregationError as e:\n             logger.error(f\"Sequential refinement aggregation failed: {str(e)}\")\n             raise # Re-raise known errors\n        except Exception as e:\n            logger.error(f\"Unexpected error during Sequential Refinement aggregation: {str(e)}\", exc_info=True)\n            # Wrap unexpected errors\n            raise AggregationError(f\"Sequential Refinement aggregation failed unexpectedly: {str(e)}\")\n</code></pre>"},{"location":"api/aggregation/#ai_ensemble_suite.aggregation.SequentialRefinement.__init__","title":"<code>__init__(config_manager, strategy_name, model_manager=None, strategy_config_override=None)</code>","text":"<p>Initialize the SequentialRefinement aggregator.</p> Source code in <code>src\\ai_ensemble_suite\\aggregation\\sequential_refinement.py</code> <pre><code>def __init__(\n    self,\n    config_manager: \"ConfigManager\",\n    strategy_name: str,\n    model_manager: Optional[\"ModelManager\"] = None,\n    strategy_config_override: Optional[Dict[str, Any]] = None\n) -&gt; None:\n    \"\"\"Initialize the SequentialRefinement aggregator.\"\"\"\n    super().__init__(config_manager, strategy_name, model_manager, strategy_config_override)\n</code></pre>"},{"location":"api/aggregation/#ai_ensemble_suite.aggregation.SequentialRefinement.aggregate","title":"<code>aggregate(outputs, context, trace_collector=None)</code>  <code>async</code>","text":"<p>Aggregate by selecting the output from the designated final phase.</p> <p>Parameters:</p> Name Type Description Default <code>outputs</code> <code>Dict[str, Dict[str, Any]]</code> <p>Dictionary mapping phase names to their output data.</p> required <code>context</code> <code>Dict[str, Any]</code> <p>Context information (used to infer final phase if not configured).</p> required <code>trace_collector</code> <code>Optional[TraceCollector]</code> <p>Optional trace collector.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary containing: response: The text output from the final phase. confidence: The confidence score associated with the final phase output. final_phase: The name of the phase selected as final.</p> <p>Raises:</p> Type Description <code>AggregationError</code> <p>If no final phase can be determined or its output is missing.</p> Source code in <code>src\\ai_ensemble_suite\\aggregation\\sequential_refinement.py</code> <pre><code>async def aggregate(\n    self,\n    outputs: Dict[str, Dict[str, Any]],\n    context: Dict[str, Any],\n    trace_collector: Optional[TraceCollector] = None\n) -&gt; Dict[str, Any]:\n    \"\"\"Aggregate by selecting the output from the designated final phase.\n\n    Args:\n        outputs: Dictionary mapping phase names to their output data.\n        context: Context information (used to infer final phase if not configured).\n        trace_collector: Optional trace collector.\n\n    Returns:\n        Dictionary containing:\n            response: The text output from the final phase.\n            confidence: The confidence score associated with the final phase output.\n            final_phase: The name of the phase selected as final.\n\n    Raises:\n        AggregationError: If no final phase can be determined or its output is missing.\n    \"\"\"\n    start_time = time.time()\n    logger.debug(f\"Starting Sequential Refinement aggregation for strategy '{self._strategy_name}'\")\n\n    if not outputs:\n         raise AggregationError(\"No phase outputs provided for Sequential Refinement aggregation.\")\n\n    try:\n        final_phase_name: Optional[str] = None\n\n        # --- Determine the Final Phase ---\n        # 1. Check strategy configuration for 'final_phase'\n        configured_final_phase = self.get_final_phase() # Method uses self._config\n        if configured_final_phase and configured_final_phase in outputs:\n            logger.debug(f\"Using configured final_phase: '{configured_final_phase}'\")\n            final_phase_name = configured_final_phase\n\n        # 2. Infer from 'sequence' key in strategy config if final_phase not set/found\n        if not final_phase_name:\n            sequence = self._config.get(\"sequence\", [])\n            if isinstance(sequence, list) and sequence:\n                potential_final = sequence[-1]\n                if potential_final in outputs:\n                    logger.debug(f\"Inferred final phase from config sequence: '{potential_final}'\")\n                    final_phase_name = potential_final\n\n        # 3. Infer from 'phase_sequence' in the main context if still not found\n        if not final_phase_name:\n            context_sequence = context.get(\"phase_sequence\", [])\n            if isinstance(context_sequence, list) and context_sequence:\n                potential_final = context_sequence[-1]\n                if potential_final in outputs:\n                    logger.debug(f\"Inferred final phase from context sequence: '{potential_final}'\")\n                    final_phase_name = potential_final\n\n        # 4. Fallback: Look for phases named like 'refinement', 'integration', 'final' etc.\n        if not final_phase_name:\n             candidate_suffixes = [\"refinement\", \"integration\", \"final\", \"synthesis\", \"summary\", \"committee\"]\n             # Check in reverse order of phase execution if possible\n             phases_to_check = reversed(context.get(\"phase_sequence\", list(outputs.keys())))\n             for phase_name_to_check in phases_to_check:\n                  if phase_name_to_check in outputs:\n                       lower_name = phase_name_to_check.lower()\n                       if any(suffix in lower_name for suffix in candidate_suffixes):\n                            logger.debug(f\"Using phase '{phase_name_to_check}' as likely final phase based on name.\")\n                            final_phase_name = phase_name_to_check\n                            break\n\n        # 5. Last Resort: If still no final phase determined, use the last available output based on context sequence or dict order\n        if not final_phase_name:\n            phases_in_order = context.get(\"phase_sequence\", list(outputs.keys()))\n            # Find the last phase in the order that actually exists in outputs\n            for potential_final in reversed(phases_in_order):\n                 if potential_final in outputs:\n                      logger.warning(f\"Could not determine specific final phase, using last available phase: '{potential_final}'\")\n                      final_phase_name = potential_final\n                      break\n            # If even that fails (e.g., sequence empty and outputs empty, though caught earlier)\n            if not final_phase_name:\n                 raise AggregationError(\"Cannot determine final phase: No outputs available or sequence context missing.\")\n\n\n        # --- Extract Output and Confidence from Final Phase ---\n        logger.info(f\"Selected final phase for aggregation: '{final_phase_name}'\")\n        final_output_data = outputs.get(final_phase_name)\n\n        if final_output_data is None:\n            # Should not happen if logic above is correct, but safety check\n            raise AggregationError(f\"Selected final phase '{final_phase_name}' not found in outputs dictionary.\")\n\n        # Extract the response text using the utility method\n        final_response_text = self._extract_output(final_output_data)\n        # Note: _extract_output returns \"\" if extraction fails\n\n        # Get confidence score from the final phase's output data\n        confidence = 0.0 # Default if not found\n        if isinstance(final_output_data, dict) and \"confidence\" in final_output_data:\n            conf_val = final_output_data[\"confidence\"]\n            if isinstance(conf_val, (int, float)):\n                confidence = float(conf_val)\n            elif isinstance(conf_val, dict) and \"combined\" in conf_val:\n                # Prioritize 'combined' score\n                combined_score = conf_val.get(\"combined\")\n                if isinstance(combined_score, (int, float)):\n                    confidence = float(combined_score)\n\n        # Ensure confidence is valid [0, 1] range\n        confidence = min(max(confidence, 0.0), 1.0)\n\n        # If confidence is still very low/zero, maybe use the average from all phases?\n        if confidence &lt; 0.1: # Threshold check\n             average_confidence = self._calculate_confidence(outputs)\n             if average_confidence &gt; confidence:\n                  logger.debug(f\"Final phase confidence ({confidence:.3f}) is low, using average confidence ({average_confidence:.3f}) instead.\")\n                  confidence = average_confidence\n\n\n        execution_time = time.time() - start_time\n        logger.info(\n            f\"Sequential Refinement aggregation completed in {execution_time:.2f}s. \"\n            f\"Using output from phase: '{final_phase_name}'\",\n             extra={ \"final_confidence\": confidence }\n        )\n\n        # Prepare final result dictionary\n        aggregation_result = {\n            \"response\": final_response_text,\n            \"confidence\": confidence,\n            \"final_phase\": final_phase_name\n        }\n\n        # Add trace if collector is provided\n        if trace_collector:\n            trace_collector.add_aggregation_trace(\n                strategy_name=self._strategy_name,\n                inputs={\n                    \"phase_output_keys\": list(outputs.keys()),\n                    \"context_keys\": list(context.keys()),\n                    \"determined_final_phase\": final_phase_name\n                },\n                output=aggregation_result, # Response, confidence, final_phase name\n                execution_time=execution_time,\n                parameters={\"final_phase_configured\": configured_final_phase} # Specific params\n            )\n\n        return aggregation_result\n\n    except AggregationError as e:\n         logger.error(f\"Sequential refinement aggregation failed: {str(e)}\")\n         raise # Re-raise known errors\n    except Exception as e:\n        logger.error(f\"Unexpected error during Sequential Refinement aggregation: {str(e)}\", exc_info=True)\n        # Wrap unexpected errors\n        raise AggregationError(f\"Sequential Refinement aggregation failed unexpectedly: {str(e)}\")\n</code></pre>"},{"location":"api/aggregation/#ai_ensemble_suite.aggregation.WeightedVoting","title":"<code>WeightedVoting</code>","text":"<p>               Bases: <code>BaseAggregator</code></p> <p>Weighted Voting aggregation strategy.</p> <p>Aggregates by assigning weights to different phase outputs (based on config) and selecting the output text that receives the highest total weight.</p> Source code in <code>src\\ai_ensemble_suite\\aggregation\\weighted_voting.py</code> <pre><code>class WeightedVoting(BaseAggregator):\n    \"\"\"Weighted Voting aggregation strategy.\n\n    Aggregates by assigning weights to different phase outputs (based on config)\n    and selecting the output text that receives the highest total weight.\n    \"\"\"\n\n     # Override __init__ for consistency\n    def __init__(\n        self,\n        config_manager: \"ConfigManager\",\n        strategy_name: str,\n        model_manager: Optional[\"ModelManager\"] = None,\n        strategy_config_override: Optional[Dict[str, Any]] = None\n    ) -&gt; None:\n        \"\"\"Initialize the WeightedVoting aggregator.\"\"\"\n        super().__init__(config_manager, strategy_name, model_manager, strategy_config_override)\n        # No specific init needed\n\n\n    async def aggregate(\n        self,\n        outputs: Dict[str, Dict[str, Any]],\n        context: Dict[str, Any],\n        trace_collector: Optional[TraceCollector] = None\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Aggregate phase outputs using weighted voting based on configuration.\n\n        Args:\n            outputs: Dictionary mapping phase names to their output data.\n            context: Context information from collaboration phases.\n            trace_collector: Optional trace collector.\n\n        Returns:\n            Dictionary containing:\n                response: The output text that received the highest total weight.\n                confidence: Confidence score, potentially combining weights and source confidence.\n                weights: Dictionary mapping output texts to their normalized weights.\n                source_phase: Name(s) of the phase(s) that produced the winning output.\n\n        Raises:\n            AggregationError: If no valid outputs or weights are available.\n        \"\"\"\n        start_time = time.time()\n        logger.debug(f\"Starting Weighted Voting aggregation for strategy '{self._strategy_name}'\")\n\n        if not outputs:\n             raise AggregationError(\"No phase outputs provided for Weighted Voting aggregation.\")\n\n        try:\n            # Get weights from strategy configuration: { \"phase_name\": weight }\n            phase_weights = self._config.get(\"weights\", {})\n            if not isinstance(phase_weights, dict):\n                 logger.warning(\"Invalid 'weights' config (must be dict), using default weights.\")\n                 phase_weights = {}\n\n            default_weight = self._config.get(\"default_weight\", 1.0)\n            if not isinstance(default_weight, (int, float)):\n                 logger.warning(f\"Invalid 'default_weight' {default_weight}, using 1.0.\")\n                 default_weight = 1.0\n\n            # --- Tally weighted votes for each unique output text ---\n            output_votes: Counter[str] = Counter() # Use Counter for summing weights per text\n            valid_phases_processed = 0\n\n            for phase_name, phase_output_data in outputs.items():\n                 # Get the configured weight for this phase, or use default\n                 weight = phase_weights.get(phase_name, default_weight)\n\n                 # Skip phases with non-positive weight\n                 if weight &lt;= 0:\n                      logger.debug(f\"Skipping phase '{phase_name}' due to weight &lt;= 0.\")\n                      continue\n\n                 # Extract the output text content\n                 output_text = self._extract_output(phase_output_data)\n\n                 # Skip phases where text extraction failed or resulted in empty string\n                 if not output_text:\n                      logger.debug(f\"Could not extract valid text from phase '{phase_name}', skipping vote.\")\n                      continue\n\n                 # Add the weight to the vote count for this specific output text\n                 output_votes[output_text] += weight\n                 valid_phases_processed += 1\n                 logger.debug(f\"Phase '{phase_name}' voted for output (hash:{hash(output_text)%1000:03d}) with weight {weight:.2f}\")\n\n\n            # Check if any valid votes were cast\n            if not output_votes or valid_phases_processed == 0:\n                # If no weights were configured AND a final phase exists, use fallback logic?\n                # This duplicates logic from base/other strategies, maybe avoid here.\n                # Let's just error if no votes.\n                logger.error(\"No valid outputs or positive weights found for aggregation.\")\n                raise AggregationError(\"Weighted voting failed: No valid weighted outputs available.\")\n\n            # --- Determine the winning output ---\n            # Find the output text with the highest total weight\n            # most_common(1) returns list of tuples: [ (text, total_weight) ]\n            winner_list = output_votes.most_common(1)\n            if not winner_list: # Should not happen if output_votes is not empty\n                 raise AggregationError(\"Internal error: Could not determine winner from votes.\")\n\n            best_output_text, highest_total_weight = winner_list[0]\n\n            # --- Calculate Normalized Weights and Confidence ---\n            # Calculate the sum of all weights cast\n            total_weight_sum = sum(output_votes.values())\n\n            # Normalize the weights for each unique output (for reporting/tracing)\n            normalized_weights = {}\n            if total_weight_sum &gt; 0:\n                 normalized_weights = {text: votes / total_weight_sum for text, votes in output_votes.items()}\n                 # The weight of the winning output corresponds to its normalized score\n                 best_output_normalized_weight = normalized_weights.get(best_output_text, 0.0)\n            else: # Should not happen if weights &gt; 0\n                 logger.warning(\"Total weight sum is zero, cannot normalize.\")\n                 best_output_normalized_weight = 1.0 if len(output_votes) == 1 else 0.0 # Assign 1 if only one option\n\n            # Calculate confidence: Blend the normalized weight of the winner\n            # with the average confidence of the phases that produced the winning output.\n            source_phases_confidences = []\n            source_phase_names = []\n            for phase_name, phase_output_data in outputs.items():\n                 if self._extract_output(phase_output_data) == best_output_text:\n                      source_phase_names.append(phase_name)\n                      # Extract confidence from this source phase\n                      if isinstance(phase_output_data, dict) and \"confidence\" in phase_output_data:\n                           conf_val = phase_output_data[\"confidence\"]\n                           score = None\n                           if isinstance(conf_val, (int, float)): score = float(conf_val)\n                           elif isinstance(conf_val, dict): score = conf_val.get(\"combined\")\n                           if isinstance(score, (int, float)):\n                                source_phases_confidences.append(min(max(float(score), 0.0), 1.0))\n\n            avg_source_confidence = (sum(source_phases_confidences) / len(source_phases_confidences)) \\\n                                     if source_phases_confidences else 0.7 # Default if no source confidences\n\n            # Blend: e.g., 60% weight score, 40% source confidence average\n            final_confidence = (0.6 * best_output_normalized_weight) + (0.4 * avg_source_confidence)\n            final_confidence = min(max(final_confidence, 0.0), 1.0) # Clamp to [0, 1]\n\n\n            execution_time = time.time() - start_time\n            logger.info(\n                f\"Weighted Voting aggregation completed in {execution_time:.2f}s. \"\n                f\"Winner score: {best_output_normalized_weight:.3f} \"\n                f\"(from phases: {source_phase_names})\",\n                 extra={\"final_confidence\": final_confidence}\n            )\n\n            # Prepare final result dictionary\n            # Sort normalized weights for clearer reporting/tracing\n            sorted_normalized_weights = dict(sorted(normalized_weights.items(), key=lambda item: item[1], reverse=True))\n\n            aggregation_result = {\n                \"response\": best_output_text,\n                \"confidence\": final_confidence,\n                 # Report normalized weights per unique text output\n                \"weights\": sorted_normalized_weights,\n                 # List phases that produced the winning output\n                \"source_phase\": source_phase_names[0] if len(source_phase_names) == 1 else source_phase_names,\n            }\n\n\n            # Add trace if collector is provided\n            if trace_collector:\n                trace_collector.add_aggregation_trace(\n                    strategy_name=self._strategy_name,\n                    inputs={\n                        \"phase_output_keys\": list(outputs.keys()),\n                        \"context_keys\": list(context.keys()),\n                        \"configured_weights\": phase_weights,\n                        \"default_weight\": default_weight,\n                    },\n                    output=aggregation_result, # Contains response, conf, normalized weights map, source(s)\n                    execution_time=execution_time,\n                    parameters={ # Parameters influencing the vote\n                        \"configured_weights\": phase_weights,\n                        \"default_weight\": default_weight,\n                         # Trace the raw votes before normalization for debugging\n                        \"raw_votes\": dict(output_votes)\n                    }\n                )\n\n            return aggregation_result\n\n        except AggregationError as e:\n             logger.error(f\"Weighted voting aggregation failed: {str(e)}\")\n             raise # Re-raise known errors\n        except Exception as e:\n            logger.error(f\"Unexpected error during Weighted Voting aggregation: {str(e)}\", exc_info=True)\n            # Wrap unexpected errors\n            raise AggregationError(f\"Weighted Voting aggregation failed unexpectedly: {str(e)}\")\n</code></pre>"},{"location":"api/aggregation/#ai_ensemble_suite.aggregation.WeightedVoting.__init__","title":"<code>__init__(config_manager, strategy_name, model_manager=None, strategy_config_override=None)</code>","text":"<p>Initialize the WeightedVoting aggregator.</p> Source code in <code>src\\ai_ensemble_suite\\aggregation\\weighted_voting.py</code> <pre><code>def __init__(\n    self,\n    config_manager: \"ConfigManager\",\n    strategy_name: str,\n    model_manager: Optional[\"ModelManager\"] = None,\n    strategy_config_override: Optional[Dict[str, Any]] = None\n) -&gt; None:\n    \"\"\"Initialize the WeightedVoting aggregator.\"\"\"\n    super().__init__(config_manager, strategy_name, model_manager, strategy_config_override)\n</code></pre>"},{"location":"api/aggregation/#ai_ensemble_suite.aggregation.WeightedVoting.aggregate","title":"<code>aggregate(outputs, context, trace_collector=None)</code>  <code>async</code>","text":"<p>Aggregate phase outputs using weighted voting based on configuration.</p> <p>Parameters:</p> Name Type Description Default <code>outputs</code> <code>Dict[str, Dict[str, Any]]</code> <p>Dictionary mapping phase names to their output data.</p> required <code>context</code> <code>Dict[str, Any]</code> <p>Context information from collaboration phases.</p> required <code>trace_collector</code> <code>Optional[TraceCollector]</code> <p>Optional trace collector.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary containing: response: The output text that received the highest total weight. confidence: Confidence score, potentially combining weights and source confidence. weights: Dictionary mapping output texts to their normalized weights. source_phase: Name(s) of the phase(s) that produced the winning output.</p> <p>Raises:</p> Type Description <code>AggregationError</code> <p>If no valid outputs or weights are available.</p> Source code in <code>src\\ai_ensemble_suite\\aggregation\\weighted_voting.py</code> <pre><code>async def aggregate(\n    self,\n    outputs: Dict[str, Dict[str, Any]],\n    context: Dict[str, Any],\n    trace_collector: Optional[TraceCollector] = None\n) -&gt; Dict[str, Any]:\n    \"\"\"Aggregate phase outputs using weighted voting based on configuration.\n\n    Args:\n        outputs: Dictionary mapping phase names to their output data.\n        context: Context information from collaboration phases.\n        trace_collector: Optional trace collector.\n\n    Returns:\n        Dictionary containing:\n            response: The output text that received the highest total weight.\n            confidence: Confidence score, potentially combining weights and source confidence.\n            weights: Dictionary mapping output texts to their normalized weights.\n            source_phase: Name(s) of the phase(s) that produced the winning output.\n\n    Raises:\n        AggregationError: If no valid outputs or weights are available.\n    \"\"\"\n    start_time = time.time()\n    logger.debug(f\"Starting Weighted Voting aggregation for strategy '{self._strategy_name}'\")\n\n    if not outputs:\n         raise AggregationError(\"No phase outputs provided for Weighted Voting aggregation.\")\n\n    try:\n        # Get weights from strategy configuration: { \"phase_name\": weight }\n        phase_weights = self._config.get(\"weights\", {})\n        if not isinstance(phase_weights, dict):\n             logger.warning(\"Invalid 'weights' config (must be dict), using default weights.\")\n             phase_weights = {}\n\n        default_weight = self._config.get(\"default_weight\", 1.0)\n        if not isinstance(default_weight, (int, float)):\n             logger.warning(f\"Invalid 'default_weight' {default_weight}, using 1.0.\")\n             default_weight = 1.0\n\n        # --- Tally weighted votes for each unique output text ---\n        output_votes: Counter[str] = Counter() # Use Counter for summing weights per text\n        valid_phases_processed = 0\n\n        for phase_name, phase_output_data in outputs.items():\n             # Get the configured weight for this phase, or use default\n             weight = phase_weights.get(phase_name, default_weight)\n\n             # Skip phases with non-positive weight\n             if weight &lt;= 0:\n                  logger.debug(f\"Skipping phase '{phase_name}' due to weight &lt;= 0.\")\n                  continue\n\n             # Extract the output text content\n             output_text = self._extract_output(phase_output_data)\n\n             # Skip phases where text extraction failed or resulted in empty string\n             if not output_text:\n                  logger.debug(f\"Could not extract valid text from phase '{phase_name}', skipping vote.\")\n                  continue\n\n             # Add the weight to the vote count for this specific output text\n             output_votes[output_text] += weight\n             valid_phases_processed += 1\n             logger.debug(f\"Phase '{phase_name}' voted for output (hash:{hash(output_text)%1000:03d}) with weight {weight:.2f}\")\n\n\n        # Check if any valid votes were cast\n        if not output_votes or valid_phases_processed == 0:\n            # If no weights were configured AND a final phase exists, use fallback logic?\n            # This duplicates logic from base/other strategies, maybe avoid here.\n            # Let's just error if no votes.\n            logger.error(\"No valid outputs or positive weights found for aggregation.\")\n            raise AggregationError(\"Weighted voting failed: No valid weighted outputs available.\")\n\n        # --- Determine the winning output ---\n        # Find the output text with the highest total weight\n        # most_common(1) returns list of tuples: [ (text, total_weight) ]\n        winner_list = output_votes.most_common(1)\n        if not winner_list: # Should not happen if output_votes is not empty\n             raise AggregationError(\"Internal error: Could not determine winner from votes.\")\n\n        best_output_text, highest_total_weight = winner_list[0]\n\n        # --- Calculate Normalized Weights and Confidence ---\n        # Calculate the sum of all weights cast\n        total_weight_sum = sum(output_votes.values())\n\n        # Normalize the weights for each unique output (for reporting/tracing)\n        normalized_weights = {}\n        if total_weight_sum &gt; 0:\n             normalized_weights = {text: votes / total_weight_sum for text, votes in output_votes.items()}\n             # The weight of the winning output corresponds to its normalized score\n             best_output_normalized_weight = normalized_weights.get(best_output_text, 0.0)\n        else: # Should not happen if weights &gt; 0\n             logger.warning(\"Total weight sum is zero, cannot normalize.\")\n             best_output_normalized_weight = 1.0 if len(output_votes) == 1 else 0.0 # Assign 1 if only one option\n\n        # Calculate confidence: Blend the normalized weight of the winner\n        # with the average confidence of the phases that produced the winning output.\n        source_phases_confidences = []\n        source_phase_names = []\n        for phase_name, phase_output_data in outputs.items():\n             if self._extract_output(phase_output_data) == best_output_text:\n                  source_phase_names.append(phase_name)\n                  # Extract confidence from this source phase\n                  if isinstance(phase_output_data, dict) and \"confidence\" in phase_output_data:\n                       conf_val = phase_output_data[\"confidence\"]\n                       score = None\n                       if isinstance(conf_val, (int, float)): score = float(conf_val)\n                       elif isinstance(conf_val, dict): score = conf_val.get(\"combined\")\n                       if isinstance(score, (int, float)):\n                            source_phases_confidences.append(min(max(float(score), 0.0), 1.0))\n\n        avg_source_confidence = (sum(source_phases_confidences) / len(source_phases_confidences)) \\\n                                 if source_phases_confidences else 0.7 # Default if no source confidences\n\n        # Blend: e.g., 60% weight score, 40% source confidence average\n        final_confidence = (0.6 * best_output_normalized_weight) + (0.4 * avg_source_confidence)\n        final_confidence = min(max(final_confidence, 0.0), 1.0) # Clamp to [0, 1]\n\n\n        execution_time = time.time() - start_time\n        logger.info(\n            f\"Weighted Voting aggregation completed in {execution_time:.2f}s. \"\n            f\"Winner score: {best_output_normalized_weight:.3f} \"\n            f\"(from phases: {source_phase_names})\",\n             extra={\"final_confidence\": final_confidence}\n        )\n\n        # Prepare final result dictionary\n        # Sort normalized weights for clearer reporting/tracing\n        sorted_normalized_weights = dict(sorted(normalized_weights.items(), key=lambda item: item[1], reverse=True))\n\n        aggregation_result = {\n            \"response\": best_output_text,\n            \"confidence\": final_confidence,\n             # Report normalized weights per unique text output\n            \"weights\": sorted_normalized_weights,\n             # List phases that produced the winning output\n            \"source_phase\": source_phase_names[0] if len(source_phase_names) == 1 else source_phase_names,\n        }\n\n\n        # Add trace if collector is provided\n        if trace_collector:\n            trace_collector.add_aggregation_trace(\n                strategy_name=self._strategy_name,\n                inputs={\n                    \"phase_output_keys\": list(outputs.keys()),\n                    \"context_keys\": list(context.keys()),\n                    \"configured_weights\": phase_weights,\n                    \"default_weight\": default_weight,\n                },\n                output=aggregation_result, # Contains response, conf, normalized weights map, source(s)\n                execution_time=execution_time,\n                parameters={ # Parameters influencing the vote\n                    \"configured_weights\": phase_weights,\n                    \"default_weight\": default_weight,\n                     # Trace the raw votes before normalization for debugging\n                    \"raw_votes\": dict(output_votes)\n                }\n            )\n\n        return aggregation_result\n\n    except AggregationError as e:\n         logger.error(f\"Weighted voting aggregation failed: {str(e)}\")\n         raise # Re-raise known errors\n    except Exception as e:\n        logger.error(f\"Unexpected error during Weighted Voting aggregation: {str(e)}\", exc_info=True)\n        # Wrap unexpected errors\n        raise AggregationError(f\"Weighted Voting aggregation failed unexpectedly: {str(e)}\")\n</code></pre>"},{"location":"api/collaboration/","title":"Collaboration API","text":"<p>API reference for collaboration mechanisms. </p> <p>Collaboration phases for ai-ensemble-suite.</p>"},{"location":"api/collaboration/#ai_ensemble_suite.collaboration.AdversarialImprovement","title":"<code>AdversarialImprovement</code>","text":"<p>               Bases: <code>BaseCollaborationPhase</code></p> <p>Adversarial Improvement collaboration phase.</p> <p>Models improve a solution by actively seeking its weaknesses and addressing them in an iterative process.</p> Source code in <code>src\\ai_ensemble_suite\\collaboration\\adversarial_improvement.py</code> <pre><code>class AdversarialImprovement(BaseCollaborationPhase):\n    \"\"\"Adversarial Improvement collaboration phase.\n\n    Models improve a solution by actively seeking its weaknesses\n    and addressing them in an iterative process.\n    \"\"\"\n\n    def __init__(\n        self, \n        model_manager: \"ModelManager\",\n        config_manager: \"ConfigManager\",\n        phase_name: str\n    ) -&gt; None:\n        \"\"\"Initialize the adversarial improvement phase.\n\n        Args:\n            model_manager: The ModelManager instance.\n            config_manager: The ConfigManager instance.\n            phase_name: The name of the phase for configuration lookup.\n\n        Raises:\n            ConfigurationError: If phase configuration is invalid.\n        \"\"\"\n        super().__init__(model_manager, config_manager, phase_name)\n\n        # Get number of iterations\n        self._iterations = self._config.get(\"iterations\", 3)\n        if self._iterations &lt; 1:\n            logger.warning(\n                f\"Invalid iterations ({self._iterations}) for phase '{phase_name}', \"\n                \"using default: 3\"\n            )\n            self._iterations = 3\n\n        # Get template names\n        self._initial_template = self._config.get(\"initial_template\", \"adversarial_initial\")\n        self._critique_template = self._config.get(\"critique_template\", \"adversarial_critique\")\n        self._improvement_template = self._config.get(\"improvement_template\", \"adversarial_improvement\")\n\n        # Get model roles\n        self._initial_model = self._config.get(\"initial_model\")\n        self._critique_model = self._config.get(\"critique_model\")\n        self._improvement_model = self._config.get(\"improvement_model\")\n\n        # If roles not specified, assign them based on available models\n        if not self._initial_model and self._model_ids:\n            self._initial_model = self._model_ids[0]\n\n        if not self._critique_model and len(self._model_ids) &gt; 1:\n            self._critique_model = self._model_ids[1]\n        elif not self._critique_model:\n            self._critique_model = self._initial_model\n\n        if not self._improvement_model and len(self._model_ids) &gt; 2:\n            self._improvement_model = self._model_ids[2]\n        elif not self._improvement_model:\n            self._improvement_model = self._initial_model\n\n        logger.debug(\n            f\"Initialized AdversarialImprovement phase '{phase_name}' with \"\n            f\"{self._iterations} iterations\"\n        )\n\n    async def execute(\n        self, \n        query: str,\n        context: Dict[str, Any],\n        trace_collector: Optional[TraceCollector] = None\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Execute the Adversarial Improvement phase.\n\n        Args:\n            query: The user query to process.\n            context: Context information from previous phases.\n            trace_collector: Optional trace collector for gathering execution details.\n\n        Returns:\n            Dictionary containing:\n                output: The final improved response.\n                context: Updated context with iteration history.\n\n        Raises:\n            CollaborationError: If phase execution fails.\n        \"\"\"\n        start_time = time.time()\n\n        try:\n            # Get inputs from previous phases\n            inputs = self._get_inputs_from_context(context)\n\n            # Step 1: Generate initial solution\n            initial_solution = await self._generate_initial_solution(\n                query, inputs, trace_collector\n            )\n\n            # Initialize iteration history\n            iterations = []\n            current_solution = initial_solution\n\n            # Step 2: Iterative improvement\n            for i in range(self._iterations):\n                logger.debug(f\"Starting adversarial iteration {i + 1}/{self._iterations}\")\n\n                # Generate critique\n                critique = await self._generate_critique(\n                    query, current_solution, trace_collector\n                )\n\n                # Generate improvement\n                improved_solution = await self._generate_improvement(\n                    query, current_solution, critique, trace_collector\n                )\n\n                # Store iteration\n                iterations.append({\n                    \"iteration\": i + 1,\n                    \"solution\": current_solution,\n                    \"critique\": critique,\n                    \"improvement\": improved_solution\n                })\n\n                # Update current solution for next iteration\n                current_solution = improved_solution\n\n            execution_time = time.time() - start_time\n\n            # Log completion\n            logger.info(\n                f\"AdversarialImprovement phase '{self._phase_name}' completed in {execution_time:.2f}s\",\n                extra={\"iterations\": self._iterations}\n            )\n\n            # Add phase trace if collector is provided\n            if trace_collector:\n                trace_collector.add_phase_trace(\n                    phase_name=self._phase_name,\n                    input_data={\"query\": query, \"inputs\": inputs},\n                    output_data={\n                        \"initial_solution\": initial_solution,\n                        \"iterations\": iterations,\n                        \"final_solution\": current_solution\n                    },\n                    execution_time=execution_time,\n                    phase_parameters=self._config\n                )\n\n            # Return results\n            return {\n                \"output\": current_solution,\n                \"initial_solution\": initial_solution,\n                \"iterations\": iterations,\n                \"confidence\": 0.7 + (0.05 * min(self._iterations, 5))  # Confidence increases with iterations\n            }\n\n        except Exception as e:\n            raise CollaborationError(\n                f\"AdversarialImprovement phase '{self._phase_name}' failed: {str(e)}\"\n            )\n\n    async def _generate_initial_solution(\n        self,\n        query: str,\n        inputs: Dict[str, Any],\n        trace_collector: Optional[TraceCollector] = None\n    ) -&gt; str:\n        \"\"\"Generate the initial solution.\n\n        Args:\n            query: The user query.\n            inputs: Inputs from previous phases.\n            trace_collector: Optional trace collector.\n\n        Returns:\n            Initial solution text.\n\n        Raises:\n            CollaborationError: If generation fails.\n        \"\"\"\n        logger.debug(\"Generating initial solution\")\n\n        # Check if initial solution came from previous phase\n        if self._input_from and inputs:\n            for source in self._input_from:\n                if source in inputs:\n                    source_data = inputs[source]\n\n                    if isinstance(source_data, str):\n                        logger.debug(f\"Using solution from previous phase: {source}\")\n                        return source_data\n                    elif isinstance(source_data, dict) and \"output\" in source_data:\n                        logger.debug(f\"Using solution from previous phase: {source}\")\n                        return source_data[\"output\"]\n\n        # If no solution from previous phases, generate new one\n        if not self._initial_model:\n            raise CollaborationError(\"No model available for generating initial solution\")\n\n        # Format prompt for initial solution\n        try:\n            context = {\"query\": query, **inputs}\n            initial_prompt = self.render_template(self._initial_template, context)\n        except (ConfigurationError, KeyError) as e:\n            raise CollaborationError(f\"Failed to format initial solution prompt: {str(e)}\")\n\n        # Run initial model\n        try:\n            initial_result = await self._model_manager.run_inference(\n                model_id=self._initial_model,\n                prompt=initial_prompt\n            )\n\n            # Add trace if collector is provided\n            if trace_collector:\n                trace_collector.add_model_trace(\n                    model_id=self._initial_model,\n                    input_prompt=initial_prompt,\n                    output=initial_result,\n                    execution_time=initial_result.get(\"generation_time\", 0),\n                    parameters={\"role\": \"initial_solution\"}\n                )\n\n            return initial_result.get(\"text\", \"\")\n\n        except Exception as e:\n            raise CollaborationError(f\"Failed to generate initial solution: {str(e)}\")\n\n    async def _generate_critique(\n        self,\n        query: str,\n        solution: str,\n        trace_collector: Optional[TraceCollector] = None\n    ) -&gt; str:\n        \"\"\"Generate a critique of the current solution.\n\n        Args:\n            query: The user query.\n            solution: The current solution.\n            trace_collector: Optional trace collector.\n\n        Returns:\n            Critique text.\n\n        Raises:\n            CollaborationError: If critique generation fails.\n        \"\"\"\n        logger.debug(\"Generating critique\")\n\n        if not self._critique_model:\n            raise CollaborationError(\"No model available for generating critique\")\n\n        # Format prompt for critique\n        try:\n            context = {\"query\": query, \"solution\": solution}\n            critique_prompt = self.render_template(self._critique_template, context)\n        except (ConfigurationError, KeyError) as e:\n            raise CollaborationError(f\"Failed to format critique prompt: {str(e)}\")\n\n        # Run critique model\n        try:\n            critique_result = await self._model_manager.run_inference(\n                model_id=self._critique_model,\n                prompt=critique_prompt\n            )\n\n            # Add trace if collector is provided\n            if trace_collector:\n                trace_collector.add_model_trace(\n                    model_id=self._critique_model,\n                    input_prompt=critique_prompt,\n                    output=critique_result,\n                    execution_time=critique_result.get(\"generation_time\", 0),\n                    parameters={\"role\": \"critique\"}\n                )\n\n            return critique_result.get(\"text\", \"\")\n\n        except Exception as e:\n            raise CollaborationError(f\"Failed to generate critique: {str(e)}\")\n\n    async def _generate_improvement(\n        self,\n        query: str,\n        solution: str,\n        critique: str,\n        trace_collector: Optional[TraceCollector] = None\n    ) -&gt; str:\n        \"\"\"Generate an improved solution based on critique.\n\n        Args:\n            query: The user query.\n            solution: The current solution.\n            critique: The critique of the current solution.\n            trace_collector: Optional trace collector.\n\n        Returns:\n            Improved solution text.\n\n        Raises:\n            CollaborationError: If improvement generation fails.\n        \"\"\"\n        logger.debug(\"Generating improvement\")\n\n        if not self._improvement_model:\n            raise CollaborationError(\"No model available for generating improvement\")\n\n        # Format prompt for improvement\n        try:\n            context = {\"query\": query, \"solution\": solution, \"critique\": critique}\n            improvement_prompt = self.render_template(self._improvement_template, context)\n        except (ConfigurationError, KeyError) as e:\n            raise CollaborationError(f\"Failed to format improvement prompt: {str(e)}\")\n\n        # Run improvement model\n        try:\n            improvement_result = await self._model_manager.run_inference(\n                model_id=self._improvement_model,\n                prompt=improvement_prompt\n            )\n\n            # Add trace if collector is provided\n            if trace_collector:\n                trace_collector.add_model_trace(\n                    model_id=self._improvement_model,\n                    input_prompt=improvement_prompt,\n                    output=improvement_result,\n                    execution_time=improvement_result.get(\"generation_time\", 0),\n                    parameters={\"role\": \"improvement\"}\n                )\n\n            return improvement_result.get(\"text\", \"\")\n\n        except Exception as e:\n            raise CollaborationError(f\"Failed to generate improvement: {str(e)}\")\n</code></pre>"},{"location":"api/collaboration/#ai_ensemble_suite.collaboration.AdversarialImprovement.__init__","title":"<code>__init__(model_manager, config_manager, phase_name)</code>","text":"<p>Initialize the adversarial improvement phase.</p> <p>Parameters:</p> Name Type Description Default <code>model_manager</code> <code>ModelManager</code> <p>The ModelManager instance.</p> required <code>config_manager</code> <code>ConfigManager</code> <p>The ConfigManager instance.</p> required <code>phase_name</code> <code>str</code> <p>The name of the phase for configuration lookup.</p> required <p>Raises:</p> Type Description <code>ConfigurationError</code> <p>If phase configuration is invalid.</p> Source code in <code>src\\ai_ensemble_suite\\collaboration\\adversarial_improvement.py</code> <pre><code>def __init__(\n    self, \n    model_manager: \"ModelManager\",\n    config_manager: \"ConfigManager\",\n    phase_name: str\n) -&gt; None:\n    \"\"\"Initialize the adversarial improvement phase.\n\n    Args:\n        model_manager: The ModelManager instance.\n        config_manager: The ConfigManager instance.\n        phase_name: The name of the phase for configuration lookup.\n\n    Raises:\n        ConfigurationError: If phase configuration is invalid.\n    \"\"\"\n    super().__init__(model_manager, config_manager, phase_name)\n\n    # Get number of iterations\n    self._iterations = self._config.get(\"iterations\", 3)\n    if self._iterations &lt; 1:\n        logger.warning(\n            f\"Invalid iterations ({self._iterations}) for phase '{phase_name}', \"\n            \"using default: 3\"\n        )\n        self._iterations = 3\n\n    # Get template names\n    self._initial_template = self._config.get(\"initial_template\", \"adversarial_initial\")\n    self._critique_template = self._config.get(\"critique_template\", \"adversarial_critique\")\n    self._improvement_template = self._config.get(\"improvement_template\", \"adversarial_improvement\")\n\n    # Get model roles\n    self._initial_model = self._config.get(\"initial_model\")\n    self._critique_model = self._config.get(\"critique_model\")\n    self._improvement_model = self._config.get(\"improvement_model\")\n\n    # If roles not specified, assign them based on available models\n    if not self._initial_model and self._model_ids:\n        self._initial_model = self._model_ids[0]\n\n    if not self._critique_model and len(self._model_ids) &gt; 1:\n        self._critique_model = self._model_ids[1]\n    elif not self._critique_model:\n        self._critique_model = self._initial_model\n\n    if not self._improvement_model and len(self._model_ids) &gt; 2:\n        self._improvement_model = self._model_ids[2]\n    elif not self._improvement_model:\n        self._improvement_model = self._initial_model\n\n    logger.debug(\n        f\"Initialized AdversarialImprovement phase '{phase_name}' with \"\n        f\"{self._iterations} iterations\"\n    )\n</code></pre>"},{"location":"api/collaboration/#ai_ensemble_suite.collaboration.AdversarialImprovement.execute","title":"<code>execute(query, context, trace_collector=None)</code>  <code>async</code>","text":"<p>Execute the Adversarial Improvement phase.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The user query to process.</p> required <code>context</code> <code>Dict[str, Any]</code> <p>Context information from previous phases.</p> required <code>trace_collector</code> <code>Optional[TraceCollector]</code> <p>Optional trace collector for gathering execution details.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary containing: output: The final improved response. context: Updated context with iteration history.</p> <p>Raises:</p> Type Description <code>CollaborationError</code> <p>If phase execution fails.</p> Source code in <code>src\\ai_ensemble_suite\\collaboration\\adversarial_improvement.py</code> <pre><code>async def execute(\n    self, \n    query: str,\n    context: Dict[str, Any],\n    trace_collector: Optional[TraceCollector] = None\n) -&gt; Dict[str, Any]:\n    \"\"\"Execute the Adversarial Improvement phase.\n\n    Args:\n        query: The user query to process.\n        context: Context information from previous phases.\n        trace_collector: Optional trace collector for gathering execution details.\n\n    Returns:\n        Dictionary containing:\n            output: The final improved response.\n            context: Updated context with iteration history.\n\n    Raises:\n        CollaborationError: If phase execution fails.\n    \"\"\"\n    start_time = time.time()\n\n    try:\n        # Get inputs from previous phases\n        inputs = self._get_inputs_from_context(context)\n\n        # Step 1: Generate initial solution\n        initial_solution = await self._generate_initial_solution(\n            query, inputs, trace_collector\n        )\n\n        # Initialize iteration history\n        iterations = []\n        current_solution = initial_solution\n\n        # Step 2: Iterative improvement\n        for i in range(self._iterations):\n            logger.debug(f\"Starting adversarial iteration {i + 1}/{self._iterations}\")\n\n            # Generate critique\n            critique = await self._generate_critique(\n                query, current_solution, trace_collector\n            )\n\n            # Generate improvement\n            improved_solution = await self._generate_improvement(\n                query, current_solution, critique, trace_collector\n            )\n\n            # Store iteration\n            iterations.append({\n                \"iteration\": i + 1,\n                \"solution\": current_solution,\n                \"critique\": critique,\n                \"improvement\": improved_solution\n            })\n\n            # Update current solution for next iteration\n            current_solution = improved_solution\n\n        execution_time = time.time() - start_time\n\n        # Log completion\n        logger.info(\n            f\"AdversarialImprovement phase '{self._phase_name}' completed in {execution_time:.2f}s\",\n            extra={\"iterations\": self._iterations}\n        )\n\n        # Add phase trace if collector is provided\n        if trace_collector:\n            trace_collector.add_phase_trace(\n                phase_name=self._phase_name,\n                input_data={\"query\": query, \"inputs\": inputs},\n                output_data={\n                    \"initial_solution\": initial_solution,\n                    \"iterations\": iterations,\n                    \"final_solution\": current_solution\n                },\n                execution_time=execution_time,\n                phase_parameters=self._config\n            )\n\n        # Return results\n        return {\n            \"output\": current_solution,\n            \"initial_solution\": initial_solution,\n            \"iterations\": iterations,\n            \"confidence\": 0.7 + (0.05 * min(self._iterations, 5))  # Confidence increases with iterations\n        }\n\n    except Exception as e:\n        raise CollaborationError(\n            f\"AdversarialImprovement phase '{self._phase_name}' failed: {str(e)}\"\n        )\n</code></pre>"},{"location":"api/collaboration/#ai_ensemble_suite.collaboration.AsyncThinking","title":"<code>AsyncThinking</code>","text":"<p>               Bases: <code>BaseCollaborationPhase</code></p> <p>Asynchronous Thinking collaboration phase.</p> <p>Models work independently on a problem before their outputs are collected. This is the simplest form of collaboration where multiple models process the same prompt concurrently.</p> Source code in <code>src\\ai_ensemble_suite\\collaboration\\async_thinking.py</code> <pre><code>class AsyncThinking(BaseCollaborationPhase):\n    \"\"\"Asynchronous Thinking collaboration phase.\n\n    Models work independently on a problem before their outputs are collected.\n    This is the simplest form of collaboration where multiple models process\n    the same prompt concurrently.\n    \"\"\"\n\n    async def execute(\n        self, \n        query: str,\n        context: Dict[str, Any],\n        trace_collector: Optional[TraceCollector] = None\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Execute the Asynchronous Thinking phase.\n\n        Args:\n            query: The user query to process.\n            context: Context information from previous phases.\n            trace_collector: Optional trace collector for gathering execution details.\n\n        Returns:\n            Dictionary containing:\n                output: Dictionary mapping model IDs to their responses.\n                context: Updated context for the next phase.\n\n        Raises:\n            CollaborationError: If phase execution fails.\n        \"\"\"\n        start_time = time.time()\n\n        try:\n            # Get prompt template or use the query directly\n            prompt = query\n            if self._prompt_template:\n                try:\n                    # Format prompt with query and any inputs from previous phases\n                    inputs = self._get_inputs_from_context(context)\n                    context_vars = {\"query\": query, **inputs}\n                    prompt = self.render_template(self._prompt_template, context_vars)\n                except (ConfigurationError, KeyError) as e:\n                    raise CollaborationError(f\"Failed to format prompt: {str(e)}\")\n\n            # Run models concurrently\n            model_results = await self._run_models(\n                prompt=prompt,\n                trace_collector=trace_collector\n            )\n\n            # Process outputs for simpler consumption by next phases\n            processed_outputs: Dict[str, str] = {}\n\n            for model_id, result in model_results.items():\n                processed_outputs[model_id] = result.get(\"text\", \"\")\n\n            # Select the primary output for single model case\n            primary_output = \"\"\n            if len(processed_outputs) == 1:\n                primary_output = list(processed_outputs.values())[0]\n            elif self._model_ids and len(self._model_ids) &gt; 0:\n                # Use the first model in the model_ids list as primary if available\n                primary_model = self._model_ids[0]\n                if primary_model in processed_outputs:\n                    primary_output = processed_outputs[primary_model]\n\n            execution_time = time.time() - start_time\n\n            # Log completion\n            logger.info(\n                f\"AsyncThinking phase '{self._phase_name}' completed in {execution_time:.2f}s\",\n                extra={\n                    \"model_count\": len(model_results),\n                    \"phase\": self._phase_name\n                }\n            )\n\n            # Add phase trace if collector is provided\n            if trace_collector:\n                trace_collector.add_phase_trace(\n                    phase_name=self._phase_name,\n                    input_data={\"query\": query, \"prompt\": prompt},\n                    output_data={\"outputs\": processed_outputs},\n                    execution_time=execution_time,\n                    phase_parameters=self._config\n                )\n\n            # Calculate a combined confidence score (average of all models)\n            confidence_values = []\n            for result in model_results.values():\n                if \"confidence\" in result:\n                    if isinstance(result[\"confidence\"], dict) and \"combined\" in result[\"confidence\"]:\n                        confidence_values.append(result[\"confidence\"][\"combined\"])\n                    elif isinstance(result[\"confidence\"], (float, int)):\n                        confidence_values.append(result[\"confidence\"])\n\n            confidence_score = sum(confidence_values) / len(confidence_values) if confidence_values else 0.7\n\n            # Return results\n            return {\n                \"output\": primary_output,  \n                \"outputs\": processed_outputs,\n                \"confidence\": confidence_score,\n                \"raw_results\": model_results  # Include full model results for advanced use\n            }\n\n        except Exception as e:\n            raise CollaborationError(\n                f\"AsyncThinking phase '{self._phase_name}' failed: {str(e)}\"\n            )\n</code></pre>"},{"location":"api/collaboration/#ai_ensemble_suite.collaboration.AsyncThinking.execute","title":"<code>execute(query, context, trace_collector=None)</code>  <code>async</code>","text":"<p>Execute the Asynchronous Thinking phase.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The user query to process.</p> required <code>context</code> <code>Dict[str, Any]</code> <p>Context information from previous phases.</p> required <code>trace_collector</code> <code>Optional[TraceCollector]</code> <p>Optional trace collector for gathering execution details.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary containing: output: Dictionary mapping model IDs to their responses. context: Updated context for the next phase.</p> <p>Raises:</p> Type Description <code>CollaborationError</code> <p>If phase execution fails.</p> Source code in <code>src\\ai_ensemble_suite\\collaboration\\async_thinking.py</code> <pre><code>async def execute(\n    self, \n    query: str,\n    context: Dict[str, Any],\n    trace_collector: Optional[TraceCollector] = None\n) -&gt; Dict[str, Any]:\n    \"\"\"Execute the Asynchronous Thinking phase.\n\n    Args:\n        query: The user query to process.\n        context: Context information from previous phases.\n        trace_collector: Optional trace collector for gathering execution details.\n\n    Returns:\n        Dictionary containing:\n            output: Dictionary mapping model IDs to their responses.\n            context: Updated context for the next phase.\n\n    Raises:\n        CollaborationError: If phase execution fails.\n    \"\"\"\n    start_time = time.time()\n\n    try:\n        # Get prompt template or use the query directly\n        prompt = query\n        if self._prompt_template:\n            try:\n                # Format prompt with query and any inputs from previous phases\n                inputs = self._get_inputs_from_context(context)\n                context_vars = {\"query\": query, **inputs}\n                prompt = self.render_template(self._prompt_template, context_vars)\n            except (ConfigurationError, KeyError) as e:\n                raise CollaborationError(f\"Failed to format prompt: {str(e)}\")\n\n        # Run models concurrently\n        model_results = await self._run_models(\n            prompt=prompt,\n            trace_collector=trace_collector\n        )\n\n        # Process outputs for simpler consumption by next phases\n        processed_outputs: Dict[str, str] = {}\n\n        for model_id, result in model_results.items():\n            processed_outputs[model_id] = result.get(\"text\", \"\")\n\n        # Select the primary output for single model case\n        primary_output = \"\"\n        if len(processed_outputs) == 1:\n            primary_output = list(processed_outputs.values())[0]\n        elif self._model_ids and len(self._model_ids) &gt; 0:\n            # Use the first model in the model_ids list as primary if available\n            primary_model = self._model_ids[0]\n            if primary_model in processed_outputs:\n                primary_output = processed_outputs[primary_model]\n\n        execution_time = time.time() - start_time\n\n        # Log completion\n        logger.info(\n            f\"AsyncThinking phase '{self._phase_name}' completed in {execution_time:.2f}s\",\n            extra={\n                \"model_count\": len(model_results),\n                \"phase\": self._phase_name\n            }\n        )\n\n        # Add phase trace if collector is provided\n        if trace_collector:\n            trace_collector.add_phase_trace(\n                phase_name=self._phase_name,\n                input_data={\"query\": query, \"prompt\": prompt},\n                output_data={\"outputs\": processed_outputs},\n                execution_time=execution_time,\n                phase_parameters=self._config\n            )\n\n        # Calculate a combined confidence score (average of all models)\n        confidence_values = []\n        for result in model_results.values():\n            if \"confidence\" in result:\n                if isinstance(result[\"confidence\"], dict) and \"combined\" in result[\"confidence\"]:\n                    confidence_values.append(result[\"confidence\"][\"combined\"])\n                elif isinstance(result[\"confidence\"], (float, int)):\n                    confidence_values.append(result[\"confidence\"])\n\n        confidence_score = sum(confidence_values) / len(confidence_values) if confidence_values else 0.7\n\n        # Return results\n        return {\n            \"output\": primary_output,  \n            \"outputs\": processed_outputs,\n            \"confidence\": confidence_score,\n            \"raw_results\": model_results  # Include full model results for advanced use\n        }\n\n    except Exception as e:\n        raise CollaborationError(\n            f\"AsyncThinking phase '{self._phase_name}' failed: {str(e)}\"\n        )\n</code></pre>"},{"location":"api/collaboration/#ai_ensemble_suite.collaboration.Bagging","title":"<code>Bagging</code>","text":"<p>               Bases: <code>BaseCollaborationPhase</code></p> <p>Bagging (Bootstrap Aggregating) collaboration phase.</p> <p>Models process different variations of the same input, then their outputs are aggregated to produce a more robust result. This reduces variance and increases stability in the ensemble's output.</p> Source code in <code>src\\ai_ensemble_suite\\collaboration\\bagging.py</code> <pre><code>class Bagging(BaseCollaborationPhase):\n    \"\"\"Bagging (Bootstrap Aggregating) collaboration phase.\n\n    Models process different variations of the same input, then their outputs\n    are aggregated to produce a more robust result. This reduces variance and\n    increases stability in the ensemble's output.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_manager: \"ModelManager\",\n        config_manager: \"ConfigManager\",\n        phase_name: str\n    ) -&gt; None:\n        \"\"\"Initialize the Bagging collaboration phase.\n\n        Args:\n            model_manager: The ModelManager instance.\n            config_manager: The ConfigManager instance.\n            phase_name: The name of the phase for configuration lookup.\n        \"\"\"\n        super().__init__(model_manager, config_manager, phase_name)\n\n        # Load bagging-specific configuration\n        self._sample_ratio = self._config.get(\"sample_ratio\", 0.8)\n        self._variation_strategy = self._config.get(\"variation_strategy\", \"token_sampling\")\n        self._aggregation_method = self._config.get(\"aggregation_method\", \"voting\")\n        self._num_variations = self._config.get(\"num_variations\", len(self._model_ids))\n\n        if self._num_variations &lt; 1:\n            raise ConfigurationError(f\"Bagging phase '{phase_name}' requires at least 1 variation\")\n\n        logger.debug(\n            f\"Configured Bagging phase with {self._num_variations} variations using '{self._variation_strategy}' strategy\",\n            extra={\"aggregation\": self._aggregation_method, \"sample_ratio\": self._sample_ratio}\n        )\n\n    def _generate_variations(self, text: str, n: int) -&gt; List[str]:\n        \"\"\"Generate input variations using the configured strategy.\n\n        Args:\n            text: The original input text.\n            n: Number of variations to generate.\n\n        Returns:\n            List of text variations.\n        \"\"\"\n        variations = []\n\n        if self._variation_strategy == \"token_sampling\":\n            # Sample tokens from the original text\n            words = text.split()\n            for _ in range(n):\n                # Sample ~sample_ratio proportion of words\n                sample_size = max(1, int(len(words) * self._sample_ratio))\n                sampled_words = random.sample(words, sample_size)\n                variations.append(\" \".join(sampled_words))\n\n        elif self._variation_strategy == \"segment_focus\":\n            # Focus on different segments of the text\n            if len(text) &lt; 50:  # For short texts, use token sampling instead\n                return self._generate_variations(text, n)\n\n            segments = max(3, n)  # At least 3 segments\n            segment_length = len(text) // segments\n\n            for i in range(n):\n                # Focus on a primary segment but include some context\n                primary_segment = i % segments\n                start = max(0, primary_segment * segment_length - segment_length//2)\n                end = min(len(text), (primary_segment + 1) * segment_length + segment_length//2)\n                variations.append(text[start:end])\n\n        elif self._variation_strategy == \"instruction_variation\":\n            # Create variations with different instructions/framing\n            instruction_variations = [\n                f\"Please answer this question: {text}\",\n                f\"Analyze the following: {text}\",\n                f\"Consider this query: {text}\",\n                f\"Respond to this input: {text}\",\n                f\"Evaluate this statement: {text}\"\n            ]\n\n            for i in range(n):\n                variations.append(instruction_variations[i % len(instruction_variations)])\n\n        else:  # Default or \"none\" - just use the original text repeated\n            variations = [text] * n\n\n        return variations\n\n    def _aggregate_outputs(self, outputs: List[str]) -&gt; str:\n        \"\"\"Aggregate multiple outputs into a single result.\n\n        Args:\n            outputs: List of model outputs to aggregate.\n\n        Returns:\n            Aggregated output string.\n        \"\"\"\n        if self._aggregation_method == \"voting\":\n            # Simple majority voting - works best for classification or short responses\n            if all(len(output.strip()) &lt; 100 for output in outputs):\n                counter = Counter(outputs)\n                return counter.most_common(1)[0][0]\n            else:\n                # For longer texts, voting might not make sense\n                # Fall back to the first output\n                logger.warning(\"Voting aggregation not suitable for long texts, using primary output instead\")\n                return outputs[0]\n\n        elif self._aggregation_method == \"concatenation\":\n            # Join all outputs with a delimiter\n            return \"\\n\\n===== NEXT RESPONSE =====\\n\\n\".join(outputs)\n\n        elif self._aggregation_method == \"summarization\":\n            # This would require an additional model call to summarize\n            # For now, we'll use a simple combination approach\n            if len(outputs) &lt;= 2:\n                return \"\\n\\n\".join(outputs)\n\n            # Take the first sentence from each output for a quick summary\n            summary_parts = []\n            for output in outputs:\n                first_sentence = output.split(\".\")[0] + \".\" if \".\" in output else output\n                summary_parts.append(first_sentence)\n\n            return \" \".join(summary_parts)\n\n        else:  # Default to first output\n            return outputs[0] if outputs else \"\"\n\n    async def execute(\n        self,\n        query: str,\n        context: Dict[str, Any],\n        trace_collector: Optional[TraceCollector] = None\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Execute the Bagging phase.\n\n        Args:\n            query: The user query to process.\n            context: Context information from previous phases.\n            trace_collector: Optional trace collector for gathering execution details.\n\n        Returns:\n            Dictionary containing:\n                output: The aggregated output from all variations.\n                outputs: Dictionary mapping variation IDs to individual responses.\n                context: Updated context for the next phase.\n\n        Raises:\n            CollaborationError: If phase execution fails.\n        \"\"\"\n        start_time = time.time()\n\n        try:\n            # Prepare the base prompt\n            base_prompt = query\n            if self._prompt_template:\n                try:\n                    inputs = self._get_inputs_from_context(context)\n                    context_vars = {\"query\": query, **inputs}\n                    base_prompt = self.render_template(self._prompt_template, context_vars)\n                except (ConfigurationError, KeyError) as e:\n                    raise CollaborationError(f\"Failed to format prompt: {str(e)}\")\n\n            # Generate variations of the prompt\n            variations = self._generate_variations(base_prompt, self._num_variations)\n\n            # Execute each variation with a model\n            all_outputs = {}\n            raw_results = {}\n\n            for i, variant_prompt in enumerate(variations):\n                # Use modulo in case we have more variations than models\n                model_idx = i % len(self._model_ids)\n                model_id = self._model_ids[model_idx]\n\n                # Run individual model on this variation\n                result = await self._run_models(\n                    prompt=variant_prompt,\n                    model_ids=[model_id],\n                    trace_collector=trace_collector\n                )\n\n                # Store the result\n                variation_id = f\"variation_{i+1}\"\n                all_outputs[variation_id] = result[model_id].get(\"text\", \"\")\n                raw_results[variation_id] = result\n\n                # Add trace for this variation if collector is provided\n                if trace_collector:\n                    trace_collector.add_model_trace(\n                        model_id=f\"{model_id}_bagging_{i+1}\",\n                        input_prompt=variant_prompt,\n                        output=result[model_id],\n                        execution_time=result[model_id].get(\"generation_time\", 0),\n                        parameters={\"variation_id\": variation_id}\n                    )\n\n            # Aggregate the outputs into a single result\n            aggregated_output = self._aggregate_outputs(list(all_outputs.values()))\n\n            execution_time = time.time() - start_time\n\n            # Calculate agreement score as a confidence measure\n            if len(all_outputs) &gt; 1:\n                # Count matching words across outputs\n                word_sets = [set(output.lower().split()) for output in all_outputs.values()]\n                union_words = set().union(*word_sets)\n                if not union_words:\n                    agreement_score = 1.0\n                else:\n                    # Count how many outputs contain each word\n                    word_counts = {}\n                    for word in union_words:\n                        word_counts[word] = sum(1 for words in word_sets if word in words)\n\n                    # Average agreement across all words\n                    avg_agreement = sum(word_counts.values()) / (len(word_counts) * len(all_outputs))\n                    agreement_score = avg_agreement\n            else:\n                agreement_score = 1.0\n\n            # Log completion\n            logger.info(\n                f\"Bagging phase '{self._phase_name}' completed in {execution_time:.2f}s\",\n                extra={\n                    \"variation_count\": len(variations),\n                    \"phase\": self._phase_name,\n                    \"agreement_score\": agreement_score\n                }\n            )\n\n            # Add phase trace if collector is provided\n            if trace_collector:\n                trace_collector.add_phase_trace(\n                    phase_name=self._phase_name,\n                    input_data={\"query\": query, \"base_prompt\": base_prompt},\n                    output_data={\"aggregated_output\": aggregated_output, \"variation_outputs\": all_outputs},\n                    execution_time=execution_time,\n                    phase_parameters={\n                        \"variation_strategy\": self._variation_strategy,\n                        \"sample_ratio\": self._sample_ratio,\n                        \"aggregation_method\": self._aggregation_method,\n                        \"num_variations\": self._num_variations\n                    }\n                )\n\n            # Return results\n            return {\n                \"output\": aggregated_output,\n                \"outputs\": all_outputs,\n                \"confidence\": agreement_score,\n                \"raw_results\": raw_results,\n                \"variation_count\": len(variations)\n            }\n\n        except Exception as e:\n            raise CollaborationError(\n                f\"Bagging phase '{self._phase_name}' failed: {str(e)}\"\n            )\n</code></pre>"},{"location":"api/collaboration/#ai_ensemble_suite.collaboration.Bagging.__init__","title":"<code>__init__(model_manager, config_manager, phase_name)</code>","text":"<p>Initialize the Bagging collaboration phase.</p> <p>Parameters:</p> Name Type Description Default <code>model_manager</code> <code>ModelManager</code> <p>The ModelManager instance.</p> required <code>config_manager</code> <code>ConfigManager</code> <p>The ConfigManager instance.</p> required <code>phase_name</code> <code>str</code> <p>The name of the phase for configuration lookup.</p> required Source code in <code>src\\ai_ensemble_suite\\collaboration\\bagging.py</code> <pre><code>def __init__(\n    self,\n    model_manager: \"ModelManager\",\n    config_manager: \"ConfigManager\",\n    phase_name: str\n) -&gt; None:\n    \"\"\"Initialize the Bagging collaboration phase.\n\n    Args:\n        model_manager: The ModelManager instance.\n        config_manager: The ConfigManager instance.\n        phase_name: The name of the phase for configuration lookup.\n    \"\"\"\n    super().__init__(model_manager, config_manager, phase_name)\n\n    # Load bagging-specific configuration\n    self._sample_ratio = self._config.get(\"sample_ratio\", 0.8)\n    self._variation_strategy = self._config.get(\"variation_strategy\", \"token_sampling\")\n    self._aggregation_method = self._config.get(\"aggregation_method\", \"voting\")\n    self._num_variations = self._config.get(\"num_variations\", len(self._model_ids))\n\n    if self._num_variations &lt; 1:\n        raise ConfigurationError(f\"Bagging phase '{phase_name}' requires at least 1 variation\")\n\n    logger.debug(\n        f\"Configured Bagging phase with {self._num_variations} variations using '{self._variation_strategy}' strategy\",\n        extra={\"aggregation\": self._aggregation_method, \"sample_ratio\": self._sample_ratio}\n    )\n</code></pre>"},{"location":"api/collaboration/#ai_ensemble_suite.collaboration.Bagging.execute","title":"<code>execute(query, context, trace_collector=None)</code>  <code>async</code>","text":"<p>Execute the Bagging phase.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The user query to process.</p> required <code>context</code> <code>Dict[str, Any]</code> <p>Context information from previous phases.</p> required <code>trace_collector</code> <code>Optional[TraceCollector]</code> <p>Optional trace collector for gathering execution details.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary containing: output: The aggregated output from all variations. outputs: Dictionary mapping variation IDs to individual responses. context: Updated context for the next phase.</p> <p>Raises:</p> Type Description <code>CollaborationError</code> <p>If phase execution fails.</p> Source code in <code>src\\ai_ensemble_suite\\collaboration\\bagging.py</code> <pre><code>async def execute(\n    self,\n    query: str,\n    context: Dict[str, Any],\n    trace_collector: Optional[TraceCollector] = None\n) -&gt; Dict[str, Any]:\n    \"\"\"Execute the Bagging phase.\n\n    Args:\n        query: The user query to process.\n        context: Context information from previous phases.\n        trace_collector: Optional trace collector for gathering execution details.\n\n    Returns:\n        Dictionary containing:\n            output: The aggregated output from all variations.\n            outputs: Dictionary mapping variation IDs to individual responses.\n            context: Updated context for the next phase.\n\n    Raises:\n        CollaborationError: If phase execution fails.\n    \"\"\"\n    start_time = time.time()\n\n    try:\n        # Prepare the base prompt\n        base_prompt = query\n        if self._prompt_template:\n            try:\n                inputs = self._get_inputs_from_context(context)\n                context_vars = {\"query\": query, **inputs}\n                base_prompt = self.render_template(self._prompt_template, context_vars)\n            except (ConfigurationError, KeyError) as e:\n                raise CollaborationError(f\"Failed to format prompt: {str(e)}\")\n\n        # Generate variations of the prompt\n        variations = self._generate_variations(base_prompt, self._num_variations)\n\n        # Execute each variation with a model\n        all_outputs = {}\n        raw_results = {}\n\n        for i, variant_prompt in enumerate(variations):\n            # Use modulo in case we have more variations than models\n            model_idx = i % len(self._model_ids)\n            model_id = self._model_ids[model_idx]\n\n            # Run individual model on this variation\n            result = await self._run_models(\n                prompt=variant_prompt,\n                model_ids=[model_id],\n                trace_collector=trace_collector\n            )\n\n            # Store the result\n            variation_id = f\"variation_{i+1}\"\n            all_outputs[variation_id] = result[model_id].get(\"text\", \"\")\n            raw_results[variation_id] = result\n\n            # Add trace for this variation if collector is provided\n            if trace_collector:\n                trace_collector.add_model_trace(\n                    model_id=f\"{model_id}_bagging_{i+1}\",\n                    input_prompt=variant_prompt,\n                    output=result[model_id],\n                    execution_time=result[model_id].get(\"generation_time\", 0),\n                    parameters={\"variation_id\": variation_id}\n                )\n\n        # Aggregate the outputs into a single result\n        aggregated_output = self._aggregate_outputs(list(all_outputs.values()))\n\n        execution_time = time.time() - start_time\n\n        # Calculate agreement score as a confidence measure\n        if len(all_outputs) &gt; 1:\n            # Count matching words across outputs\n            word_sets = [set(output.lower().split()) for output in all_outputs.values()]\n            union_words = set().union(*word_sets)\n            if not union_words:\n                agreement_score = 1.0\n            else:\n                # Count how many outputs contain each word\n                word_counts = {}\n                for word in union_words:\n                    word_counts[word] = sum(1 for words in word_sets if word in words)\n\n                # Average agreement across all words\n                avg_agreement = sum(word_counts.values()) / (len(word_counts) * len(all_outputs))\n                agreement_score = avg_agreement\n        else:\n            agreement_score = 1.0\n\n        # Log completion\n        logger.info(\n            f\"Bagging phase '{self._phase_name}' completed in {execution_time:.2f}s\",\n            extra={\n                \"variation_count\": len(variations),\n                \"phase\": self._phase_name,\n                \"agreement_score\": agreement_score\n            }\n        )\n\n        # Add phase trace if collector is provided\n        if trace_collector:\n            trace_collector.add_phase_trace(\n                phase_name=self._phase_name,\n                input_data={\"query\": query, \"base_prompt\": base_prompt},\n                output_data={\"aggregated_output\": aggregated_output, \"variation_outputs\": all_outputs},\n                execution_time=execution_time,\n                phase_parameters={\n                    \"variation_strategy\": self._variation_strategy,\n                    \"sample_ratio\": self._sample_ratio,\n                    \"aggregation_method\": self._aggregation_method,\n                    \"num_variations\": self._num_variations\n                }\n            )\n\n        # Return results\n        return {\n            \"output\": aggregated_output,\n            \"outputs\": all_outputs,\n            \"confidence\": agreement_score,\n            \"raw_results\": raw_results,\n            \"variation_count\": len(variations)\n        }\n\n    except Exception as e:\n        raise CollaborationError(\n            f\"Bagging phase '{self._phase_name}' failed: {str(e)}\"\n        )\n</code></pre>"},{"location":"api/collaboration/#ai_ensemble_suite.collaboration.BaseCollaborationPhase","title":"<code>BaseCollaborationPhase</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for collaboration phases.</p> <p>Defines the interface for collaboration phases and provides common functionality.</p> Source code in <code>src\\ai_ensemble_suite\\collaboration\\base.py</code> <pre><code>class BaseCollaborationPhase(ABC):\n    \"\"\"Abstract base class for collaboration phases.\n\n    Defines the interface for collaboration phases and provides common functionality.\n    \"\"\"\n\n    def __init__(\n            self,\n            model_manager: \"ModelManager\",\n            config_manager: \"ConfigManager\",\n            phase_name: str\n    ) -&gt; None:\n        \"\"\"Initialize the collaboration phase.\n\n        Args:\n            model_manager: The ModelManager instance.\n            config_manager: The ConfigManager instance.\n            phase_name: The name of the phase for configuration lookup.\n\n        Raises:\n            ConfigurationError: If phase configuration is invalid.\n        \"\"\"\n        self._model_manager = model_manager\n        self._config_manager = config_manager\n        self._phase_name = phase_name\n\n        # Get reference to template manager through model_manager.ensemble\n        self._template_manager = None\n        if hasattr(model_manager, 'ensemble') and model_manager.ensemble and hasattr(model_manager.ensemble,\n                                                                                     'template_manager'):\n            self._template_manager = model_manager.ensemble.template_manager\n\n        # Load phase configuration (existing code)\n        try:\n            self._config = self._config_manager.get_collaboration_config(phase_name)\n        except ConfigurationError as e:\n            raise ConfigurationError(f\"Failed to load configuration for phase '{phase_name}': {str(e)}\")\n\n        # Get phase type\n        self._phase_type = self._config.get(\"type\")\n        if not self._phase_type:\n            raise ConfigurationError(f\"Phase '{phase_name}' is missing required field: type\")\n\n        # Get model IDs for this phase\n        self._model_ids = self._config.get(\"models\", [])\n\n        # Get input sources (previous phases)\n        self._input_from = self._config.get(\"input_from\", [])\n        if isinstance(self._input_from, str):\n            self._input_from = [self._input_from]\n\n        # Get prompt template name\n        self._prompt_template = self._config.get(\"prompt_template\")\n\n        logger.debug(\n            f\"Initialized collaboration phase '{phase_name}' of type '{self._phase_type}'\",\n            extra={\"models\": self._model_ids}\n        )\n\n    @abstractmethod\n    async def execute(\n        self, \n        query: str,\n        context: Dict[str, Any],\n        trace_collector: Optional[TraceCollector] = None\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Execute the collaboration phase.\n\n        Args:\n            query: The user query to process.\n            context: Context information from previous phases.\n            trace_collector: Optional trace collector for gathering execution details.\n\n        Returns:\n            Dictionary containing:\n                output: The phase output.\n                context: Updated context for the next phase.\n\n        Raises:\n            CollaborationError: If phase execution fails.\n        \"\"\"\n        pass\n\n    def get_config(self) -&gt; Dict[str, Any]:\n        \"\"\"Get the phase configuration.\n\n        Returns:\n            Dictionary containing the phase configuration.\n        \"\"\"\n        return self._config.copy()\n\n    def get_name(self) -&gt; str:\n        \"\"\"Get the phase name.\n\n        Returns:\n            The phase name.\n        \"\"\"\n        return self._phase_name\n\n    def get_type(self) -&gt; str:\n        \"\"\"Get the phase type.\n\n        Returns:\n            The phase type.\n        \"\"\"\n        return self._phase_type\n\n    def get_required_models(self) -&gt; Set[str]:\n        \"\"\"Get the models required by this phase.\n\n        Returns:\n            Set of model IDs required by this phase.\n        \"\"\"\n        return set(self._model_ids)\n\n    def get_input_phases(self) -&gt; List[str]:\n        \"\"\"Get the input phases this phase depends on.\n\n        Returns:\n            List of phase names this phase takes input from.\n        \"\"\"\n        return list(self._input_from)\n\n    def render_template(\n            self,\n            template_name: str,\n            context: Dict[str, Any]\n    ) -&gt; str:\n        \"\"\"Render a template using Jinja2 templating.\n\n        Args:\n            template_name: The name of the template to render.\n            context: Dictionary of context variables for rendering.\n\n        Returns:\n            The rendered template string.\n\n        Raises:\n            ConfigurationError: If template is not found.\n            ValueError: If template rendering fails.\n        \"\"\"\n        if self._template_manager:\n            # Use the template manager if available\n            return self._template_manager.render_template(template_name, context)\n\n    # def format_prompt(\n    #         self,\n    #         template_name: str,\n    #         **kwargs: Any\n    # ) -&gt; str:\n    #     \"\"\"Format a prompt template with the provided values using simple substitution.\n    #\n    #     Note:\n    #         For more advanced templating features, use render_template() instead.\n    #\n    #     Args:\n    #         template_name: The name of the template to format.\n    #         **kwargs: Values to format the template with.\n    #\n    #     Returns:\n    #         The formatted prompt.\n    #\n    #     Raises:\n    #         ConfigurationError: If the template does not exist.\n    #     \"\"\"\n    #     template = self._config_manager.get_template(template_name)\n    #     if template is None:\n    #         raise ConfigurationError(f\"Template not found: {template_name}\")\n    #\n    #     return format_prompt(template, **kwargs)\n\n    async def _run_models(\n        self, \n        prompt: str,\n        model_ids: Optional[List[str]] = None,\n        trace_collector: Optional[TraceCollector] = None,\n        **kwargs: Any\n    ) -&gt; Dict[str, Dict[str, Any]]:\n        \"\"\"Run inference on specific models or all phase models.\n\n        Args:\n            prompt: The prompt to send to the models.\n            model_ids: Optional list of model IDs to use. If None, uses all phase models.\n            trace_collector: Optional trace collector for gathering execution details.\n            **kwargs: Additional parameters for model inference.\n\n        Returns:\n            Dictionary mapping model IDs to their outputs.\n\n        Raises:\n            CollaborationError: If model inference fails.\n        \"\"\"\n        ids_to_use = model_ids if model_ids is not None else self._model_ids\n        if not ids_to_use:\n            raise CollaborationError(f\"No models specified for phase '{self._phase_name}'\")\n\n        try:\n            start_time = time.time()\n            results = await self._model_manager.run_all_models(prompt, ids_to_use, **kwargs)\n\n            # Add traces if collector is provided\n            if trace_collector:\n                for model_id, result in results.items():\n                    trace_collector.add_model_trace(\n                        model_id=model_id,\n                        input_prompt=prompt,\n                        output=result,\n                        execution_time=result.get(\"generation_time\", 0),\n                        parameters=kwargs\n                    )\n\n            return results\n\n        except Exception as e:\n            raise CollaborationError(\n                f\"Failed to run models for phase '{self._phase_name}': {str(e)}\"\n            )\n\n    def _get_inputs_from_context(\n        self, \n        context: Dict[str, Any]\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Extract inputs from context based on input_from configuration.\n\n        Args:\n            context: The context dictionary containing previous phase outputs.\n\n        Returns:\n            Dictionary containing inputs for this phase.\n\n        Raises:\n            CollaborationError: If a required input is missing.\n        \"\"\"\n        inputs = {}\n\n        # If no input_from is specified, return empty inputs\n        if not self._input_from:\n            return inputs\n\n        # Extract inputs from context\n        for phase_name in self._input_from:\n            if phase_name not in context:\n                raise CollaborationError(\n                    f\"Phase '{self._phase_name}' requires input from phase '{phase_name}', \"\n                    \"but it is not available in the context\"\n                )\n\n            inputs[phase_name] = context[phase_name]\n\n        return inputs\n</code></pre>"},{"location":"api/collaboration/#ai_ensemble_suite.collaboration.BaseCollaborationPhase.__init__","title":"<code>__init__(model_manager, config_manager, phase_name)</code>","text":"<p>Initialize the collaboration phase.</p> <p>Parameters:</p> Name Type Description Default <code>model_manager</code> <code>ModelManager</code> <p>The ModelManager instance.</p> required <code>config_manager</code> <code>ConfigManager</code> <p>The ConfigManager instance.</p> required <code>phase_name</code> <code>str</code> <p>The name of the phase for configuration lookup.</p> required <p>Raises:</p> Type Description <code>ConfigurationError</code> <p>If phase configuration is invalid.</p> Source code in <code>src\\ai_ensemble_suite\\collaboration\\base.py</code> <pre><code>def __init__(\n        self,\n        model_manager: \"ModelManager\",\n        config_manager: \"ConfigManager\",\n        phase_name: str\n) -&gt; None:\n    \"\"\"Initialize the collaboration phase.\n\n    Args:\n        model_manager: The ModelManager instance.\n        config_manager: The ConfigManager instance.\n        phase_name: The name of the phase for configuration lookup.\n\n    Raises:\n        ConfigurationError: If phase configuration is invalid.\n    \"\"\"\n    self._model_manager = model_manager\n    self._config_manager = config_manager\n    self._phase_name = phase_name\n\n    # Get reference to template manager through model_manager.ensemble\n    self._template_manager = None\n    if hasattr(model_manager, 'ensemble') and model_manager.ensemble and hasattr(model_manager.ensemble,\n                                                                                 'template_manager'):\n        self._template_manager = model_manager.ensemble.template_manager\n\n    # Load phase configuration (existing code)\n    try:\n        self._config = self._config_manager.get_collaboration_config(phase_name)\n    except ConfigurationError as e:\n        raise ConfigurationError(f\"Failed to load configuration for phase '{phase_name}': {str(e)}\")\n\n    # Get phase type\n    self._phase_type = self._config.get(\"type\")\n    if not self._phase_type:\n        raise ConfigurationError(f\"Phase '{phase_name}' is missing required field: type\")\n\n    # Get model IDs for this phase\n    self._model_ids = self._config.get(\"models\", [])\n\n    # Get input sources (previous phases)\n    self._input_from = self._config.get(\"input_from\", [])\n    if isinstance(self._input_from, str):\n        self._input_from = [self._input_from]\n\n    # Get prompt template name\n    self._prompt_template = self._config.get(\"prompt_template\")\n\n    logger.debug(\n        f\"Initialized collaboration phase '{phase_name}' of type '{self._phase_type}'\",\n        extra={\"models\": self._model_ids}\n    )\n</code></pre>"},{"location":"api/collaboration/#ai_ensemble_suite.collaboration.BaseCollaborationPhase.execute","title":"<code>execute(query, context, trace_collector=None)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Execute the collaboration phase.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The user query to process.</p> required <code>context</code> <code>Dict[str, Any]</code> <p>Context information from previous phases.</p> required <code>trace_collector</code> <code>Optional[TraceCollector]</code> <p>Optional trace collector for gathering execution details.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary containing: output: The phase output. context: Updated context for the next phase.</p> <p>Raises:</p> Type Description <code>CollaborationError</code> <p>If phase execution fails.</p> Source code in <code>src\\ai_ensemble_suite\\collaboration\\base.py</code> <pre><code>@abstractmethod\nasync def execute(\n    self, \n    query: str,\n    context: Dict[str, Any],\n    trace_collector: Optional[TraceCollector] = None\n) -&gt; Dict[str, Any]:\n    \"\"\"Execute the collaboration phase.\n\n    Args:\n        query: The user query to process.\n        context: Context information from previous phases.\n        trace_collector: Optional trace collector for gathering execution details.\n\n    Returns:\n        Dictionary containing:\n            output: The phase output.\n            context: Updated context for the next phase.\n\n    Raises:\n        CollaborationError: If phase execution fails.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/collaboration/#ai_ensemble_suite.collaboration.BaseCollaborationPhase.get_config","title":"<code>get_config()</code>","text":"<p>Get the phase configuration.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary containing the phase configuration.</p> Source code in <code>src\\ai_ensemble_suite\\collaboration\\base.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Get the phase configuration.\n\n    Returns:\n        Dictionary containing the phase configuration.\n    \"\"\"\n    return self._config.copy()\n</code></pre>"},{"location":"api/collaboration/#ai_ensemble_suite.collaboration.BaseCollaborationPhase.get_input_phases","title":"<code>get_input_phases()</code>","text":"<p>Get the input phases this phase depends on.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of phase names this phase takes input from.</p> Source code in <code>src\\ai_ensemble_suite\\collaboration\\base.py</code> <pre><code>def get_input_phases(self) -&gt; List[str]:\n    \"\"\"Get the input phases this phase depends on.\n\n    Returns:\n        List of phase names this phase takes input from.\n    \"\"\"\n    return list(self._input_from)\n</code></pre>"},{"location":"api/collaboration/#ai_ensemble_suite.collaboration.BaseCollaborationPhase.get_name","title":"<code>get_name()</code>","text":"<p>Get the phase name.</p> <p>Returns:</p> Type Description <code>str</code> <p>The phase name.</p> Source code in <code>src\\ai_ensemble_suite\\collaboration\\base.py</code> <pre><code>def get_name(self) -&gt; str:\n    \"\"\"Get the phase name.\n\n    Returns:\n        The phase name.\n    \"\"\"\n    return self._phase_name\n</code></pre>"},{"location":"api/collaboration/#ai_ensemble_suite.collaboration.BaseCollaborationPhase.get_required_models","title":"<code>get_required_models()</code>","text":"<p>Get the models required by this phase.</p> <p>Returns:</p> Type Description <code>Set[str]</code> <p>Set of model IDs required by this phase.</p> Source code in <code>src\\ai_ensemble_suite\\collaboration\\base.py</code> <pre><code>def get_required_models(self) -&gt; Set[str]:\n    \"\"\"Get the models required by this phase.\n\n    Returns:\n        Set of model IDs required by this phase.\n    \"\"\"\n    return set(self._model_ids)\n</code></pre>"},{"location":"api/collaboration/#ai_ensemble_suite.collaboration.BaseCollaborationPhase.get_type","title":"<code>get_type()</code>","text":"<p>Get the phase type.</p> <p>Returns:</p> Type Description <code>str</code> <p>The phase type.</p> Source code in <code>src\\ai_ensemble_suite\\collaboration\\base.py</code> <pre><code>def get_type(self) -&gt; str:\n    \"\"\"Get the phase type.\n\n    Returns:\n        The phase type.\n    \"\"\"\n    return self._phase_type\n</code></pre>"},{"location":"api/collaboration/#ai_ensemble_suite.collaboration.BaseCollaborationPhase.render_template","title":"<code>render_template(template_name, context)</code>","text":"<p>Render a template using Jinja2 templating.</p> <p>Parameters:</p> Name Type Description Default <code>template_name</code> <code>str</code> <p>The name of the template to render.</p> required <code>context</code> <code>Dict[str, Any]</code> <p>Dictionary of context variables for rendering.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The rendered template string.</p> <p>Raises:</p> Type Description <code>ConfigurationError</code> <p>If template is not found.</p> <code>ValueError</code> <p>If template rendering fails.</p> Source code in <code>src\\ai_ensemble_suite\\collaboration\\base.py</code> <pre><code>def render_template(\n        self,\n        template_name: str,\n        context: Dict[str, Any]\n) -&gt; str:\n    \"\"\"Render a template using Jinja2 templating.\n\n    Args:\n        template_name: The name of the template to render.\n        context: Dictionary of context variables for rendering.\n\n    Returns:\n        The rendered template string.\n\n    Raises:\n        ConfigurationError: If template is not found.\n        ValueError: If template rendering fails.\n    \"\"\"\n    if self._template_manager:\n        # Use the template manager if available\n        return self._template_manager.render_template(template_name, context)\n</code></pre>"},{"location":"api/collaboration/#ai_ensemble_suite.collaboration.BaseDebate","title":"<code>BaseDebate</code>","text":"<p>               Bases: <code>BaseCollaborationPhase</code></p> <p>Base class for structured debate collaboration phases.</p> <p>Provides common functionality for different debate patterns.</p> Source code in <code>src\\ai_ensemble_suite\\collaboration\\structured_debate\\base_debate.py</code> <pre><code>class BaseDebate(BaseCollaborationPhase):\n    \"\"\"Base class for structured debate collaboration phases.\n\n    Provides common functionality for different debate patterns.\n    \"\"\"\n\n    def __init__(\n        self, \n        model_manager: \"ModelManager\",\n        config_manager: \"ConfigManager\",\n        phase_name: str\n    ) -&gt; None:\n        \"\"\"Initialize the base debate phase.\n\n        Args:\n            model_manager: The ModelManager instance.\n            config_manager: The ConfigManager instance.\n            phase_name: The name of the phase for configuration lookup.\n\n        Raises:\n            ConfigurationError: If phase configuration is invalid.\n        \"\"\"\n        super().__init__(model_manager, config_manager, phase_name)\n\n        # Get debate-specific configuration\n        self._subtype = self._config.get(\"subtype\", \"critique\")\n\n        # Get debate rounds (default to 1)\n        self._rounds = self._config.get(\"rounds\", 1)\n\n        # Validate rounds\n        if not isinstance(self._rounds, int) or self._rounds &lt; 1:\n            logger.warning(\n                f\"Invalid rounds value for phase '{phase_name}', defaulting to 1\"\n            )\n            self._rounds = 1\n\n        # Track rounds in debugging\n        logger.debug(\n            f\"Initialized debate phase '{phase_name}' with subtype '{self._subtype}' \"\n            f\"and {self._rounds} rounds\"\n        )\n\n    async def execute(\n        self, \n        query: str,\n        context: Dict[str, Any],\n        trace_collector: Optional[TraceCollector] = None\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Execute the debate phase.\n\n        Args:\n            query: The user query to process.\n            context: Context information from previous phases.\n            trace_collector: Optional trace collector for gathering execution details.\n\n        Returns:\n            Dictionary containing:\n                output: The debate output (typically from the final round).\n                context: Updated context including intermediate debate exchanges.\n\n        Raises:\n            CollaborationError: If phase execution fails.\n        \"\"\"\n        start_time = time.time()\n\n        try:\n            # Get inputs from previous phases\n            inputs = self._get_inputs_from_context(context)\n\n            # Validate inputs\n            if not inputs and self._input_from:\n                raise CollaborationError(\n                    f\"Debate phase '{self._phase_name}' requires inputs from \"\n                    f\"previous phases: {', '.join(self._input_from)}\"\n                )\n\n            # Get or create the initial response that will be critiqued\n            initial_response = self._get_initial_response(inputs)\n            if not initial_response:\n                raise CollaborationError(\n                    f\"Debate phase '{self._phase_name}' could not determine \"\n                    \"an initial response to critique\"\n                )\n\n            # Execute the appropriate debate pattern\n            debate_results = await self._execute_debate_pattern(\n                query, \n                initial_response, \n                inputs,\n                trace_collector\n            )\n\n            execution_time = time.time() - start_time\n\n            # Log completion\n            logger.info(\n                f\"Debate phase '{self._phase_name}' completed in {execution_time:.2f}s\",\n                extra={\n                    \"debate_type\": self._subtype,\n                    \"rounds\": self._rounds,\n                    \"phase\": self._phase_name\n                }\n            )\n\n            # Add phase trace if collector is provided\n            if trace_collector:\n                trace_collector.add_phase_trace(\n                    phase_name=self._phase_name,\n                    input_data={\n                        \"query\": query, \n                        \"initial_response\": initial_response,\n                        \"inputs\": inputs\n                    },\n                    output_data=debate_results,\n                    execution_time=execution_time,\n                    phase_parameters=self._config\n                )\n\n            # Return results - ensure we include an \"output\" key for consistent interface\n            if \"output\" not in debate_results:\n                # Default to the final critique or perspective as the output\n                if \"critique\" in debate_results:\n                    debate_results[\"output\"] = debate_results[\"critique\"]\n                elif \"final_perspective\" in debate_results:\n                    debate_results[\"output\"] = debate_results[\"final_perspective\"]\n                elif \"exchanges\" in debate_results and debate_results[\"exchanges\"]:\n                    # Take the last exchange as the output\n                    debate_results[\"output\"] = debate_results[\"exchanges\"][-1][\"content\"]\n                else:\n                    # Fallback to empty string\n                    debate_results[\"output\"] = \"\"\n\n            return debate_results\n\n        except Exception as e:\n            raise CollaborationError(\n                f\"Debate phase '{self._phase_name}' failed: {str(e)}\"\n            )\n\n    def _get_initial_response(self, inputs: Dict[str, Any]) -&gt; str:\n        \"\"\"Get the initial response that will be critiqued.\n\n        Args:\n            inputs: Inputs from previous phases.\n\n        Returns:\n            The initial response as a string.\n        \"\"\"\n        # Check input_from configuration to determine source\n        if not self._input_from:\n            return \"\"\n\n        # Typically the first input source is the initial response\n        source_phase = self._input_from[0]\n\n        # Check if the source phase exists in inputs\n        if source_phase not in inputs:\n            logger.warning(\n                f\"Initial response source '{source_phase}' not found in inputs\"\n            )\n            return \"\"\n\n        # Extract the response from the source phase\n        source = inputs[source_phase]\n\n        # Handle different input formats\n        if isinstance(source, str):\n            # Direct string response\n            return source\n        elif isinstance(source, dict):\n            # Dictionary response - try to get the output field\n            if \"output\" in source:\n                return source[\"output\"]\n            # If no output field, try to find any string field\n            for key, value in source.items():\n                if isinstance(value, str):\n                    return value\n\n        # Fallback to empty string if no suitable response found\n        logger.warning(\n            f\"Could not extract initial response from source '{source_phase}'\"\n        )\n        return \"\"\n\n    @staticmethod\n    def _extract_key_points(text: str, max_points: int = 5) -&gt; List[str]:\n        \"\"\"Extract key points from a text.\n\n        This is a simple implementation that finds sentences containing strong indicators.\n\n        Args:\n            text: The text to extract points from.\n            max_points: Maximum number of points to extract.\n\n        Returns:\n            List of key points extracted from the text.\n        \"\"\"\n        # List of indicator phrases for key points\n        indicators = [\n            r'important',\n            r'significant',\n            r'key',\n            r'critical',\n            r'essential',\n            r'main',\n            r'primary',\n            r'fundamental',\n            r'primarily',\n            r'notably',\n            r'specifically',\n            r'particularly',\n            r'must',\n            r'should',\n            r'need to',\n            r'\\d+\\.',  # Numbered points like \"1.\"\n            r'firstly',\n            r'secondly',\n            r'lastly',\n            r'finally',\n            r'in conclusion',\n        ]\n\n        # Split text into sentences\n        sentences = re.split(r'(?&lt;=[.!?])\\s+', text)\n\n        # Filter sentences containing indicators\n        key_sentences = []\n        for sentence in sentences:\n            for indicator in indicators:\n                if re.search(r'\\b' + indicator + r'\\b', sentence, re.IGNORECASE):\n                    key_sentences.append(sentence.strip())\n                    break\n\n        # Limit to max_points\n        points = key_sentences[:max_points]\n\n        # If no points found, take the first few sentences\n        if not points and sentences:\n            points = sentences[:min(max_points, len(sentences))]\n\n        return points\n\n    async def _execute_debate_pattern(\n        self,\n        query: str,\n        initial_response: str,\n        inputs: Dict[str, Any],\n        trace_collector: Optional[TraceCollector] = None\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Execute the specific debate pattern defined by the subtype.\n\n        This method should be implemented by subclasses.\n\n        Args:\n            query: The user query to process.\n            initial_response: The initial response to critique.\n            inputs: Inputs from previous phases.\n            trace_collector: Optional trace collector.\n\n        Returns:\n            Dictionary containing debate results.\n\n        Raises:\n            NotImplementedError: If not implemented by subclass.\n        \"\"\"\n        raise NotImplementedError(\n            f\"Debate pattern '{self._subtype}' not implemented in base class\"\n        )\n</code></pre>"},{"location":"api/collaboration/#ai_ensemble_suite.collaboration.BaseDebate.__init__","title":"<code>__init__(model_manager, config_manager, phase_name)</code>","text":"<p>Initialize the base debate phase.</p> <p>Parameters:</p> Name Type Description Default <code>model_manager</code> <code>ModelManager</code> <p>The ModelManager instance.</p> required <code>config_manager</code> <code>ConfigManager</code> <p>The ConfigManager instance.</p> required <code>phase_name</code> <code>str</code> <p>The name of the phase for configuration lookup.</p> required <p>Raises:</p> Type Description <code>ConfigurationError</code> <p>If phase configuration is invalid.</p> Source code in <code>src\\ai_ensemble_suite\\collaboration\\structured_debate\\base_debate.py</code> <pre><code>def __init__(\n    self, \n    model_manager: \"ModelManager\",\n    config_manager: \"ConfigManager\",\n    phase_name: str\n) -&gt; None:\n    \"\"\"Initialize the base debate phase.\n\n    Args:\n        model_manager: The ModelManager instance.\n        config_manager: The ConfigManager instance.\n        phase_name: The name of the phase for configuration lookup.\n\n    Raises:\n        ConfigurationError: If phase configuration is invalid.\n    \"\"\"\n    super().__init__(model_manager, config_manager, phase_name)\n\n    # Get debate-specific configuration\n    self._subtype = self._config.get(\"subtype\", \"critique\")\n\n    # Get debate rounds (default to 1)\n    self._rounds = self._config.get(\"rounds\", 1)\n\n    # Validate rounds\n    if not isinstance(self._rounds, int) or self._rounds &lt; 1:\n        logger.warning(\n            f\"Invalid rounds value for phase '{phase_name}', defaulting to 1\"\n        )\n        self._rounds = 1\n\n    # Track rounds in debugging\n    logger.debug(\n        f\"Initialized debate phase '{phase_name}' with subtype '{self._subtype}' \"\n        f\"and {self._rounds} rounds\"\n    )\n</code></pre>"},{"location":"api/collaboration/#ai_ensemble_suite.collaboration.BaseDebate.execute","title":"<code>execute(query, context, trace_collector=None)</code>  <code>async</code>","text":"<p>Execute the debate phase.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The user query to process.</p> required <code>context</code> <code>Dict[str, Any]</code> <p>Context information from previous phases.</p> required <code>trace_collector</code> <code>Optional[TraceCollector]</code> <p>Optional trace collector for gathering execution details.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary containing: output: The debate output (typically from the final round). context: Updated context including intermediate debate exchanges.</p> <p>Raises:</p> Type Description <code>CollaborationError</code> <p>If phase execution fails.</p> Source code in <code>src\\ai_ensemble_suite\\collaboration\\structured_debate\\base_debate.py</code> <pre><code>async def execute(\n    self, \n    query: str,\n    context: Dict[str, Any],\n    trace_collector: Optional[TraceCollector] = None\n) -&gt; Dict[str, Any]:\n    \"\"\"Execute the debate phase.\n\n    Args:\n        query: The user query to process.\n        context: Context information from previous phases.\n        trace_collector: Optional trace collector for gathering execution details.\n\n    Returns:\n        Dictionary containing:\n            output: The debate output (typically from the final round).\n            context: Updated context including intermediate debate exchanges.\n\n    Raises:\n        CollaborationError: If phase execution fails.\n    \"\"\"\n    start_time = time.time()\n\n    try:\n        # Get inputs from previous phases\n        inputs = self._get_inputs_from_context(context)\n\n        # Validate inputs\n        if not inputs and self._input_from:\n            raise CollaborationError(\n                f\"Debate phase '{self._phase_name}' requires inputs from \"\n                f\"previous phases: {', '.join(self._input_from)}\"\n            )\n\n        # Get or create the initial response that will be critiqued\n        initial_response = self._get_initial_response(inputs)\n        if not initial_response:\n            raise CollaborationError(\n                f\"Debate phase '{self._phase_name}' could not determine \"\n                \"an initial response to critique\"\n            )\n\n        # Execute the appropriate debate pattern\n        debate_results = await self._execute_debate_pattern(\n            query, \n            initial_response, \n            inputs,\n            trace_collector\n        )\n\n        execution_time = time.time() - start_time\n\n        # Log completion\n        logger.info(\n            f\"Debate phase '{self._phase_name}' completed in {execution_time:.2f}s\",\n            extra={\n                \"debate_type\": self._subtype,\n                \"rounds\": self._rounds,\n                \"phase\": self._phase_name\n            }\n        )\n\n        # Add phase trace if collector is provided\n        if trace_collector:\n            trace_collector.add_phase_trace(\n                phase_name=self._phase_name,\n                input_data={\n                    \"query\": query, \n                    \"initial_response\": initial_response,\n                    \"inputs\": inputs\n                },\n                output_data=debate_results,\n                execution_time=execution_time,\n                phase_parameters=self._config\n            )\n\n        # Return results - ensure we include an \"output\" key for consistent interface\n        if \"output\" not in debate_results:\n            # Default to the final critique or perspective as the output\n            if \"critique\" in debate_results:\n                debate_results[\"output\"] = debate_results[\"critique\"]\n            elif \"final_perspective\" in debate_results:\n                debate_results[\"output\"] = debate_results[\"final_perspective\"]\n            elif \"exchanges\" in debate_results and debate_results[\"exchanges\"]:\n                # Take the last exchange as the output\n                debate_results[\"output\"] = debate_results[\"exchanges\"][-1][\"content\"]\n            else:\n                # Fallback to empty string\n                debate_results[\"output\"] = \"\"\n\n        return debate_results\n\n    except Exception as e:\n        raise CollaborationError(\n            f\"Debate phase '{self._phase_name}' failed: {str(e)}\"\n        )\n</code></pre>"},{"location":"api/collaboration/#ai_ensemble_suite.collaboration.ChainOfThoughtBranching","title":"<code>ChainOfThoughtBranching</code>","text":"<p>               Bases: <code>BaseCollaborationPhase</code></p> <p>Chain of Thought Branching collaboration phase.</p> <p>Models trace through multiple reasoning paths, then evaluate and select the most promising path.</p> Source code in <code>src\\ai_ensemble_suite\\collaboration\\chain_of_thought.py</code> <pre><code>class ChainOfThoughtBranching(BaseCollaborationPhase):\n    \"\"\"Chain of Thought Branching collaboration phase.\n\n    Models trace through multiple reasoning paths, then evaluate\n    and select the most promising path.\n    \"\"\"\n\n    def __init__(\n            self,\n            model_manager: \"ModelManager\",\n            config_manager: \"ConfigManager\",\n            phase_name: str\n    ) -&gt; None:\n        \"\"\"Initialize the chain of thought branching phase.\n\n        Args:\n            model_manager: The ModelManager instance.\n            config_manager: The ConfigManager instance.\n            phase_name: The name of the phase for configuration lookup.\n\n        Raises:\n            ConfigurationError: If phase configuration is invalid.\n        \"\"\"\n        super().__init__(model_manager, config_manager, phase_name)\n\n        # Get branch count\n        self._branch_count = self._config.get(\"branch_count\", 3)\n        if self._branch_count &lt; 1:\n            logger.warning(\n                f\"Invalid branch_count ({self._branch_count}) for phase '{phase_name}', \"\n                \"using default: 3\"\n            )\n            self._branch_count = 3\n\n        # Get branch depth\n        self._branch_depth = self._config.get(\"branch_depth\", 2)\n        if self._branch_depth &lt; 1:\n            logger.warning(\n                f\"Invalid branch_depth ({self._branch_depth}) for phase '{phase_name}', \"\n                \"using default: 2\"\n            )\n            self._branch_depth = 2\n\n        # Get template names\n        self._initial_template = self._config.get(\"initial_template\", \"cot_initial\")\n        self._branch_template = self._config.get(\"branch_template\", \"cot_branch\")\n        self._evaluation_template = self._config.get(\"evaluation_template\", \"cot_evaluation\")\n\n        # Get evaluation model (default to first model)\n        self._evaluation_model = self._config.get(\"evaluation_model\")\n        if not self._evaluation_model and self._model_ids:\n            self._evaluation_model = self._model_ids[0]\n\n        logger.debug(\n            f\"Initialized ChainOfThoughtBranching phase '{phase_name}' with \"\n            f\"{self._branch_count} branches of depth {self._branch_depth}\"\n        )\n\n    async def execute(\n            self,\n            query: str,\n            context: Dict[str, Any],\n            trace_collector: Optional[TraceCollector] = None\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Execute the Chain of Thought Branching phase.\n\n        Args:\n            query: The user query to process.\n            context: Context information from previous phases.\n            trace_collector: Optional trace collector for gathering execution details.\n\n        Returns:\n            Dictionary containing:\n                output: The final response after evaluating reasoning branches.\n                context: Updated context with branch information.\n\n        Raises:\n            CollaborationError: If phase execution fails.\n        \"\"\"\n        start_time = time.time()\n\n        try:\n            # Get inputs from previous phases\n            inputs = self._get_inputs_from_context(context)\n\n            # Step 1: Generate initial thoughts\n            initial_thoughts = await self._generate_initial_thoughts(\n                query, context, trace_collector\n            )\n\n            # Step 2: Develop reasoning branches\n            branches = await self._develop_branches(\n                query, initial_thoughts, context, trace_collector\n            )\n\n            # Step 3: Evaluate branches\n            evaluation_results = await self._evaluate_branches(\n                query, branches, context, trace_collector\n            )\n\n            execution_time = time.time() - start_time\n\n            # Log completion\n            logger.info(\n                f\"ChainOfThoughtBranching phase '{self._phase_name}' completed in {execution_time:.2f}s\",\n                extra={\"branches\": len(branches), \"depth\": self._branch_depth}\n            )\n\n            # Add phase trace if collector is provided\n            if trace_collector:\n                trace_collector.add_phase_trace(\n                    phase_name=self._phase_name,\n                    input_data={\"query\": query, \"context\": context},\n                    output_data={\n                        \"initial_thoughts\": initial_thoughts,\n                        \"branches\": branches,\n                        \"evaluation_results\": evaluation_results\n                    },\n                    execution_time=execution_time,\n                    phase_parameters=self._config\n                )\n\n            # Get final output\n            final_output = evaluation_results.get(\"best_branch_conclusion\", \"\")\n            if not final_output:\n                final_output = evaluation_results.get(\"evaluation_text\", \"\")\n\n            # Calculate confidence\n            confidence = evaluation_results.get(\"confidence\", 0.7)\n\n            # Return results\n            return {\n                \"output\": final_output,\n                \"initial_thoughts\": initial_thoughts,\n                \"branches\": branches,\n                \"evaluation_results\": evaluation_results,\n                \"confidence\": confidence\n            }\n\n        except Exception as e:\n            raise CollaborationError(\n                f\"ChainOfThoughtBranching phase '{self._phase_name}' failed: {str(e)}\"\n            )\n\n    async def _generate_initial_thoughts(\n            self,\n            query: str,\n            context: Dict[str, Any],\n            trace_collector: Optional[TraceCollector] = None\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Generate initial thoughts on the query.\n\n        Args:\n            query: The user query.\n            context: Context information from previous phases.\n            trace_collector: Optional trace collector.\n\n        Returns:\n            Dictionary with initial thoughts.\n\n        Raises:\n            CollaborationError: If generation fails.\n        \"\"\"\n        logger.debug(\"Generating initial thoughts\")\n\n        # Get inputs from previous phases\n        inputs = self._get_inputs_from_context(context)\n\n        # Format initial prompt\n        try:\n            context = {\"query\": query, **inputs}\n            initial_prompt = self.render_template(self._initial_template, context)\n        except (ConfigurationError, KeyError) as e:\n            raise CollaborationError(f\"Failed to format initial thoughts prompt: {str(e)}\")\n\n        # Select model for initial thoughts (first model)\n        initial_model = self._model_ids[0] if self._model_ids else None\n        if not initial_model:\n            raise CollaborationError(\"No models available for generating initial thoughts\")\n\n        # Run model\n        try:\n            initial_result = await self._model_manager.run_inference(\n                model_id=initial_model,\n                prompt=initial_prompt\n            )\n\n            # Add trace if collector is provided\n            if trace_collector:\n                trace_collector.add_model_trace(\n                    model_id=initial_model,\n                    input_prompt=initial_prompt,\n                    output=initial_result,\n                    execution_time=initial_result.get(\"generation_time\", 0),\n                    parameters={}\n                )\n\n            return {\n                \"initial_thoughts\": initial_result.get(\"text\", \"\"),\n                \"model_id\": initial_model,\n                \"raw_result\": initial_result\n            }\n\n        except Exception as e:\n            raise CollaborationError(f\"Failed to generate initial thoughts: {str(e)}\")\n\n    async def _develop_branches(\n            self,\n            query: str,\n            initial_thoughts: Dict[str, Any],\n            context: Dict[str, Any],\n            trace_collector: Optional[TraceCollector] = None\n    ) -&gt; List[Dict[str, Any]]:\n        \"\"\"Develop multiple reasoning branches from initial thoughts.\n\n        Args:\n            query: The user query.\n            initial_thoughts: Results from initial thoughts step.\n            context: Context information from previous phases.\n            trace_collector: Optional trace collector.\n\n        Returns:\n            List of branch dictionaries.\n\n        Raises:\n            CollaborationError: If branch development fails.\n        \"\"\"\n        logger.debug(f\"Developing {self._branch_count} reasoning branches\")\n\n        initial_text = initial_thoughts.get(\"initial_thoughts\", \"\")\n        branches = []\n\n        # Distribute models across branches\n        model_assignments = []\n        for i in range(self._branch_count):\n            model_idx = i % len(self._model_ids)\n            model_assignments.append(self._model_ids[model_idx])\n\n        # Create initial branches\n        for branch_idx in range(self._branch_count):\n            branch = {\n                \"branch_id\": f\"branch_{branch_idx + 1}\",\n                \"initial_thoughts\": initial_text,\n                \"steps\": [],\n                \"model_id\": model_assignments[branch_idx]\n            }\n            branches.append(branch)\n\n        # Develop each branch step by step\n        for step_idx in range(self._branch_depth):\n            logger.debug(f\"Developing step {step_idx + 1}/{self._branch_depth}\")\n\n            for branch in branches:\n                # Format branch prompt\n                try:\n                    # Gather previous steps for context\n                    previous_steps = \"\"\n                    for prev_step in branch[\"steps\"]:\n                        previous_steps += f\"\\nStep {prev_step['step_number']}: {prev_step['content']}\\n\"\n\n                    context = {\n                        \"query\": query,\n                        \"initial_thoughts\": branch[\"initial_thoughts\"],\n                        \"previous_steps\": previous_steps,\n                        \"step_number\": step_idx + 1\n                    }\n                    branch_prompt = self.render_template(self._branch_template, context)\n                except (ConfigurationError, KeyError) as e:\n                    logger.warning(\n                        f\"Failed to format prompt for branch {branch['branch_id']}, \"\n                        f\"step {step_idx + 1}: {str(e)}\"\n                    )\n                    continue\n\n                # Run model for this branch step\n                try:\n                    model_id = branch[\"model_id\"]\n                    step_result = await self._model_manager.run_inference(\n                        model_id=model_id,\n                        prompt=branch_prompt\n                    )\n\n                    # Add step to branch\n                    branch[\"steps\"].append({\n                        \"step_number\": step_idx + 1,\n                        \"content\": step_result.get(\"text\", \"\"),\n                        \"model_id\": model_id\n                    })\n\n                    # Add trace if collector is provided\n                    if trace_collector:\n                        trace_collector.add_model_trace(\n                            model_id=model_id,\n                            input_prompt=branch_prompt,\n                            output=step_result,\n                            execution_time=step_result.get(\"generation_time\", 0),\n                            parameters={\n                                \"branch_id\": branch[\"branch_id\"],\n                                \"step_number\": step_idx + 1\n                            }\n                        )\n\n                except Exception as e:\n                    logger.warning(\n                        f\"Failed to develop step {step_idx + 1} for branch {branch['branch_id']}: {str(e)}\"\n                    )\n\n        # Extract conclusions from final steps\n        for branch in branches:\n            if branch[\"steps\"]:\n                final_step = branch[\"steps\"][-1]\n                branch[\"conclusion\"] = final_step[\"content\"]\n            else:\n                branch[\"conclusion\"] = \"No conclusion reached.\"\n\n        return branches\n\n    async def _evaluate_branches(\n            self,\n            query: str,\n            branches: List[Dict[str, Any]],\n            context: Dict[str, Any],\n            trace_collector: Optional[TraceCollector] = None\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Evaluate reasoning branches and select the best one.\n\n        Args:\n            query: The user query.\n            branches: List of developed reasoning branches.\n            context: Context information from previous phases.\n            trace_collector: Optional trace collector.\n\n        Returns:\n            Dictionary with evaluation results.\n\n        Raises:\n            CollaborationError: If evaluation fails.\n        \"\"\"\n        logger.debug(\"Evaluating reasoning branches\")\n\n        # Check if evaluation model is available\n        if not self._evaluation_model:\n            raise CollaborationError(\"No evaluation model specified\")\n\n        # Format branches for evaluation\n        formatted_branches = \"\"\n        for branch in branches:\n            formatted_branches += f\"\\n\\n## Branch {branch['branch_id']}\\n\\n\"\n            formatted_branches += f\"Initial thoughts:\\n{branch['initial_thoughts']}\\n\\n\"\n\n            for step in branch[\"steps\"]:\n                formatted_branches += f\"Step {step['step_number']}: {step['content']}\\n\\n\"\n\n            formatted_branches += f\"Conclusion: {branch.get('conclusion', '')}\\n\"\n\n        # Format evaluation prompt\n        try:\n            context = {\n                \"query\": query,\n                \"branches\": formatted_branches\n            }\n            evaluation_prompt = self.render_template(self._evaluation_template, context)\n        except (ConfigurationError, KeyError) as e:\n            raise CollaborationError(f\"Failed to format evaluation prompt: {str(e)}\")\n\n        # Run evaluation model\n        try:\n            evaluation_result = await self._model_manager.run_inference(\n                model_id=self._evaluation_model,\n                prompt=evaluation_prompt\n            )\n\n            # Add trace if collector is provided\n            if trace_collector:\n                trace_collector.add_model_trace(\n                    model_id=self._evaluation_model,\n                    input_prompt=evaluation_prompt,\n                    output=evaluation_result,\n                    execution_time=evaluation_result.get(\"generation_time\", 0),\n                    parameters={\"role\": \"evaluator\"}\n                )\n\n            # Default values in case evaluation_result is None\n            evaluation_text = \"\"\n            if evaluation_result is not None:\n                evaluation_text = evaluation_result.get(\"text\", \"\")\n\n            # Try to identify best branch\n            best_branch_id = None\n            for branch in branches:\n                branch_id = branch[\"branch_id\"]\n                if (evaluation_text and\n                        (f\"best branch is {branch_id}\" in evaluation_text.lower() or\n                         f\"select {branch_id}\" in evaluation_text.lower() or\n                         f\"choose {branch_id}\" in evaluation_text.lower())):\n                    best_branch_id = branch_id\n                    break\n\n            # If explicit selection not found, look for branch numbers\n            if not best_branch_id and evaluation_text:\n                for branch in branches:\n                    branch_num = branch[\"branch_id\"].split(\"_\")[-1]\n                    if (f\"branch {branch_num} provides\" in evaluation_text.lower() or\n                            f\"branch {branch_num} is\" in evaluation_text.lower()):\n                        best_branch_id = branch[\"branch_id\"]\n                        break\n\n            # Get best branch conclusion\n            best_branch_conclusion = \"\"\n            if best_branch_id:\n                for branch in branches:\n                    if branch[\"branch_id\"] == best_branch_id:\n                        best_branch_conclusion = branch.get(\"conclusion\", \"\")\n                        break\n            else:\n                # If no best branch identified, use the evaluation text itself or first branch\n                if evaluation_text:\n                    best_branch_conclusion = evaluation_text\n                elif branches:\n                    # Fallback to first branch if no evaluation text\n                    best_branch_id = branches[0][\"branch_id\"]\n                    best_branch_conclusion = branches[0].get(\"conclusion\", \"\")\n\n            # Safely extract confidence value\n            confidence_score = 0.7  # Default confidence\n            if evaluation_result is not None and isinstance(evaluation_result, dict):\n                confidence_data = evaluation_result.get(\"confidence\")\n                if isinstance(confidence_data, dict) and \"combined\" in confidence_data:\n                    confidence_score = confidence_data.get(\"combined\", 0.7)\n                elif isinstance(confidence_data, (float, int)):\n                    confidence_score = float(confidence_data)\n\n            return {\n                \"evaluation_text\": evaluation_text,\n                \"best_branch_id\": best_branch_id,\n                \"best_branch_conclusion\": best_branch_conclusion,\n                \"model_id\": self._evaluation_model,\n                \"raw_result\": evaluation_result,\n                \"confidence\": confidence_score\n            }\n\n        except Exception as e:\n            # More robust error handling to provide default values even on failure\n            logger.error(f\"Error during branch evaluation: {str(e)}\")\n\n            # Provide default fallback values\n            best_branch_id = branches[0][\"branch_id\"] if branches else None\n            best_branch_conclusion = branches[0].get(\"conclusion\", \"\") if branches else \"\"\n\n            return {\n                \"evaluation_text\": f\"Evaluation failed: {str(e)}\",\n                \"best_branch_id\": best_branch_id,\n                \"best_branch_conclusion\": best_branch_conclusion,\n                \"model_id\": self._evaluation_model,\n                \"raw_result\": None,\n                \"confidence\": 0.5,  # Lower confidence due to failure\n                \"error\": str(e)\n            }\n</code></pre>"},{"location":"api/collaboration/#ai_ensemble_suite.collaboration.ChainOfThoughtBranching.__init__","title":"<code>__init__(model_manager, config_manager, phase_name)</code>","text":"<p>Initialize the chain of thought branching phase.</p> <p>Parameters:</p> Name Type Description Default <code>model_manager</code> <code>ModelManager</code> <p>The ModelManager instance.</p> required <code>config_manager</code> <code>ConfigManager</code> <p>The ConfigManager instance.</p> required <code>phase_name</code> <code>str</code> <p>The name of the phase for configuration lookup.</p> required <p>Raises:</p> Type Description <code>ConfigurationError</code> <p>If phase configuration is invalid.</p> Source code in <code>src\\ai_ensemble_suite\\collaboration\\chain_of_thought.py</code> <pre><code>def __init__(\n        self,\n        model_manager: \"ModelManager\",\n        config_manager: \"ConfigManager\",\n        phase_name: str\n) -&gt; None:\n    \"\"\"Initialize the chain of thought branching phase.\n\n    Args:\n        model_manager: The ModelManager instance.\n        config_manager: The ConfigManager instance.\n        phase_name: The name of the phase for configuration lookup.\n\n    Raises:\n        ConfigurationError: If phase configuration is invalid.\n    \"\"\"\n    super().__init__(model_manager, config_manager, phase_name)\n\n    # Get branch count\n    self._branch_count = self._config.get(\"branch_count\", 3)\n    if self._branch_count &lt; 1:\n        logger.warning(\n            f\"Invalid branch_count ({self._branch_count}) for phase '{phase_name}', \"\n            \"using default: 3\"\n        )\n        self._branch_count = 3\n\n    # Get branch depth\n    self._branch_depth = self._config.get(\"branch_depth\", 2)\n    if self._branch_depth &lt; 1:\n        logger.warning(\n            f\"Invalid branch_depth ({self._branch_depth}) for phase '{phase_name}', \"\n            \"using default: 2\"\n        )\n        self._branch_depth = 2\n\n    # Get template names\n    self._initial_template = self._config.get(\"initial_template\", \"cot_initial\")\n    self._branch_template = self._config.get(\"branch_template\", \"cot_branch\")\n    self._evaluation_template = self._config.get(\"evaluation_template\", \"cot_evaluation\")\n\n    # Get evaluation model (default to first model)\n    self._evaluation_model = self._config.get(\"evaluation_model\")\n    if not self._evaluation_model and self._model_ids:\n        self._evaluation_model = self._model_ids[0]\n\n    logger.debug(\n        f\"Initialized ChainOfThoughtBranching phase '{phase_name}' with \"\n        f\"{self._branch_count} branches of depth {self._branch_depth}\"\n    )\n</code></pre>"},{"location":"api/collaboration/#ai_ensemble_suite.collaboration.ChainOfThoughtBranching.execute","title":"<code>execute(query, context, trace_collector=None)</code>  <code>async</code>","text":"<p>Execute the Chain of Thought Branching phase.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The user query to process.</p> required <code>context</code> <code>Dict[str, Any]</code> <p>Context information from previous phases.</p> required <code>trace_collector</code> <code>Optional[TraceCollector]</code> <p>Optional trace collector for gathering execution details.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary containing: output: The final response after evaluating reasoning branches. context: Updated context with branch information.</p> <p>Raises:</p> Type Description <code>CollaborationError</code> <p>If phase execution fails.</p> Source code in <code>src\\ai_ensemble_suite\\collaboration\\chain_of_thought.py</code> <pre><code>async def execute(\n        self,\n        query: str,\n        context: Dict[str, Any],\n        trace_collector: Optional[TraceCollector] = None\n) -&gt; Dict[str, Any]:\n    \"\"\"Execute the Chain of Thought Branching phase.\n\n    Args:\n        query: The user query to process.\n        context: Context information from previous phases.\n        trace_collector: Optional trace collector for gathering execution details.\n\n    Returns:\n        Dictionary containing:\n            output: The final response after evaluating reasoning branches.\n            context: Updated context with branch information.\n\n    Raises:\n        CollaborationError: If phase execution fails.\n    \"\"\"\n    start_time = time.time()\n\n    try:\n        # Get inputs from previous phases\n        inputs = self._get_inputs_from_context(context)\n\n        # Step 1: Generate initial thoughts\n        initial_thoughts = await self._generate_initial_thoughts(\n            query, context, trace_collector\n        )\n\n        # Step 2: Develop reasoning branches\n        branches = await self._develop_branches(\n            query, initial_thoughts, context, trace_collector\n        )\n\n        # Step 3: Evaluate branches\n        evaluation_results = await self._evaluate_branches(\n            query, branches, context, trace_collector\n        )\n\n        execution_time = time.time() - start_time\n\n        # Log completion\n        logger.info(\n            f\"ChainOfThoughtBranching phase '{self._phase_name}' completed in {execution_time:.2f}s\",\n            extra={\"branches\": len(branches), \"depth\": self._branch_depth}\n        )\n\n        # Add phase trace if collector is provided\n        if trace_collector:\n            trace_collector.add_phase_trace(\n                phase_name=self._phase_name,\n                input_data={\"query\": query, \"context\": context},\n                output_data={\n                    \"initial_thoughts\": initial_thoughts,\n                    \"branches\": branches,\n                    \"evaluation_results\": evaluation_results\n                },\n                execution_time=execution_time,\n                phase_parameters=self._config\n            )\n\n        # Get final output\n        final_output = evaluation_results.get(\"best_branch_conclusion\", \"\")\n        if not final_output:\n            final_output = evaluation_results.get(\"evaluation_text\", \"\")\n\n        # Calculate confidence\n        confidence = evaluation_results.get(\"confidence\", 0.7)\n\n        # Return results\n        return {\n            \"output\": final_output,\n            \"initial_thoughts\": initial_thoughts,\n            \"branches\": branches,\n            \"evaluation_results\": evaluation_results,\n            \"confidence\": confidence\n        }\n\n    except Exception as e:\n        raise CollaborationError(\n            f\"ChainOfThoughtBranching phase '{self._phase_name}' failed: {str(e)}\"\n        )\n</code></pre>"},{"location":"api/collaboration/#ai_ensemble_suite.collaboration.CompetitiveEvaluation","title":"<code>CompetitiveEvaluation</code>","text":"<p>               Bases: <code>BaseCollaborationPhase</code></p> <p>Competitive Evaluation collaboration phase.</p> <p>Models are pitted against each other in a competition, with each evaluating the others' outputs and a winner being determined.</p> Source code in <code>src\\ai_ensemble_suite\\collaboration\\competitive_evaluation.py</code> <pre><code>class CompetitiveEvaluation(BaseCollaborationPhase):\n    \"\"\"Competitive Evaluation collaboration phase.\n\n    Models are pitted against each other in a competition, with each evaluating\n    the others' outputs and a winner being determined.\n    \"\"\"\n\n    def __init__(\n        self, \n        model_manager: \"ModelManager\",\n        config_manager: \"ConfigManager\",\n        phase_name: str\n    ) -&gt; None:\n        \"\"\"Initialize the competitive evaluation phase.\n\n        Args:\n            model_manager: The ModelManager instance.\n            config_manager: The ConfigManager instance.\n            phase_name: The name of the phase for configuration lookup.\n\n        Raises:\n            ConfigurationError: If phase configuration is invalid.\n        \"\"\"\n        super().__init__(model_manager, config_manager, phase_name)\n\n        # Get evaluation criteria\n        self._evaluation_criteria = self._config.get(\"evaluation_criteria\", [\n            \"accuracy\", \"reasoning\", \"completeness\", \"clarity\"\n        ])\n\n        # Get competitors (default to all models)\n        self._competitors = self._config.get(\"competitors\", self._model_ids)\n\n        # Get judge (default to first model)\n        self._judge = self._config.get(\"judge\")\n        if not self._judge and self._model_ids:\n            self._judge = self._model_ids[0]\n\n        # Get template names\n        self._competitor_template = self._config.get(\"competitor_template\", \"competitor\")\n        self._evaluation_template = self._config.get(\"evaluation_template\", \"evaluation\")\n\n        logger.debug(\n            f\"Initialized CompetitiveEvaluation phase '{phase_name}' with \"\n            f\"{len(self._competitors)} competitors and judge: {self._judge}\"\n        )\n\n    async def execute(\n        self, \n        query: str,\n        context: Dict[str, Any],\n        trace_collector: Optional[TraceCollector] = None\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Execute the Competitive Evaluation phase.\n\n        Args:\n            query: The user query to process.\n            context: Context information from previous phases.\n            trace_collector: Optional trace collector for gathering execution details.\n\n        Returns:\n            Dictionary containing:\n                output: The winning response.\n                context: Updated context with competition results.\n\n        Raises:\n            CollaborationError: If phase execution fails.\n        \"\"\"\n        start_time = time.time()\n\n        try:\n            # Step 1: Have competitors generate responses\n            competitor_responses = await self._generate_competitor_responses(\n                query, context, trace_collector\n            )\n\n            # Step 2: Judge evaluates the responses\n            evaluation_results = await self._evaluate_responses(\n                query, competitor_responses, context, trace_collector\n            )\n\n            # Step 3: Determine the winner\n            winner, scores = self._determine_winner(\n                competitor_responses, evaluation_results\n            )\n\n            execution_time = time.time() - start_time\n\n            # Log completion\n            logger.info(\n                f\"CompetitiveEvaluation phase '{self._phase_name}' completed in {execution_time:.2f}s\",\n                extra={\"winner\": winner, \"competitors\": len(competitor_responses)}\n            )\n\n            # Add phase trace if collector is provided\n            if trace_collector:\n                trace_collector.add_phase_trace(\n                    phase_name=self._phase_name,\n                    input_data={\n                        \"query\": query, \n                        \"context\": context\n                    },\n                    output_data={\n                        \"competitor_responses\": competitor_responses,\n                        \"evaluation_results\": evaluation_results,\n                        \"winner\": winner,\n                        \"scores\": scores\n                    },\n                    execution_time=execution_time,\n                    phase_parameters=self._config\n                )\n\n            # Get winning response\n            winning_response = competitor_responses.get(winner, \"\")\n\n            # Return results\n            return {\n                \"output\": winning_response,\n                \"winner\": winner,\n                \"competitor_responses\": competitor_responses,\n                \"evaluation_results\": evaluation_results,\n                \"scores\": scores,\n                \"confidence\": scores.get(winner, 0.7)  # Use winner's score as confidence\n            }\n\n        except Exception as e:\n            raise CollaborationError(\n                f\"CompetitiveEvaluation phase '{self._phase_name}' failed: {str(e)}\"\n            )\n\n    async def _generate_competitor_responses(\n        self,\n        query: str,\n        context: Dict[str, Any],\n        trace_collector: Optional[TraceCollector] = None\n    ) -&gt; Dict[str, str]:\n        \"\"\"Generate responses from all competitors.\n\n        Args:\n            query: The user query.\n            context: Context information from previous phases.\n            trace_collector: Optional trace collector.\n\n        Returns:\n            Dictionary mapping competitor IDs to their responses.\n\n        Raises:\n            CollaborationError: If generation fails.\n        \"\"\"\n        logger.debug(f\"Generating responses from {len(self._competitors)} competitors\")\n\n        # Get inputs from previous phases\n        inputs = self._get_inputs_from_context(context)\n\n        # Get competitor template\n        if not self._competitor_template:\n            logger.warning(\n                f\"No competitor template specified, using {self._prompt_template or 'single_query'}\"\n            )\n            self._competitor_template = self._prompt_template or \"single_query\"\n\n        # Format prompt\n        try:\n            context = {\"query\": query, **inputs}\n            competitor_prompt = self.render_template(self._competitor_template, context)\n        except (ConfigurationError, KeyError) as e:\n            raise CollaborationError(f\"Failed to format competitor prompt: {str(e)}\")\n\n        # Run competitors\n        model_results = await self._run_models(\n            prompt=competitor_prompt,\n            model_ids=self._competitors,\n            trace_collector=trace_collector\n        )\n\n        # Extract responses\n        competitor_responses = {}\n        for model_id, result in model_results.items():\n            competitor_responses[model_id] = result.get(\"text\", \"\")\n\n        return competitor_responses\n\n    async def _evaluate_responses(\n        self,\n        query: str,\n        competitor_responses: Dict[str, str],\n        context: Dict[str, Any],\n        trace_collector: Optional[TraceCollector] = None\n    ) -&gt; Dict[str, str]:\n        \"\"\"Evaluate competitor responses using the judge.\n\n        Args:\n            query: The user query.\n            competitor_responses: Responses from competitors.\n            context: Context information from previous phases.\n            trace_collector: Optional trace collector.\n\n        Returns:\n            Dictionary with evaluation results.\n\n        Raises:\n            CollaborationError: If evaluation fails.\n        \"\"\"\n        logger.debug(f\"Evaluating competitor responses using judge: {self._judge}\")\n\n        # Check if judge is available\n        if not self._judge:\n            raise CollaborationError(\"No judge specified for evaluation\")\n\n        # Format the responses for evaluation\n        formatted_responses = \"\"\n        for model_id, response in competitor_responses.items():\n            formatted_responses += f\"\\n\\n## Response from Competitor {model_id}\\n\\n{response}\"\n\n        # Create criteria string\n        criteria_str = \", \".join(self._evaluation_criteria)\n\n        # Format evaluation prompt\n        try:\n            context = {\n                \"query\": query,\n                \"responses\": formatted_responses,\n                \"criteria\": criteria_str\n            }\n            evaluation_prompt = self.render_template(self._evaluation_template, context)\n        except (ConfigurationError, KeyError) as e:\n            raise CollaborationError(f\"Failed to format evaluation prompt: {str(e)}\")\n\n        # Run judge\n        try:\n            judge_results = await self._model_manager.run_inference(\n                model_id=self._judge,\n                prompt=evaluation_prompt\n            )\n\n            # Add trace if collector is provided\n            if trace_collector:\n                trace_collector.add_model_trace(\n                    model_id=self._judge,\n                    input_prompt=evaluation_prompt,\n                    output=judge_results,\n                    execution_time=judge_results.get(\"generation_time\", 0),\n                    parameters={}\n                )\n\n            return {\n                \"evaluation_text\": judge_results.get(\"text\", \"\"),\n                \"judge_id\": self._judge,\n                \"raw_result\": judge_results\n            }\n\n        except Exception as e:\n            raise CollaborationError(f\"Failed to run judge evaluation: {str(e)}\")\n\n    def _determine_winner(\n        self,\n        competitor_responses: Dict[str, str],\n        evaluation_results: Dict[str, Any]\n    ) -&gt; Tuple[str, Dict[str, float]]:\n        \"\"\"Determine the winner based on evaluation results.\n\n        Args:\n            competitor_responses: Responses from competitors.\n            evaluation_results: Evaluation results from judge.\n\n        Returns:\n            Tuple of (winner_id, score_dict) where score_dict maps\n            competitor IDs to their scores.\n\n        Raises:\n            CollaborationError: If winner determination fails.\n        \"\"\"\n        evaluation_text = evaluation_results.get(\"evaluation_text\", \"\")\n\n        # Parse scores for each competitor\n        scores: Dict[str, float] = {}\n\n        for competitor_id in competitor_responses.keys():\n            # Look for score patterns for this competitor\n            score_pattern = rf'(?:Competitor|Model|Response)\\s+{competitor_id}[^\\d]*?(\\d+(?:\\.\\d+)?)/10'\n            match = re.search(score_pattern, evaluation_text)\n\n            if match:\n                # Found explicit score\n                scores[competitor_id] = float(match.group(1)) / 10.0  # Normalize to 0-1 range\n            else:\n                # Try alternative pattern (looking for overall rating)\n                alt_pattern = rf'(?:Competitor|Model|Response)\\s+{competitor_id}.*?overall.*?(\\d+(?:\\.\\d+)?)/10'\n                match = re.search(alt_pattern, evaluation_text, re.IGNORECASE | re.DOTALL)\n\n                if match:\n                    scores[competitor_id] = float(match.group(1)) / 10.0\n                else:\n                    # No explicit score found, try to determine from text\n                    competitor_mentions = len(re.findall(\n                        rf'(?:Competitor|Model|Response)\\s+{competitor_id}', \n                        evaluation_text\n                    ))\n\n                    # Set a default score based on mentions\n                    scores[competitor_id] = 0.5 + (0.1 * min(competitor_mentions, 5))\n\n        # Look for explicit winner declaration\n        winner_pattern = r'(?:winner|best|highest)[^\\w]+(Competitor|Model|Response)\\s+([a-zA-Z0-9_]+)'\n        winner_match = re.search(winner_pattern, evaluation_text, re.IGNORECASE)\n\n        if winner_match:\n            winner_id = winner_match.group(2)\n            # Ensure winner_id is a valid competitor\n            if winner_id in competitor_responses:\n                # Ensure winner has the highest score\n                scores[winner_id] = max(scores.values()) + 0.1\n            else:\n                logger.warning(f\"Declared winner '{winner_id}' is not a valid competitor\")\n\n        # If no scores were found, assign default scores\n        if not scores:\n            logger.warning(\"No scores could be extracted from evaluation text\")\n            scores = {competitor_id: 0.5 for competitor_id in competitor_responses.keys()}\n\n        # Determine winner (highest score)\n        winner = max(scores.items(), key=lambda x: x[1])[0] if scores else \"\"\n\n        if not winner and competitor_responses:\n            # Fallback to first competitor\n            winner = next(iter(competitor_responses.keys()))\n\n        return winner, scores\n</code></pre>"},{"location":"api/collaboration/#ai_ensemble_suite.collaboration.CompetitiveEvaluation.__init__","title":"<code>__init__(model_manager, config_manager, phase_name)</code>","text":"<p>Initialize the competitive evaluation phase.</p> <p>Parameters:</p> Name Type Description Default <code>model_manager</code> <code>ModelManager</code> <p>The ModelManager instance.</p> required <code>config_manager</code> <code>ConfigManager</code> <p>The ConfigManager instance.</p> required <code>phase_name</code> <code>str</code> <p>The name of the phase for configuration lookup.</p> required <p>Raises:</p> Type Description <code>ConfigurationError</code> <p>If phase configuration is invalid.</p> Source code in <code>src\\ai_ensemble_suite\\collaboration\\competitive_evaluation.py</code> <pre><code>def __init__(\n    self, \n    model_manager: \"ModelManager\",\n    config_manager: \"ConfigManager\",\n    phase_name: str\n) -&gt; None:\n    \"\"\"Initialize the competitive evaluation phase.\n\n    Args:\n        model_manager: The ModelManager instance.\n        config_manager: The ConfigManager instance.\n        phase_name: The name of the phase for configuration lookup.\n\n    Raises:\n        ConfigurationError: If phase configuration is invalid.\n    \"\"\"\n    super().__init__(model_manager, config_manager, phase_name)\n\n    # Get evaluation criteria\n    self._evaluation_criteria = self._config.get(\"evaluation_criteria\", [\n        \"accuracy\", \"reasoning\", \"completeness\", \"clarity\"\n    ])\n\n    # Get competitors (default to all models)\n    self._competitors = self._config.get(\"competitors\", self._model_ids)\n\n    # Get judge (default to first model)\n    self._judge = self._config.get(\"judge\")\n    if not self._judge and self._model_ids:\n        self._judge = self._model_ids[0]\n\n    # Get template names\n    self._competitor_template = self._config.get(\"competitor_template\", \"competitor\")\n    self._evaluation_template = self._config.get(\"evaluation_template\", \"evaluation\")\n\n    logger.debug(\n        f\"Initialized CompetitiveEvaluation phase '{phase_name}' with \"\n        f\"{len(self._competitors)} competitors and judge: {self._judge}\"\n    )\n</code></pre>"},{"location":"api/collaboration/#ai_ensemble_suite.collaboration.CompetitiveEvaluation.execute","title":"<code>execute(query, context, trace_collector=None)</code>  <code>async</code>","text":"<p>Execute the Competitive Evaluation phase.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The user query to process.</p> required <code>context</code> <code>Dict[str, Any]</code> <p>Context information from previous phases.</p> required <code>trace_collector</code> <code>Optional[TraceCollector]</code> <p>Optional trace collector for gathering execution details.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary containing: output: The winning response. context: Updated context with competition results.</p> <p>Raises:</p> Type Description <code>CollaborationError</code> <p>If phase execution fails.</p> Source code in <code>src\\ai_ensemble_suite\\collaboration\\competitive_evaluation.py</code> <pre><code>async def execute(\n    self, \n    query: str,\n    context: Dict[str, Any],\n    trace_collector: Optional[TraceCollector] = None\n) -&gt; Dict[str, Any]:\n    \"\"\"Execute the Competitive Evaluation phase.\n\n    Args:\n        query: The user query to process.\n        context: Context information from previous phases.\n        trace_collector: Optional trace collector for gathering execution details.\n\n    Returns:\n        Dictionary containing:\n            output: The winning response.\n            context: Updated context with competition results.\n\n    Raises:\n        CollaborationError: If phase execution fails.\n    \"\"\"\n    start_time = time.time()\n\n    try:\n        # Step 1: Have competitors generate responses\n        competitor_responses = await self._generate_competitor_responses(\n            query, context, trace_collector\n        )\n\n        # Step 2: Judge evaluates the responses\n        evaluation_results = await self._evaluate_responses(\n            query, competitor_responses, context, trace_collector\n        )\n\n        # Step 3: Determine the winner\n        winner, scores = self._determine_winner(\n            competitor_responses, evaluation_results\n        )\n\n        execution_time = time.time() - start_time\n\n        # Log completion\n        logger.info(\n            f\"CompetitiveEvaluation phase '{self._phase_name}' completed in {execution_time:.2f}s\",\n            extra={\"winner\": winner, \"competitors\": len(competitor_responses)}\n        )\n\n        # Add phase trace if collector is provided\n        if trace_collector:\n            trace_collector.add_phase_trace(\n                phase_name=self._phase_name,\n                input_data={\n                    \"query\": query, \n                    \"context\": context\n                },\n                output_data={\n                    \"competitor_responses\": competitor_responses,\n                    \"evaluation_results\": evaluation_results,\n                    \"winner\": winner,\n                    \"scores\": scores\n                },\n                execution_time=execution_time,\n                phase_parameters=self._config\n            )\n\n        # Get winning response\n        winning_response = competitor_responses.get(winner, \"\")\n\n        # Return results\n        return {\n            \"output\": winning_response,\n            \"winner\": winner,\n            \"competitor_responses\": competitor_responses,\n            \"evaluation_results\": evaluation_results,\n            \"scores\": scores,\n            \"confidence\": scores.get(winner, 0.7)  # Use winner's score as confidence\n        }\n\n    except Exception as e:\n        raise CollaborationError(\n            f\"CompetitiveEvaluation phase '{self._phase_name}' failed: {str(e)}\"\n        )\n</code></pre>"},{"location":"api/collaboration/#ai_ensemble_suite.collaboration.ExpertCommittee","title":"<code>ExpertCommittee</code>","text":"<p>               Bases: <code>BaseCollaborationPhase</code></p> <p>Expert Committee collaboration phase.</p> <p>Final processing/structuring of model outputs before aggregation. Organizes information and evaluates the quality of responses.</p> Source code in <code>src\\ai_ensemble_suite\\collaboration\\expert_committee.py</code> <pre><code>class ExpertCommittee(BaseCollaborationPhase):\n    \"\"\"Expert Committee collaboration phase.\n\n    Final processing/structuring of model outputs before aggregation.\n    Organizes information and evaluates the quality of responses.\n    \"\"\"\n\n    def __init__(\n        self, \n        model_manager: \"ModelManager\",\n        config_manager: \"ConfigManager\",\n        phase_name: str\n    ) -&gt; None:\n        \"\"\"Initialize the expert committee phase.\n\n        Args:\n            model_manager: The ModelManager instance.\n            config_manager: The ConfigManager instance.\n            phase_name: The name of the phase for configuration lookup.\n\n        Raises:\n            ConfigurationError: If phase configuration is invalid.\n        \"\"\"\n        super().__init__(model_manager, config_manager, phase_name)\n\n        # Get committee-specific configuration\n        self._committee_type = self._config.get(\"committee_type\", \"evaluative\")\n        self._evaluation_criteria = self._config.get(\"evaluation_criteria\", [\n            \"accuracy\", \"completeness\", \"clarity\", \"reasoning\"\n        ])\n        self._format_output = self._config.get(\"format_output\", True)\n\n        # Log configuration\n        logger.debug(\n            f\"Initialized Expert Committee phase '{phase_name}' with \"\n            f\"type '{self._committee_type}' and {len(self._evaluation_criteria)} criteria\"\n        )\n\n    async def execute(\n        self, \n        query: str,\n        context: Dict[str, Any],\n        trace_collector: Optional[TraceCollector] = None\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Execute the Expert Committee phase.\n\n        Args:\n            query: The user query to process.\n            context: Context information from previous phases.\n            trace_collector: Optional trace collector for gathering execution details.\n\n        Returns:\n            Dictionary containing:\n                output: The committee's final output.\n                context: Updated context including evaluations and structured output.\n\n        Raises:\n            CollaborationError: If phase execution fails.\n        \"\"\"\n        start_time = time.time()\n\n        try:\n            # Get inputs from previous phases\n            inputs = self._get_inputs_from_context(context)\n\n            # Validate inputs\n            if not inputs and self._input_from:\n                raise CollaborationError(\n                    f\"Expert Committee phase '{self._phase_name}' requires inputs from \"\n                    f\"previous phases: {', '.join(self._input_from)}\"\n                )\n\n            # Find the content to evaluate based on inputs and committee type\n            processing_results = await self._process_inputs(query, inputs, trace_collector)\n\n            # If evaluative committee, perform evaluation\n            if self._committee_type == \"evaluative\":\n                evaluation_results = await self._evaluate_inputs(query, inputs, processing_results, trace_collector)\n                processing_results.update(evaluation_results)\n\n            # If formatting is enabled, format the final output\n            if self._format_output:\n                formatted_output = await self._format_final_output(\n                    query, inputs, processing_results, trace_collector\n                )\n                processing_results[\"output\"] = formatted_output\n\n            execution_time = time.time() - start_time\n\n            # Log completion\n            logger.info(\n                f\"Expert Committee phase '{self._phase_name}' completed in {execution_time:.2f}s\",\n                extra={\n                    \"committee_type\": self._committee_type,\n                    \"phase\": self._phase_name\n                }\n            )\n\n            # Add phase trace if collector is provided\n            if trace_collector:\n                trace_collector.add_phase_trace(\n                    phase_name=self._phase_name,\n                    input_data={\"query\": query, \"inputs\": inputs},\n                    output_data=processing_results,\n                    execution_time=execution_time,\n                    phase_parameters=self._config\n                )\n\n            return processing_results\n\n        except Exception as e:\n            raise CollaborationError(\n                f\"Expert Committee phase '{self._phase_name}' failed: {str(e)}\"\n            )\n\n    async def _process_inputs(\n        self,\n        query: str,\n        inputs: Dict[str, Any],\n        trace_collector: Optional[TraceCollector] = None\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Process and organize inputs from previous phases.\n\n        Args:\n            query: The user query.\n            inputs: Inputs from previous phases.\n            trace_collector: Optional trace collector.\n\n        Returns:\n            Dictionary containing processing results.\n\n        Raises:\n            CollaborationError: If processing fails.\n        \"\"\"\n        # Get or create processing template\n        template_name = self._prompt_template or \"committee_processing\"\n\n        # Prepare inputs for processing\n        organized_inputs = {}\n\n        # Convert inputs to a format suitable for the template\n        for phase_name, phase_output in inputs.items():\n            if isinstance(phase_output, str):\n                # Direct string output\n                organized_inputs[phase_name] = phase_output\n            elif isinstance(phase_output, dict):\n                # Dictionary output - extract the most relevant field\n                if \"output\" in phase_output:\n                    organized_inputs[phase_name] = phase_output[\"output\"]\n                elif \"text\" in phase_output:\n                    organized_inputs[phase_name] = phase_output[\"text\"]\n                elif \"response\" in phase_output:\n                    organized_inputs[phase_name] = phase_output[\"response\"]\n                else:\n                    # Try to find any string field\n                    for key, value in phase_output.items():\n                        if isinstance(value, str):\n                            organized_inputs[phase_name] = value\n                            break\n\n            # If no suitable output found, log warning\n            if phase_name not in organized_inputs:\n                logger.warning(f\"Could not extract output from phase '{phase_name}'\")\n\n        # Format prompt with organized inputs\n        try:\n            # Create a single combined input if there are multiple organized inputs\n            if len(organized_inputs) &gt; 1:\n                combined_input = \"# Inputs from Previous Phases\\n\\n\"\n                for phase_name, content in organized_inputs.items():\n                    combined_input += f\"## From {phase_name}\\n\\n{content}\\n\\n\"\n\n                context = {\"query\": query, \"inputs\": combined_input}\n\n                # Also add individual inputs\n                for phase_name, content in organized_inputs.items():\n                    context[phase_name] = content\n            else:\n                # Single input case\n                phase_name = next(iter(organized_inputs.keys()), \"\")\n                content = next(iter(organized_inputs.values()), \"\")\n                context = {\"query\": query, \"input\": content, phase_name: content}\n\n            processing_prompt = self.render_template(template_name, context)\n        except (ConfigurationError, KeyError) as e:\n            raise CollaborationError(f\"Failed to format processing prompt: {str(e)}\")\n\n        # Run processing models\n        model_results = await self._run_models(\n            prompt=processing_prompt,\n            trace_collector=trace_collector\n        )\n\n        # Process outputs\n        processed_outputs = {}\n        for model_id, result in model_results.items():\n            processed_outputs[model_id] = result.get(\"text\", \"\")\n\n        # Determine the primary processed output\n        primary_output = \"\"\n        if len(processed_outputs) == 1:\n            # Single model case\n            primary_output = list(processed_outputs.values())[0]\n        elif self._model_ids and len(self._model_ids) &gt; 0:\n            # Use the first model as primary\n            primary_model = self._model_ids[0]\n            if primary_model in processed_outputs:\n                primary_output = processed_outputs[primary_model]\n            else:\n                # Fallback to first result\n                primary_output = next(iter(processed_outputs.values()), \"\")\n\n        # Calculate confidence score (average from all models)\n        confidence_values = []\n        for result in model_results.values():\n            if \"confidence\" in result:\n                if isinstance(result[\"confidence\"], dict) and \"combined\" in result[\"confidence\"]:\n                    confidence_values.append(result[\"confidence\"][\"combined\"])\n                elif isinstance(result[\"confidence\"], (float, int)):\n                    confidence_values.append(result[\"confidence\"])\n\n        confidence = sum(confidence_values) / len(confidence_values) if confidence_values else 0.7\n\n        # Return processing results\n        return {\n            \"output\": primary_output,\n            \"processed_outputs\": processed_outputs,\n            \"raw_results\": model_results,\n            \"organized_inputs\": organized_inputs,\n            \"confidence\": confidence\n        }\n\n    async def _evaluate_inputs(\n        self,\n        query: str,\n        inputs: Dict[str, Any],\n        processing_results: Dict[str, Any],\n        trace_collector: Optional[TraceCollector] = None\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Evaluate inputs based on specified criteria.\n\n        Args:\n            query: The user query.\n            inputs: Inputs from previous phases.\n            processing_results: Results from processing step.\n            trace_collector: Optional trace collector.\n\n        Returns:\n            Dictionary containing evaluation results.\n\n        Raises:\n            CollaborationError: If evaluation fails.\n        \"\"\"\n        # Use a specific evaluation template if available\n        eval_template_name = self._config.get(\"evaluation_template\", \"committee_evaluation\")\n\n        # Get content to evaluate (priority: processed output &gt; inputs)\n        content_to_evaluate = processing_results.get(\"output\", \"\")\n        if not content_to_evaluate:\n            # Try to get from organized inputs\n            organized_inputs = processing_results.get(\"organized_inputs\", {})\n            if organized_inputs:\n                # Combine all inputs\n                content_to_evaluate = \"\\n\\n\".join(organized_inputs.values())\n\n        # If still no content, log warning and return empty results\n        if not content_to_evaluate:\n            logger.warning(f\"No content to evaluate in committee phase '{self._phase_name}'\")\n            return {\n                \"evaluations\": {},\n                \"evaluation_summary\": \"\",\n                \"overall_score\": 0.0\n            }\n\n        # Format evaluation prompt\n        try:\n            # Create criteria string\n            criteria_str = \", \".join(self._evaluation_criteria)\n\n            context = {\n                \"query\": query,\n                \"content\": content_to_evaluate,\n                \"criteria\": criteria_str\n            }\n            eval_prompt = self.render_template(eval_template_name, context)\n        except (ConfigurationError, KeyError) as e:\n            raise CollaborationError(f\"Failed to format evaluation prompt: {str(e)}\")\n\n        # Run evaluation models\n        model_results = await self._run_models(\n            prompt=eval_prompt,\n            trace_collector=trace_collector\n        )\n\n        # Process outputs\n        eval_outputs = {}\n        for model_id, result in model_results.items():\n            eval_outputs[model_id] = result.get(\"text\", \"\")\n\n        # Determine the primary evaluation output\n        eval_summary = \"\"\n        if len(eval_outputs) == 1:\n            # Single model case\n            eval_summary = list(eval_outputs.values())[0]\n        elif self._model_ids and len(self._model_ids) &gt; 0:\n            # Use the first model as primary\n            primary_model = self._model_ids[0]\n            if primary_model in eval_outputs:\n                eval_summary = eval_outputs[primary_model]\n            else:\n                # Fallback to first result\n                eval_summary = next(iter(eval_outputs.values()), \"\")\n\n        # Parse scores from evaluation text\n        evaluations = {}\n        overall_score = 0.0\n\n        # Try to extract scores for each criterion\n        for criterion in self._evaluation_criteria:\n            score_pattern = rf'(?i){criterion}[^\\d]*?(\\d+(?:\\.\\d+)?)/10'\n            match = re.search(score_pattern, eval_summary)\n            if match:\n                score = float(match.group(1)) / 10.0  # Normalize to 0-1 range\n                evaluations[criterion] = score\n\n        # Calculate overall score as average of individual scores\n        if evaluations:\n            overall_score = sum(evaluations.values()) / len(evaluations)\n\n        # Return evaluation results\n        return {\n            \"evaluations\": evaluations,\n            \"evaluation_summary\": eval_summary,\n            \"overall_score\": overall_score,\n            \"evaluation_outputs\": eval_outputs\n        }\n\n    async def _format_final_output(\n        self,\n        query: str,\n        inputs: Dict[str, Any],\n        results: Dict[str, Any],\n        trace_collector: Optional[TraceCollector] = None\n    ) -&gt; str:\n        \"\"\"Format the final output based on processing and evaluation.\n\n        Args:\n            query: The user query.\n            inputs: Inputs from previous phases.\n            results: Results from processing and evaluation.\n            trace_collector: Optional trace collector.\n\n        Returns:\n            Formatted final output.\n\n        Raises:\n            CollaborationError: If formatting fails.\n        \"\"\"\n        # Use a specific formatting template if available\n        format_template_name = self._config.get(\"formatting_template\", \"committee_formatting\")\n\n        # Get processed output and evaluation summary\n        processed_output = results.get(\"output\", \"\")\n        evaluation_summary = results.get(\"evaluation_summary\", \"\")\n\n        # If no processed output, return empty string\n        if not processed_output:\n            logger.warning(f\"No processed output to format in committee phase '{self._phase_name}'\")\n            return \"\"\n\n        # If no formatting template, just return the processed output\n        if not self._config_manager.get_template(format_template_name):\n            return processed_output\n\n        # Format the output\n        try:\n            context = {\n                \"query\": query,\n                \"content\": processed_output,\n                \"evaluation\": evaluation_summary\n            }\n            formatted_prompt = self.render_template(format_template_name, context)\n        except (ConfigurationError, KeyError) as e:\n            # Log warning and return unformatted output\n            logger.warning(f\"Failed to format output: {str(e)}\")\n            return processed_output\n\n        # Run formatting models\n        model_results = await self._run_models(\n            prompt=formatted_prompt,\n            trace_collector=trace_collector\n        )\n\n        # Get the first result\n        if model_results:\n            first_model_id = next(iter(model_results.keys()))\n            formatted_output = model_results[first_model_id].get(\"text\", \"\")\n            return formatted_output\n\n        # Fallback to unformatted output\n        return processed_output\n</code></pre>"},{"location":"api/collaboration/#ai_ensemble_suite.collaboration.ExpertCommittee.__init__","title":"<code>__init__(model_manager, config_manager, phase_name)</code>","text":"<p>Initialize the expert committee phase.</p> <p>Parameters:</p> Name Type Description Default <code>model_manager</code> <code>ModelManager</code> <p>The ModelManager instance.</p> required <code>config_manager</code> <code>ConfigManager</code> <p>The ConfigManager instance.</p> required <code>phase_name</code> <code>str</code> <p>The name of the phase for configuration lookup.</p> required <p>Raises:</p> Type Description <code>ConfigurationError</code> <p>If phase configuration is invalid.</p> Source code in <code>src\\ai_ensemble_suite\\collaboration\\expert_committee.py</code> <pre><code>def __init__(\n    self, \n    model_manager: \"ModelManager\",\n    config_manager: \"ConfigManager\",\n    phase_name: str\n) -&gt; None:\n    \"\"\"Initialize the expert committee phase.\n\n    Args:\n        model_manager: The ModelManager instance.\n        config_manager: The ConfigManager instance.\n        phase_name: The name of the phase for configuration lookup.\n\n    Raises:\n        ConfigurationError: If phase configuration is invalid.\n    \"\"\"\n    super().__init__(model_manager, config_manager, phase_name)\n\n    # Get committee-specific configuration\n    self._committee_type = self._config.get(\"committee_type\", \"evaluative\")\n    self._evaluation_criteria = self._config.get(\"evaluation_criteria\", [\n        \"accuracy\", \"completeness\", \"clarity\", \"reasoning\"\n    ])\n    self._format_output = self._config.get(\"format_output\", True)\n\n    # Log configuration\n    logger.debug(\n        f\"Initialized Expert Committee phase '{phase_name}' with \"\n        f\"type '{self._committee_type}' and {len(self._evaluation_criteria)} criteria\"\n    )\n</code></pre>"},{"location":"api/collaboration/#ai_ensemble_suite.collaboration.ExpertCommittee.execute","title":"<code>execute(query, context, trace_collector=None)</code>  <code>async</code>","text":"<p>Execute the Expert Committee phase.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The user query to process.</p> required <code>context</code> <code>Dict[str, Any]</code> <p>Context information from previous phases.</p> required <code>trace_collector</code> <code>Optional[TraceCollector]</code> <p>Optional trace collector for gathering execution details.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary containing: output: The committee's final output. context: Updated context including evaluations and structured output.</p> <p>Raises:</p> Type Description <code>CollaborationError</code> <p>If phase execution fails.</p> Source code in <code>src\\ai_ensemble_suite\\collaboration\\expert_committee.py</code> <pre><code>async def execute(\n    self, \n    query: str,\n    context: Dict[str, Any],\n    trace_collector: Optional[TraceCollector] = None\n) -&gt; Dict[str, Any]:\n    \"\"\"Execute the Expert Committee phase.\n\n    Args:\n        query: The user query to process.\n        context: Context information from previous phases.\n        trace_collector: Optional trace collector for gathering execution details.\n\n    Returns:\n        Dictionary containing:\n            output: The committee's final output.\n            context: Updated context including evaluations and structured output.\n\n    Raises:\n        CollaborationError: If phase execution fails.\n    \"\"\"\n    start_time = time.time()\n\n    try:\n        # Get inputs from previous phases\n        inputs = self._get_inputs_from_context(context)\n\n        # Validate inputs\n        if not inputs and self._input_from:\n            raise CollaborationError(\n                f\"Expert Committee phase '{self._phase_name}' requires inputs from \"\n                f\"previous phases: {', '.join(self._input_from)}\"\n            )\n\n        # Find the content to evaluate based on inputs and committee type\n        processing_results = await self._process_inputs(query, inputs, trace_collector)\n\n        # If evaluative committee, perform evaluation\n        if self._committee_type == \"evaluative\":\n            evaluation_results = await self._evaluate_inputs(query, inputs, processing_results, trace_collector)\n            processing_results.update(evaluation_results)\n\n        # If formatting is enabled, format the final output\n        if self._format_output:\n            formatted_output = await self._format_final_output(\n                query, inputs, processing_results, trace_collector\n            )\n            processing_results[\"output\"] = formatted_output\n\n        execution_time = time.time() - start_time\n\n        # Log completion\n        logger.info(\n            f\"Expert Committee phase '{self._phase_name}' completed in {execution_time:.2f}s\",\n            extra={\n                \"committee_type\": self._committee_type,\n                \"phase\": self._phase_name\n            }\n        )\n\n        # Add phase trace if collector is provided\n        if trace_collector:\n            trace_collector.add_phase_trace(\n                phase_name=self._phase_name,\n                input_data={\"query\": query, \"inputs\": inputs},\n                output_data=processing_results,\n                execution_time=execution_time,\n                phase_parameters=self._config\n            )\n\n        return processing_results\n\n    except Exception as e:\n        raise CollaborationError(\n            f\"Expert Committee phase '{self._phase_name}' failed: {str(e)}\"\n        )\n</code></pre>"},{"location":"api/collaboration/#ai_ensemble_suite.collaboration.HierarchicalReview","title":"<code>HierarchicalReview</code>","text":"<p>               Bases: <code>BaseCollaborationPhase</code></p> <p>Hierarchical Review collaboration phase.</p> <p>Content is progressively reviewed and refined by models in a hierarchical structure, with specialists focusing on different aspects of the content.</p> Source code in <code>src\\ai_ensemble_suite\\collaboration\\hierarchical_review.py</code> <pre><code>class HierarchicalReview(BaseCollaborationPhase):\n    \"\"\"Hierarchical Review collaboration phase.\n\n    Content is progressively reviewed and refined by models in a hierarchical\n    structure, with specialists focusing on different aspects of the content.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_manager: \"ModelManager\",\n        config_manager: \"ConfigManager\",\n        phase_name: str\n    ) -&gt; None:\n        \"\"\"Initialize the hierarchical review phase.\n\n        Args:\n            model_manager: The ModelManager instance.\n            config_manager: The ConfigManager instance.\n            phase_name: The name of the phase for configuration lookup.\n\n        Raises:\n            ConfigurationError: If phase configuration is invalid.\n        \"\"\"\n        super().__init__(model_manager, config_manager, phase_name)\n\n        # Get review levels and reviewers\n        self._review_levels = self._config.get(\"review_levels\", [])\n        if not self._review_levels or not isinstance(self._review_levels, list):\n            logger.warning(\n                f\"No valid 'review_levels' list specified for HierarchicalReview phase '{phase_name}', \"\n                \"using default configuration based on available models.\"\n            )\n            # Set default review levels based on available models\n            # Ensure _model_ids is available from super().__init__\n            available_models = self._model_ids if hasattr(self, '_model_ids') else []\n            num_models = len(available_models)\n\n            # Generate default levels only if models are available\n            self._review_levels = []\n            if num_models &gt;= 1:\n                self._review_levels.append({\n                    \"name\": \"technical_review\",\n                     # Use first model\n                    \"models\": available_models[:1],\n                    \"template\": \"hierarchical_technical_review\" # Default template name\n                })\n            if num_models &gt;= 2:\n                 self._review_levels.append({\n                    \"name\": \"clarity_review\",\n                     # Use second model\n                    \"models\": available_models[1:2],\n                    \"template\": \"hierarchical_clarity_review\" # Default template name\n                 })\n            # Require at least 3 models for a separate final refinement step\n            # Otherwise, the previous step is effectively the final one\n            if num_models &gt;= 3:\n                 refinement_model_index = -1 # Use the last model\n                 self._review_levels.append({\n                    \"name\": \"final_refinement\",\n                     # Use last model\n                    \"models\": [available_models[refinement_model_index]],\n                    \"template\": \"hierarchical_final_refinement\" # Default template name\n                 })\n\n            if not self._review_levels:\n                 logger.warning(f\"Could not create default review levels for phase '{phase_name}' as no models are assigned.\")\n\n\n        # Further validation of review levels structure\n        if not self._review_levels:\n            # Allow phase to potentially run with no review levels if draft is generated,\n            # but log a warning. The execute method should handle this.\n            logger.warning(\n                f\"HierarchicalReview phase '{phase_name}' has no review levels configured or generated. It might only produce a draft.\"\n            )\n\n        # Validate each level's structure\n        all_level_models = set()\n        phase_models_set = set(self._model_ids) # Models configured for the overall phase\n        model_manager_models = self._model_manager.get_model_ids() if self._model_manager else set()\n\n        for i, level in enumerate(self._review_levels):\n             if not isinstance(level, dict):\n                 raise ConfigurationError(f\"Review level at index {i} for phase '{phase_name}' must be a dictionary.\")\n\n             # Ensure 'name' exists and is a string\n             level_name = level.get(\"name\")\n             if not level_name or not isinstance(level_name, str):\n                  default_name = f\"level_{i}\"\n                  logger.warning(f\"Review level {i} missing valid 'name', using default '{default_name}'.\")\n                  level[\"name\"] = default_name # Assign default name back into the config dict for this instance\n\n             # Ensure 'models' exists, is a list, and models are valid\n             level_models = level.get(\"models\")\n             if not isinstance(level_models, list):\n                  raise ConfigurationError(f\"Review level '{level['name']}' must have 'models' as a list.\")\n             if not level_models:\n                  logger.warning(f\"Review level '{level['name']}' has an empty 'models' list. It will be skipped.\")\n\n             # Check if models in level exist in ModelManager\n             for model_id in level_models:\n                  if model_id not in model_manager_models:\n                      raise ConfigurationError(f\"Model '{model_id}' specified in review level '{level['name']}' not found in ModelManager.\")\n                  all_level_models.add(model_id)\n\n             # Ensure 'template' exists and is a string\n             level_template = level.get(\"template\")\n             if not level_template or not isinstance(level_template, str):\n                  logger.warning(f\"Review level '{level['name']}' missing 'template' string. Will attempt fallback to phase default or named template.\")\n                  # No need to assign here, execute logic handles fallback\n\n        # Check if models used in levels are actually available in *this phase's* config list\n        missing_in_phase_config = all_level_models - phase_models_set\n        if missing_in_phase_config:\n             logger.warning(\n                 f\"Models {missing_in_phase_config} used in review_levels are not listed \"\n                 f\"in the main 'models' list ({list(phase_models_set)}) for phase '{phase_name}'. \"\n                 \"Ensure these models are loaded by the ModelManager.\"\n             )\n             # This might be intentional (using models not explicitly listed for the phase), or an error.\n\n        logger.debug(\n            f\"Initialized HierarchicalReview phase '{phase_name}' with \"\n            f\"{len(self._review_levels)} review levels.\"\n        )\n\n    async def execute(\n        self,\n        query: str,\n        context: Dict[str, Any],\n        trace_collector: Optional[TraceCollector] = None\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Execute the Hierarchical Review phase.\n\n        Args:\n            query: The user query to process.\n            context: Context information from previous phases.\n            trace_collector: Optional trace collector for gathering execution details.\n\n        Returns:\n            Dictionary containing:\n                output: The final reviewed and refined content.\n                reviews: Dictionary mapping level names to their outputs/status.\n                initial_content: The content before the review process started.\n                review_progression: List tracking content changes after each level.\n                confidence: Estimated confidence score for the final output.\n\n        Raises:\n            CollaborationError: If phase execution fails.\n        \"\"\"\n        start_time = time.time()\n\n        try:\n            # Get inputs from previous phases this phase depends on\n            inputs = self._get_inputs_from_context(context)\n\n            # Step 1: Get initial content to review (either from inputs or generate a draft)\n            initial_content = await self._get_initial_content(query, inputs, trace_collector) # Pass tracer\n            if not initial_content and inputs: # If draft failed but inputs exist, maybe use first input?\n                 logger.warning(f\"No draft generated for phase '{self._phase_name}', trying to use primary input.\")\n                 # Try to extract from the first input specified in 'input_from'\n                 primary_input_phase = self._input_from[0] if self._input_from else None\n                 if primary_input_phase and primary_input_phase in inputs:\n                      input_val = inputs[primary_input_phase]\n                      extracted = self._extract_input_content(input_val) # Use helper\n                      if extracted:\n                           initial_content = extracted\n                           logger.info(f\"Using content from input phase '{primary_input_phase}' as initial content.\")\n\n            # Create review process context, starting with the initial content\n            review_context = {\n                \"query\": query,\n                \"initial_content\": initial_content,\n                \"current_content\": initial_content, # This will be updated\n                \"reviews\": {}, # Store output of each review step keyed by level name\n                **inputs # Include inputs from previous phases for template formatting\n            }\n            # Track content changes (start with initial content)\n            review_progression = [initial_content]\n\n\n            # Check if there are any review levels to execute\n            if not self._review_levels:\n                 logger.warning(f\"Phase '{self._phase_name}' has no review levels. Output will be the initial content.\")\n                 last_valid_output = initial_content\n            else:\n                 # Process each review level defined in the configuration\n                 last_valid_output = initial_content # Keep track of the last successful output\n                 for level_idx, level_config in enumerate(self._review_levels):\n                    level_name = level_config.get(\"name\") # Should exist due to __init__ validation\n                    level_models = level_config.get(\"models\", [])\n                    level_template = level_config.get(\"template\") # Template name for this level\n\n                    logger.debug(f\"Executing review level '{level_name}' (Level {level_idx + 1}/{len(self._review_levels)})\")\n\n                    # If no models specified for this level, skip it\n                    if not level_models:\n                        logger.warning(f\"No models specified for review level '{level_name}', skipping.\")\n                        review_context[\"reviews\"][level_name] = {\"status\": \"skipped\", \"reason\": \"no models\"}\n                        review_progression.append(last_valid_output) # Content doesn't change\n                        continue\n\n                    # Determine the template name to use\n                    # Priority: level-specific template &gt; phase default template &gt; constructed fallback\n                    template_to_use = level_template # From level config\n                    if not template_to_use:\n                        template_to_use = self._prompt_template # From phase config (via Base class)\n                    if not template_to_use:\n                         # Construct a fallback template name if none specified anywhere\n                         template_to_use = f\"hierarchical_{level_name}\" # e.g., hierarchical_technical_review\n                         logger.warning(f\"No template specified for level '{level_name}', trying fallback name '{template_to_use}'.\")\n\n\n                    # Format prompt for this review level, ensuring current content is passed\n                    try:\n                        context = {**review_context}\n                        # Ensure 'current_content' key exists even if empty\n                        context.setdefault(\"current_content\", \"\")\n                        # Add a specific key for the content being reviewed *in this step* for clarity in templates\n                        context[\"content_to_review\"] = review_context.get(\"current_content\", \"\")\n\n                        # Format using BaseCollaborationPhase method\n                        review_prompt = self.render_template(template_to_use, context)\n                    except (ConfigurationError, KeyError, CollaborationError) as e: # Catch formatting or template errors\n                        logger.error(f\"Failed to format prompt for review level '{level_name}' using template '{template_to_use}': {str(e)}. Skipping level.\", exc_info=True)\n                        review_context[\"reviews\"][level_name] = {\"status\": \"error\", \"reason\": f\"prompt formatting error: {e}\"}\n                        review_progression.append(last_valid_output) # Content doesn't change\n                        continue\n                    except Exception as e: # Catch other unexpected formatting errors\n                         logger.error(f\"Unexpected error formatting prompt for level '{level_name}': {str(e)}.\", exc_info=True)\n                         review_context[\"reviews\"][level_name] = {\"status\": \"error\", \"reason\": f\"unexpected prompt error: {e}\"}\n                         review_progression.append(last_valid_output)\n                         continue\n\n\n                    # Run the specified models for this review level\n                    try:\n                        # Use _run_models which handles multiple models and tracing internally\n                        # Pass role info for tracing\n                        level_results = await self._run_models(\n                            prompt=review_prompt,\n                            model_ids=level_models,\n                            trace_collector=trace_collector,\n                            role=f\"reviewer_{level_name}\" # Add role context for tracing\n                        )\n                    except (CollaborationError, ModelError) as e:\n                         logger.error(f\"Failed to run models for review level '{level_name}': {str(e)}. Skipping level.\")\n                         review_context[\"reviews\"][level_name] = {\"status\": \"error\", \"reason\": f\"model execution error: {e}\"}\n                         review_progression.append(last_valid_output) # Content doesn't change\n                         continue\n                    except Exception as e:\n                         logger.error(f\"Unexpected error running models for level '{level_name}': {str(e)}.\", exc_info=True)\n                         review_context[\"reviews\"][level_name] = {\"status\": \"error\", \"reason\": f\"unexpected model error: {e}\"}\n                         review_progression.append(last_valid_output)\n                         continue\n\n\n                    # --- Process outputs from the level's models ---\n                    # Extract text outputs from model results, handling potential errors\n                    level_outputs_text: Dict[str, str] = {}\n                    failed_models = []\n                    for model_id, result in level_results.items():\n                        if isinstance(result, dict):\n                            if \"error\" in result:\n                                 logger.warning(f\"Model '{model_id}' failed during review level '{level_name}': {result['error']}\")\n                                 failed_models.append(model_id)\n                            else:\n                                 level_outputs_text[model_id] = result.get(\"text\", \"\").strip()\n                        else:\n                             logger.warning(f\"Unexpected result type from model '{model_id}' for level '{level_name}': {type(result)}\")\n                             failed_models.append(model_id)\n\n\n                    # Combine outputs if multiple models contributed *successfully* to this level\n                    successful_outputs = {mid: txt for mid, txt in level_outputs_text.items() if mid not in failed_models and txt}\n\n                    level_output_final = \"\" # Initialize final output for the level\n                    if len(successful_outputs) &gt; 1:\n                        logger.debug(f\"Combining {len(successful_outputs)} outputs for level '{level_name}'\")\n                        # Combine outputs with headers - simple concatenation for now\n                        combined_output = f\"# Combined Review/Refinement from Level '{level_name}'\\n\\n\"\n                        for model_id, output_text in successful_outputs.items():\n                            combined_output += f\"## Contribution from {model_id}\\n\\n{output_text}\\n\\n---\\n\\n\"\n                        level_output_final = combined_output.strip() # Use combined text\n                    elif len(successful_outputs) == 1:\n                        # Just use the single successful model's output text\n                        level_output_final = next(iter(successful_outputs.values()))\n                    else:\n                        # No valid successful outputs from this level\n                        logger.warning(f\"No valid text output received from models for review level '{level_name}'. Keeping previous content.\")\n                        level_output_final = last_valid_output # Keep the content from before this level\n                        review_context[\"reviews\"][level_name] = {\"status\": \"warning\", \"reason\": \"no text output from models\"}\n\n\n                    # Update 'current_content' and store results *only if* new content was generated\n                    if level_output_final != last_valid_output:\n                         review_context[\"reviews\"][level_name] = {\n                             \"status\": \"completed\",\n                             \"output\": level_output_final,\n                             \"raw_model_results\": level_results # Store raw results too\n                         }\n                         # Add role-specific key in context for easier access in subsequent steps/templates\n                         review_context[level_name] = level_output_final\n\n                         review_context[\"current_content\"] = level_output_final\n                         last_valid_output = level_output_final # Update tracker\n                         review_progression.append(level_output_final) # Add to progression\n                    else:\n                         # If content didn't change (e.g., no output), just record the status\n                         if level_name not in review_context[\"reviews\"]: # Avoid overwriting previous status\n                              review_context[\"reviews\"][level_name] = {\n                                    \"status\": \"no_change\",\n                                    \"reason\": \"Output matched previous content or was empty.\",\n                                    \"raw_model_results\": level_results\n                              }\n                         review_progression.append(last_valid_output) # Content didn't change\n\n\n\n            # --- Post-loop ---\n            execution_time = time.time() - start_time\n\n            logger.info(\n                f\"HierarchicalReview phase '{self._phase_name}' completed {len(self._review_levels)} levels in {execution_time:.2f}s\"\n            )\n\n            # Calculate final confidence score (e.g., average confidence from the *last active* level)\n            confidence = 0.7 # Default confidence\n            last_active_level_name = None\n            # Find the last level that actually completed or resulted in no change\n            for level_config in reversed(self._review_levels):\n                 lname = level_config[\"name\"]\n                 if lname in review_context[\"reviews\"] and review_context[\"reviews\"][lname].get(\"status\") in [\"completed\", \"no_change\"]:\n                      last_active_level_name = lname\n                      break\n\n            if last_active_level_name:\n                 final_review_data = review_context[\"reviews\"][last_active_level_name]\n                 final_level_raw_results = final_review_data.get(\"raw_model_results\", {})\n                 confidence_values = []\n                 for result in final_level_raw_results.values():\n                       # Extract confidence from the result dict (using 'combined' if available)\n                       model_conf = result.get(\"confidence\") # Should be dict from run_inference\n                       if isinstance(model_conf, dict):\n                           confidence_values.append(model_conf.get(\"combined\", 0.0)) # Use combined or 0\n                       elif isinstance(model_conf, (float, int)):\n                            # Less likely, but handle direct score\n                           confidence_values.append(max(0.0, min(1.0, float(model_conf))))\n                 if confidence_values:\n                     confidence = sum(confidence_values) / len(confidence_values)\n\n\n            # Add phase trace if collector is provided\n            if trace_collector:\n                # Create output data, potentially trimming large fields\n                output_trace_data = {\n                     \"output\": last_valid_output, # Final content\n                     \"reviews\": review_context.get(\"reviews\", {}), # Status/outputs of levels\n                     \"initial_content\": initial_content,\n                     \"confidence\": confidence,\n                     # Consider not tracing full progression if very long\n                     # \"review_progression\": review_progression\n                }\n                # Clean potentially large raw results from trace output\n                if \"reviews\" in output_trace_data:\n                     for level_name, level_data in output_trace_data[\"reviews\"].items():\n                          if \"raw_model_results\" in level_data:\n                               level_data[\"raw_model_results\"] = \"...\" # Placeholder\n\n                trace_collector.add_phase_trace(\n                    phase_name=self._phase_name,\n                    input_data={\"query\": query, \"inputs\": list(inputs.keys())}, # Initial inputs keys\n                    output_data=output_trace_data,\n                    execution_time=execution_time,\n                    phase_parameters=self._config # Include phase config (review levels etc.)\n                )\n\n\n            # Return the final results\n            return {\n                \"output\": last_valid_output, # The content after the last successful level\n                \"reviews\": review_context.get(\"reviews\", {}), # Details of each level\n                \"initial_content\": initial_content,\n                \"review_progression\": review_progression, # History of content state\n                \"confidence\": confidence\n            }\n\n        except CollaborationError as e:\n             # Log and re-raise known collaboration errors\n             logger.error(f\"Collaboration error in HierarchicalReview phase '{self._phase_name}': {str(e)}\", exc_info=True)\n             raise\n        except Exception as e:\n            # Catch-all for unexpected errors during execution\n            logger.error(f\"Unexpected error in HierarchicalReview phase '{self._phase_name}': {str(e)}\", exc_info=True)\n            # Wrap in CollaborationError for consistent handling upstream\n            raise CollaborationError(f\"HierarchicalReview phase '{self._phase_name}' failed unexpectedly: {str(e)}\")\n\n\n    async def _get_initial_content(\n        self,\n        query: str,\n        inputs: Dict[str, Any],\n        trace_collector: Optional[TraceCollector] = None # Added tracer\n    ) -&gt; str:\n        \"\"\"Get or generate the initial content to be reviewed by the hierarchy.\n\n        Args:\n            query: The user query.\n            inputs: Inputs from previous phases (if specified by 'input_from').\n            trace_collector: Optional trace collector.\n\n        Returns:\n            String containing the initial content, or empty string if none found/generated.\n        \"\"\"\n        # --- Priority 1: Check if specified inputs already provide content ---\n        # Use input_from defined in the phase config\n        if self._input_from:\n            logger.debug(f\"Checking inputs {self._input_from} for initial content...\")\n            for source_phase_name in self._input_from:\n                if source_phase_name in inputs:\n                    source_data = inputs[source_phase_name]\n                    content = self._extract_input_content(source_data) # Use helper\n                    if content:\n                         logger.info(f\"Using initial content from previous phase: '{source_phase_name}'\")\n                         return content\n\n        logger.debug(\"No suitable initial content found in configured inputs.\")\n\n        # --- Priority 2: Generate a draft if configured ---\n        draft_template = self._config.get(\"draft_template\")\n        if draft_template:\n            logger.debug(f\"Attempting to generate initial draft using template: '{draft_template}'\")\n            try:\n                # Combine query and any available inputs for the draft prompt\n                context = {\"query\": query, **inputs}\n                # Use render_template from Base Collaboration class\n                draft_prompt = self.render_template(draft_template, context)\n\n                # Determine which model(s) to use for drafting\n                draft_model_id = self._config.get(\"draft_model\") # Explicit draft model?\n                draft_models = []\n                if draft_model_id and isinstance(draft_model_id, str):\n                    # Ensure specified draft model is managed\n                    if draft_model_id in self._model_manager.get_model_ids():\n                         draft_models = [draft_model_id]\n                    else:\n                         logger.warning(f\"Specified draft_model '{draft_model_id}' not found in ModelManager. Cannot use for drafting.\")\n                elif self._model_ids: # Use first model assigned to the phase?\n                    draft_models = self._model_ids[:1]\n\n                if not draft_models:\n                    logger.warning(f\"No models specified or available for drafting in phase '{self._phase_name}'. Cannot generate draft.\")\n                    return \"\" # Cannot generate draft\n\n                logger.info(f\"Generating draft using model(s): {draft_models}\")\n\n                # Run drafting model(s)\n                # *** FIX: Use await directly, remove asyncio.run() ***\n                # Use _run_models from BaseCollaborationPhase\n                draft_results = await self._run_models(\n                    prompt=draft_prompt,\n                    model_ids=draft_models,\n                    trace_collector=trace_collector, # Pass tracer\n                    role=\"draft_generator\" # Add role for tracing\n                )\n\n                # Extract the first successful result's text\n                if draft_results:\n                    for model_id, result in draft_results.items(): # Iterate to find first success\n                         if isinstance(result, dict) and \"error\" not in result:\n                              draft_content = result.get(\"text\", \"\").strip()\n                              if draft_content:\n                                   logger.info(f\"Generated initial draft using model '{model_id}'.\")\n                                   return draft_content\n                              else:\n                                   logger.warning(f\"Draft generation by '{model_id}' resulted in empty content.\")\n                         elif isinstance(result, dict) and \"error\" in result:\n                              logger.warning(f\"Draft generation failed for model '{model_id}': {result['error']}\")\n\n                    # If loop finishes without finding good content\n                    logger.warning(\"Draft generation ran but produced no valid content.\")\n                else:\n                     logger.warning(\"Draft generation failed or returned no results.\")\n\n            except (ConfigurationError, KeyError, CollaborationError) as e:\n                logger.error(f\"Failed to format or run draft generation using template '{draft_template}': {str(e)}\")\n            except Exception as e:\n                logger.error(f\"Unexpected error generating draft content: {str(e)}\", exc_info=True)\n\n        # --- Fallback: No content found or generated ---\n        logger.warning(f\"No initial content could be found or generated for HierarchicalReview phase '{self._phase_name}'. Proceeding with empty content.\")\n        return \"\"\n\n    def _extract_input_content(self, source_data: Any) -&gt; str:\n        \"\"\"Helper to extract string content from various input types.\"\"\"\n        content = \"\"\n        if isinstance(source_data, str):\n            content = source_data\n        elif isinstance(source_data, dict):\n             # Try common keys for output text\n            content = source_data.get(\"output\") or \\\n                      source_data.get(\"text\") or \\\n                      source_data.get(\"response\") or \\\n                      source_data.get(\"final_output\") # Add other likely keys\n        # Add handling for other potential types if necessary\n        # else: logger.debug(...)\n\n        return content.strip() if content and isinstance(content, str) else \"\"\n</code></pre>"},{"location":"api/collaboration/#ai_ensemble_suite.collaboration.HierarchicalReview.__init__","title":"<code>__init__(model_manager, config_manager, phase_name)</code>","text":"<p>Initialize the hierarchical review phase.</p> <p>Parameters:</p> Name Type Description Default <code>model_manager</code> <code>ModelManager</code> <p>The ModelManager instance.</p> required <code>config_manager</code> <code>ConfigManager</code> <p>The ConfigManager instance.</p> required <code>phase_name</code> <code>str</code> <p>The name of the phase for configuration lookup.</p> required <p>Raises:</p> Type Description <code>ConfigurationError</code> <p>If phase configuration is invalid.</p> Source code in <code>src\\ai_ensemble_suite\\collaboration\\hierarchical_review.py</code> <pre><code>def __init__(\n    self,\n    model_manager: \"ModelManager\",\n    config_manager: \"ConfigManager\",\n    phase_name: str\n) -&gt; None:\n    \"\"\"Initialize the hierarchical review phase.\n\n    Args:\n        model_manager: The ModelManager instance.\n        config_manager: The ConfigManager instance.\n        phase_name: The name of the phase for configuration lookup.\n\n    Raises:\n        ConfigurationError: If phase configuration is invalid.\n    \"\"\"\n    super().__init__(model_manager, config_manager, phase_name)\n\n    # Get review levels and reviewers\n    self._review_levels = self._config.get(\"review_levels\", [])\n    if not self._review_levels or not isinstance(self._review_levels, list):\n        logger.warning(\n            f\"No valid 'review_levels' list specified for HierarchicalReview phase '{phase_name}', \"\n            \"using default configuration based on available models.\"\n        )\n        # Set default review levels based on available models\n        # Ensure _model_ids is available from super().__init__\n        available_models = self._model_ids if hasattr(self, '_model_ids') else []\n        num_models = len(available_models)\n\n        # Generate default levels only if models are available\n        self._review_levels = []\n        if num_models &gt;= 1:\n            self._review_levels.append({\n                \"name\": \"technical_review\",\n                 # Use first model\n                \"models\": available_models[:1],\n                \"template\": \"hierarchical_technical_review\" # Default template name\n            })\n        if num_models &gt;= 2:\n             self._review_levels.append({\n                \"name\": \"clarity_review\",\n                 # Use second model\n                \"models\": available_models[1:2],\n                \"template\": \"hierarchical_clarity_review\" # Default template name\n             })\n        # Require at least 3 models for a separate final refinement step\n        # Otherwise, the previous step is effectively the final one\n        if num_models &gt;= 3:\n             refinement_model_index = -1 # Use the last model\n             self._review_levels.append({\n                \"name\": \"final_refinement\",\n                 # Use last model\n                \"models\": [available_models[refinement_model_index]],\n                \"template\": \"hierarchical_final_refinement\" # Default template name\n             })\n\n        if not self._review_levels:\n             logger.warning(f\"Could not create default review levels for phase '{phase_name}' as no models are assigned.\")\n\n\n    # Further validation of review levels structure\n    if not self._review_levels:\n        # Allow phase to potentially run with no review levels if draft is generated,\n        # but log a warning. The execute method should handle this.\n        logger.warning(\n            f\"HierarchicalReview phase '{phase_name}' has no review levels configured or generated. It might only produce a draft.\"\n        )\n\n    # Validate each level's structure\n    all_level_models = set()\n    phase_models_set = set(self._model_ids) # Models configured for the overall phase\n    model_manager_models = self._model_manager.get_model_ids() if self._model_manager else set()\n\n    for i, level in enumerate(self._review_levels):\n         if not isinstance(level, dict):\n             raise ConfigurationError(f\"Review level at index {i} for phase '{phase_name}' must be a dictionary.\")\n\n         # Ensure 'name' exists and is a string\n         level_name = level.get(\"name\")\n         if not level_name or not isinstance(level_name, str):\n              default_name = f\"level_{i}\"\n              logger.warning(f\"Review level {i} missing valid 'name', using default '{default_name}'.\")\n              level[\"name\"] = default_name # Assign default name back into the config dict for this instance\n\n         # Ensure 'models' exists, is a list, and models are valid\n         level_models = level.get(\"models\")\n         if not isinstance(level_models, list):\n              raise ConfigurationError(f\"Review level '{level['name']}' must have 'models' as a list.\")\n         if not level_models:\n              logger.warning(f\"Review level '{level['name']}' has an empty 'models' list. It will be skipped.\")\n\n         # Check if models in level exist in ModelManager\n         for model_id in level_models:\n              if model_id not in model_manager_models:\n                  raise ConfigurationError(f\"Model '{model_id}' specified in review level '{level['name']}' not found in ModelManager.\")\n              all_level_models.add(model_id)\n\n         # Ensure 'template' exists and is a string\n         level_template = level.get(\"template\")\n         if not level_template or not isinstance(level_template, str):\n              logger.warning(f\"Review level '{level['name']}' missing 'template' string. Will attempt fallback to phase default or named template.\")\n              # No need to assign here, execute logic handles fallback\n\n    # Check if models used in levels are actually available in *this phase's* config list\n    missing_in_phase_config = all_level_models - phase_models_set\n    if missing_in_phase_config:\n         logger.warning(\n             f\"Models {missing_in_phase_config} used in review_levels are not listed \"\n             f\"in the main 'models' list ({list(phase_models_set)}) for phase '{phase_name}'. \"\n             \"Ensure these models are loaded by the ModelManager.\"\n         )\n         # This might be intentional (using models not explicitly listed for the phase), or an error.\n\n    logger.debug(\n        f\"Initialized HierarchicalReview phase '{phase_name}' with \"\n        f\"{len(self._review_levels)} review levels.\"\n    )\n</code></pre>"},{"location":"api/collaboration/#ai_ensemble_suite.collaboration.HierarchicalReview.execute","title":"<code>execute(query, context, trace_collector=None)</code>  <code>async</code>","text":"<p>Execute the Hierarchical Review phase.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The user query to process.</p> required <code>context</code> <code>Dict[str, Any]</code> <p>Context information from previous phases.</p> required <code>trace_collector</code> <code>Optional[TraceCollector]</code> <p>Optional trace collector for gathering execution details.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary containing: output: The final reviewed and refined content. reviews: Dictionary mapping level names to their outputs/status. initial_content: The content before the review process started. review_progression: List tracking content changes after each level. confidence: Estimated confidence score for the final output.</p> <p>Raises:</p> Type Description <code>CollaborationError</code> <p>If phase execution fails.</p> Source code in <code>src\\ai_ensemble_suite\\collaboration\\hierarchical_review.py</code> <pre><code>async def execute(\n    self,\n    query: str,\n    context: Dict[str, Any],\n    trace_collector: Optional[TraceCollector] = None\n) -&gt; Dict[str, Any]:\n    \"\"\"Execute the Hierarchical Review phase.\n\n    Args:\n        query: The user query to process.\n        context: Context information from previous phases.\n        trace_collector: Optional trace collector for gathering execution details.\n\n    Returns:\n        Dictionary containing:\n            output: The final reviewed and refined content.\n            reviews: Dictionary mapping level names to their outputs/status.\n            initial_content: The content before the review process started.\n            review_progression: List tracking content changes after each level.\n            confidence: Estimated confidence score for the final output.\n\n    Raises:\n        CollaborationError: If phase execution fails.\n    \"\"\"\n    start_time = time.time()\n\n    try:\n        # Get inputs from previous phases this phase depends on\n        inputs = self._get_inputs_from_context(context)\n\n        # Step 1: Get initial content to review (either from inputs or generate a draft)\n        initial_content = await self._get_initial_content(query, inputs, trace_collector) # Pass tracer\n        if not initial_content and inputs: # If draft failed but inputs exist, maybe use first input?\n             logger.warning(f\"No draft generated for phase '{self._phase_name}', trying to use primary input.\")\n             # Try to extract from the first input specified in 'input_from'\n             primary_input_phase = self._input_from[0] if self._input_from else None\n             if primary_input_phase and primary_input_phase in inputs:\n                  input_val = inputs[primary_input_phase]\n                  extracted = self._extract_input_content(input_val) # Use helper\n                  if extracted:\n                       initial_content = extracted\n                       logger.info(f\"Using content from input phase '{primary_input_phase}' as initial content.\")\n\n        # Create review process context, starting with the initial content\n        review_context = {\n            \"query\": query,\n            \"initial_content\": initial_content,\n            \"current_content\": initial_content, # This will be updated\n            \"reviews\": {}, # Store output of each review step keyed by level name\n            **inputs # Include inputs from previous phases for template formatting\n        }\n        # Track content changes (start with initial content)\n        review_progression = [initial_content]\n\n\n        # Check if there are any review levels to execute\n        if not self._review_levels:\n             logger.warning(f\"Phase '{self._phase_name}' has no review levels. Output will be the initial content.\")\n             last_valid_output = initial_content\n        else:\n             # Process each review level defined in the configuration\n             last_valid_output = initial_content # Keep track of the last successful output\n             for level_idx, level_config in enumerate(self._review_levels):\n                level_name = level_config.get(\"name\") # Should exist due to __init__ validation\n                level_models = level_config.get(\"models\", [])\n                level_template = level_config.get(\"template\") # Template name for this level\n\n                logger.debug(f\"Executing review level '{level_name}' (Level {level_idx + 1}/{len(self._review_levels)})\")\n\n                # If no models specified for this level, skip it\n                if not level_models:\n                    logger.warning(f\"No models specified for review level '{level_name}', skipping.\")\n                    review_context[\"reviews\"][level_name] = {\"status\": \"skipped\", \"reason\": \"no models\"}\n                    review_progression.append(last_valid_output) # Content doesn't change\n                    continue\n\n                # Determine the template name to use\n                # Priority: level-specific template &gt; phase default template &gt; constructed fallback\n                template_to_use = level_template # From level config\n                if not template_to_use:\n                    template_to_use = self._prompt_template # From phase config (via Base class)\n                if not template_to_use:\n                     # Construct a fallback template name if none specified anywhere\n                     template_to_use = f\"hierarchical_{level_name}\" # e.g., hierarchical_technical_review\n                     logger.warning(f\"No template specified for level '{level_name}', trying fallback name '{template_to_use}'.\")\n\n\n                # Format prompt for this review level, ensuring current content is passed\n                try:\n                    context = {**review_context}\n                    # Ensure 'current_content' key exists even if empty\n                    context.setdefault(\"current_content\", \"\")\n                    # Add a specific key for the content being reviewed *in this step* for clarity in templates\n                    context[\"content_to_review\"] = review_context.get(\"current_content\", \"\")\n\n                    # Format using BaseCollaborationPhase method\n                    review_prompt = self.render_template(template_to_use, context)\n                except (ConfigurationError, KeyError, CollaborationError) as e: # Catch formatting or template errors\n                    logger.error(f\"Failed to format prompt for review level '{level_name}' using template '{template_to_use}': {str(e)}. Skipping level.\", exc_info=True)\n                    review_context[\"reviews\"][level_name] = {\"status\": \"error\", \"reason\": f\"prompt formatting error: {e}\"}\n                    review_progression.append(last_valid_output) # Content doesn't change\n                    continue\n                except Exception as e: # Catch other unexpected formatting errors\n                     logger.error(f\"Unexpected error formatting prompt for level '{level_name}': {str(e)}.\", exc_info=True)\n                     review_context[\"reviews\"][level_name] = {\"status\": \"error\", \"reason\": f\"unexpected prompt error: {e}\"}\n                     review_progression.append(last_valid_output)\n                     continue\n\n\n                # Run the specified models for this review level\n                try:\n                    # Use _run_models which handles multiple models and tracing internally\n                    # Pass role info for tracing\n                    level_results = await self._run_models(\n                        prompt=review_prompt,\n                        model_ids=level_models,\n                        trace_collector=trace_collector,\n                        role=f\"reviewer_{level_name}\" # Add role context for tracing\n                    )\n                except (CollaborationError, ModelError) as e:\n                     logger.error(f\"Failed to run models for review level '{level_name}': {str(e)}. Skipping level.\")\n                     review_context[\"reviews\"][level_name] = {\"status\": \"error\", \"reason\": f\"model execution error: {e}\"}\n                     review_progression.append(last_valid_output) # Content doesn't change\n                     continue\n                except Exception as e:\n                     logger.error(f\"Unexpected error running models for level '{level_name}': {str(e)}.\", exc_info=True)\n                     review_context[\"reviews\"][level_name] = {\"status\": \"error\", \"reason\": f\"unexpected model error: {e}\"}\n                     review_progression.append(last_valid_output)\n                     continue\n\n\n                # --- Process outputs from the level's models ---\n                # Extract text outputs from model results, handling potential errors\n                level_outputs_text: Dict[str, str] = {}\n                failed_models = []\n                for model_id, result in level_results.items():\n                    if isinstance(result, dict):\n                        if \"error\" in result:\n                             logger.warning(f\"Model '{model_id}' failed during review level '{level_name}': {result['error']}\")\n                             failed_models.append(model_id)\n                        else:\n                             level_outputs_text[model_id] = result.get(\"text\", \"\").strip()\n                    else:\n                         logger.warning(f\"Unexpected result type from model '{model_id}' for level '{level_name}': {type(result)}\")\n                         failed_models.append(model_id)\n\n\n                # Combine outputs if multiple models contributed *successfully* to this level\n                successful_outputs = {mid: txt for mid, txt in level_outputs_text.items() if mid not in failed_models and txt}\n\n                level_output_final = \"\" # Initialize final output for the level\n                if len(successful_outputs) &gt; 1:\n                    logger.debug(f\"Combining {len(successful_outputs)} outputs for level '{level_name}'\")\n                    # Combine outputs with headers - simple concatenation for now\n                    combined_output = f\"# Combined Review/Refinement from Level '{level_name}'\\n\\n\"\n                    for model_id, output_text in successful_outputs.items():\n                        combined_output += f\"## Contribution from {model_id}\\n\\n{output_text}\\n\\n---\\n\\n\"\n                    level_output_final = combined_output.strip() # Use combined text\n                elif len(successful_outputs) == 1:\n                    # Just use the single successful model's output text\n                    level_output_final = next(iter(successful_outputs.values()))\n                else:\n                    # No valid successful outputs from this level\n                    logger.warning(f\"No valid text output received from models for review level '{level_name}'. Keeping previous content.\")\n                    level_output_final = last_valid_output # Keep the content from before this level\n                    review_context[\"reviews\"][level_name] = {\"status\": \"warning\", \"reason\": \"no text output from models\"}\n\n\n                # Update 'current_content' and store results *only if* new content was generated\n                if level_output_final != last_valid_output:\n                     review_context[\"reviews\"][level_name] = {\n                         \"status\": \"completed\",\n                         \"output\": level_output_final,\n                         \"raw_model_results\": level_results # Store raw results too\n                     }\n                     # Add role-specific key in context for easier access in subsequent steps/templates\n                     review_context[level_name] = level_output_final\n\n                     review_context[\"current_content\"] = level_output_final\n                     last_valid_output = level_output_final # Update tracker\n                     review_progression.append(level_output_final) # Add to progression\n                else:\n                     # If content didn't change (e.g., no output), just record the status\n                     if level_name not in review_context[\"reviews\"]: # Avoid overwriting previous status\n                          review_context[\"reviews\"][level_name] = {\n                                \"status\": \"no_change\",\n                                \"reason\": \"Output matched previous content or was empty.\",\n                                \"raw_model_results\": level_results\n                          }\n                     review_progression.append(last_valid_output) # Content didn't change\n\n\n\n        # --- Post-loop ---\n        execution_time = time.time() - start_time\n\n        logger.info(\n            f\"HierarchicalReview phase '{self._phase_name}' completed {len(self._review_levels)} levels in {execution_time:.2f}s\"\n        )\n\n        # Calculate final confidence score (e.g., average confidence from the *last active* level)\n        confidence = 0.7 # Default confidence\n        last_active_level_name = None\n        # Find the last level that actually completed or resulted in no change\n        for level_config in reversed(self._review_levels):\n             lname = level_config[\"name\"]\n             if lname in review_context[\"reviews\"] and review_context[\"reviews\"][lname].get(\"status\") in [\"completed\", \"no_change\"]:\n                  last_active_level_name = lname\n                  break\n\n        if last_active_level_name:\n             final_review_data = review_context[\"reviews\"][last_active_level_name]\n             final_level_raw_results = final_review_data.get(\"raw_model_results\", {})\n             confidence_values = []\n             for result in final_level_raw_results.values():\n                   # Extract confidence from the result dict (using 'combined' if available)\n                   model_conf = result.get(\"confidence\") # Should be dict from run_inference\n                   if isinstance(model_conf, dict):\n                       confidence_values.append(model_conf.get(\"combined\", 0.0)) # Use combined or 0\n                   elif isinstance(model_conf, (float, int)):\n                        # Less likely, but handle direct score\n                       confidence_values.append(max(0.0, min(1.0, float(model_conf))))\n             if confidence_values:\n                 confidence = sum(confidence_values) / len(confidence_values)\n\n\n        # Add phase trace if collector is provided\n        if trace_collector:\n            # Create output data, potentially trimming large fields\n            output_trace_data = {\n                 \"output\": last_valid_output, # Final content\n                 \"reviews\": review_context.get(\"reviews\", {}), # Status/outputs of levels\n                 \"initial_content\": initial_content,\n                 \"confidence\": confidence,\n                 # Consider not tracing full progression if very long\n                 # \"review_progression\": review_progression\n            }\n            # Clean potentially large raw results from trace output\n            if \"reviews\" in output_trace_data:\n                 for level_name, level_data in output_trace_data[\"reviews\"].items():\n                      if \"raw_model_results\" in level_data:\n                           level_data[\"raw_model_results\"] = \"...\" # Placeholder\n\n            trace_collector.add_phase_trace(\n                phase_name=self._phase_name,\n                input_data={\"query\": query, \"inputs\": list(inputs.keys())}, # Initial inputs keys\n                output_data=output_trace_data,\n                execution_time=execution_time,\n                phase_parameters=self._config # Include phase config (review levels etc.)\n            )\n\n\n        # Return the final results\n        return {\n            \"output\": last_valid_output, # The content after the last successful level\n            \"reviews\": review_context.get(\"reviews\", {}), # Details of each level\n            \"initial_content\": initial_content,\n            \"review_progression\": review_progression, # History of content state\n            \"confidence\": confidence\n        }\n\n    except CollaborationError as e:\n         # Log and re-raise known collaboration errors\n         logger.error(f\"Collaboration error in HierarchicalReview phase '{self._phase_name}': {str(e)}\", exc_info=True)\n         raise\n    except Exception as e:\n        # Catch-all for unexpected errors during execution\n        logger.error(f\"Unexpected error in HierarchicalReview phase '{self._phase_name}': {str(e)}\", exc_info=True)\n        # Wrap in CollaborationError for consistent handling upstream\n        raise CollaborationError(f\"HierarchicalReview phase '{self._phase_name}' failed unexpectedly: {str(e)}\")\n</code></pre>"},{"location":"api/collaboration/#ai_ensemble_suite.collaboration.Integration","title":"<code>Integration</code>","text":"<p>               Bases: <code>BaseCollaborationPhase</code></p> <p>Integration/Refinement collaboration phase.</p> <p>Models refine responses based on feedback and insights from previous phases, integrating multiple perspectives into a coherent response.</p> Source code in <code>src\\ai_ensemble_suite\\collaboration\\integration.py</code> <pre><code>class Integration(BaseCollaborationPhase):\n    \"\"\"Integration/Refinement collaboration phase.\n\n    Models refine responses based on feedback and insights from previous phases,\n    integrating multiple perspectives into a coherent response.\n    \"\"\"\n\n    async def execute(\n        self, \n        query: str,\n        context: Dict[str, Any],\n        trace_collector: Optional[TraceCollector] = None\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Execute the Integration phase.\n\n        Args:\n            query: The user query to process.\n            context: Context information from previous phases.\n            trace_collector: Optional trace collector for gathering execution details.\n\n        Returns:\n            Dictionary containing:\n                output: The integrated/refined response.\n                context: Updated context for the next phase.\n\n        Raises:\n            CollaborationError: If phase execution fails.\n        \"\"\"\n        start_time = time.time()\n\n        try:\n            # Get inputs from previous phases\n            inputs = self._get_inputs_from_context(context)\n\n            # Validate inputs\n            if not inputs and self._input_from:\n                raise CollaborationError(\n                    f\"Integration phase '{self._phase_name}' requires inputs from \"\n                    f\"previous phases: {', '.join(self._input_from)}\"\n                )\n\n            # Get prompt template or use default integration prompt\n            if not self._prompt_template:\n                logger.warning(\n                    f\"No prompt template specified for Integration phase '{self._phase_name}', \"\n                    \"using default integration prompt\"\n                )\n                self._prompt_template = \"refinement\"\n\n            # Format prompt with query and inputs\n            try:\n                context = {\"query\": query, **inputs}\n                integration_prompt = self.render_template(self._prompt_template, context)\n            except (ConfigurationError, KeyError) as e:\n                raise CollaborationError(f\"Failed to format integration prompt: {str(e)}\")\n\n            # Run models\n            model_results = await self._run_models(\n                prompt=integration_prompt,\n                trace_collector=trace_collector\n            )\n\n            # Process outputs\n            integrated_outputs = {}\n            for model_id, result in model_results.items():\n                integrated_outputs[model_id] = result.get(\"text\", \"\")\n\n            # Determine the primary integrated output\n            primary_output = \"\"\n            if len(integrated_outputs) == 1:\n                # Single model case\n                primary_output = list(integrated_outputs.values())[0]\n            elif self._model_ids and len(self._model_ids) &gt; 0:\n                # Use the first model as primary\n                primary_model = self._model_ids[0]\n                if primary_model in integrated_outputs:\n                    primary_output = integrated_outputs[primary_model]\n                else:\n                    # Fallback to first result\n                    primary_output = next(iter(integrated_outputs.values()), \"\")\n\n            execution_time = time.time() - start_time\n\n            # Log completion\n            logger.info(\n                f\"Integration phase '{self._phase_name}' completed in {execution_time:.2f}s\",\n                extra={\n                    \"model_count\": len(model_results),\n                    \"phase\": self._phase_name\n                }\n            )\n\n            # Add phase trace if collector is provided\n            if trace_collector:\n                trace_collector.add_phase_trace(\n                    phase_name=self._phase_name,\n                    input_data={\"query\": query, \"prompt\": integration_prompt, \"inputs\": inputs},\n                    output_data={\"outputs\": integrated_outputs, \"primary_output\": primary_output},\n                    execution_time=execution_time,\n                    phase_parameters=self._config\n                )\n\n            # Calculate confidence score (average from all models)\n            confidence_values = []\n            for result in model_results.values():\n                if \"confidence\" in result:\n                    if isinstance(result[\"confidence\"], dict) and \"combined\" in result[\"confidence\"]:\n                        confidence_values.append(result[\"confidence\"][\"combined\"])\n                    elif isinstance(result[\"confidence\"], (float, int)):\n                        confidence_values.append(result[\"confidence\"])\n\n            confidence = sum(confidence_values) / len(confidence_values) if confidence_values else 0.7\n\n            # Return results\n            return {\n                \"output\": primary_output,\n                \"integrated_outputs\": integrated_outputs,\n                \"raw_results\": model_results,\n                \"confidence\": confidence\n            }\n\n        except Exception as e:\n            raise CollaborationError(\n                f\"Integration phase '{self._phase_name}' failed: {str(e)}\"\n            )\n</code></pre>"},{"location":"api/collaboration/#ai_ensemble_suite.collaboration.Integration.execute","title":"<code>execute(query, context, trace_collector=None)</code>  <code>async</code>","text":"<p>Execute the Integration phase.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The user query to process.</p> required <code>context</code> <code>Dict[str, Any]</code> <p>Context information from previous phases.</p> required <code>trace_collector</code> <code>Optional[TraceCollector]</code> <p>Optional trace collector for gathering execution details.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary containing: output: The integrated/refined response. context: Updated context for the next phase.</p> <p>Raises:</p> Type Description <code>CollaborationError</code> <p>If phase execution fails.</p> Source code in <code>src\\ai_ensemble_suite\\collaboration\\integration.py</code> <pre><code>async def execute(\n    self, \n    query: str,\n    context: Dict[str, Any],\n    trace_collector: Optional[TraceCollector] = None\n) -&gt; Dict[str, Any]:\n    \"\"\"Execute the Integration phase.\n\n    Args:\n        query: The user query to process.\n        context: Context information from previous phases.\n        trace_collector: Optional trace collector for gathering execution details.\n\n    Returns:\n        Dictionary containing:\n            output: The integrated/refined response.\n            context: Updated context for the next phase.\n\n    Raises:\n        CollaborationError: If phase execution fails.\n    \"\"\"\n    start_time = time.time()\n\n    try:\n        # Get inputs from previous phases\n        inputs = self._get_inputs_from_context(context)\n\n        # Validate inputs\n        if not inputs and self._input_from:\n            raise CollaborationError(\n                f\"Integration phase '{self._phase_name}' requires inputs from \"\n                f\"previous phases: {', '.join(self._input_from)}\"\n            )\n\n        # Get prompt template or use default integration prompt\n        if not self._prompt_template:\n            logger.warning(\n                f\"No prompt template specified for Integration phase '{self._phase_name}', \"\n                \"using default integration prompt\"\n            )\n            self._prompt_template = \"refinement\"\n\n        # Format prompt with query and inputs\n        try:\n            context = {\"query\": query, **inputs}\n            integration_prompt = self.render_template(self._prompt_template, context)\n        except (ConfigurationError, KeyError) as e:\n            raise CollaborationError(f\"Failed to format integration prompt: {str(e)}\")\n\n        # Run models\n        model_results = await self._run_models(\n            prompt=integration_prompt,\n            trace_collector=trace_collector\n        )\n\n        # Process outputs\n        integrated_outputs = {}\n        for model_id, result in model_results.items():\n            integrated_outputs[model_id] = result.get(\"text\", \"\")\n\n        # Determine the primary integrated output\n        primary_output = \"\"\n        if len(integrated_outputs) == 1:\n            # Single model case\n            primary_output = list(integrated_outputs.values())[0]\n        elif self._model_ids and len(self._model_ids) &gt; 0:\n            # Use the first model as primary\n            primary_model = self._model_ids[0]\n            if primary_model in integrated_outputs:\n                primary_output = integrated_outputs[primary_model]\n            else:\n                # Fallback to first result\n                primary_output = next(iter(integrated_outputs.values()), \"\")\n\n        execution_time = time.time() - start_time\n\n        # Log completion\n        logger.info(\n            f\"Integration phase '{self._phase_name}' completed in {execution_time:.2f}s\",\n            extra={\n                \"model_count\": len(model_results),\n                \"phase\": self._phase_name\n            }\n        )\n\n        # Add phase trace if collector is provided\n        if trace_collector:\n            trace_collector.add_phase_trace(\n                phase_name=self._phase_name,\n                input_data={\"query\": query, \"prompt\": integration_prompt, \"inputs\": inputs},\n                output_data={\"outputs\": integrated_outputs, \"primary_output\": primary_output},\n                execution_time=execution_time,\n                phase_parameters=self._config\n            )\n\n        # Calculate confidence score (average from all models)\n        confidence_values = []\n        for result in model_results.values():\n            if \"confidence\" in result:\n                if isinstance(result[\"confidence\"], dict) and \"combined\" in result[\"confidence\"]:\n                    confidence_values.append(result[\"confidence\"][\"combined\"])\n                elif isinstance(result[\"confidence\"], (float, int)):\n                    confidence_values.append(result[\"confidence\"])\n\n        confidence = sum(confidence_values) / len(confidence_values) if confidence_values else 0.7\n\n        # Return results\n        return {\n            \"output\": primary_output,\n            \"integrated_outputs\": integrated_outputs,\n            \"raw_results\": model_results,\n            \"confidence\": confidence\n        }\n\n    except Exception as e:\n        raise CollaborationError(\n            f\"Integration phase '{self._phase_name}' failed: {str(e)}\"\n        )\n</code></pre>"},{"location":"api/collaboration/#ai_ensemble_suite.collaboration.PerspectiveRotation","title":"<code>PerspectiveRotation</code>","text":"<p>               Bases: <code>BaseCollaborationPhase</code></p> <p>Perspective Rotation collaboration phase.</p> <p>Models iterate on a problem by assuming different perspectives or stakeholder viewpoints in sequence.</p> Source code in <code>src\\ai_ensemble_suite\\collaboration\\perspective_rotation.py</code> <pre><code>class PerspectiveRotation(BaseCollaborationPhase):\n    \"\"\"Perspective Rotation collaboration phase.\n\n    Models iterate on a problem by assuming different perspectives\n    or stakeholder viewpoints in sequence.\n    \"\"\"\n\n    def __init__(\n        self, \n        model_manager: \"ModelManager\",\n        config_manager: \"ConfigManager\",\n        phase_name: str\n    ) -&gt; None:\n        \"\"\"Initialize the perspective rotation phase.\n\n        Args:\n            model_manager: The ModelManager instance.\n            config_manager: The ConfigManager instance.\n            phase_name: The name of the phase for configuration lookup.\n\n        Raises:\n            ConfigurationError: If phase configuration is invalid.\n        \"\"\"\n        super().__init__(model_manager, config_manager, phase_name)\n\n        # Get perspectives to rotate through\n        self._perspectives = self._config.get(\"perspectives\", [])\n\n        # If no perspectives defined, use default set\n        if not self._perspectives:\n            self._perspectives = [\n                \"technical\", \"ethical\", \"practical\", \"creative\", \"critical\"\n            ]\n            logger.debug(\n                f\"No perspectives specified for PerspectiveRotation phase '{phase_name}', \"\n                f\"using defaults: {self._perspectives}\"\n            )\n\n        # Get template name\n        self._perspective_template = self._config.get(\"perspective_template\", \"perspective\")\n\n        # Get synthesis template\n        self._synthesis_template = self._config.get(\"synthesis_template\", \"synthesis\")\n\n        # Get synthesis model (default to first model)\n        self._synthesis_model = self._config.get(\"synthesis_model\")\n        if not self._synthesis_model and self._model_ids:\n            self._synthesis_model = self._model_ids[0]\n\n        logger.debug(\n            f\"Initialized PerspectiveRotation phase '{phase_name}' with \"\n            f\"{len(self._perspectives)} perspectives\"\n        )\n\n    async def execute(\n        self, \n        query: str,\n        context: Dict[str, Any],\n        trace_collector: Optional[TraceCollector] = None\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Execute the Perspective Rotation phase.\n\n        Args:\n            query: The user query to process.\n            context: Context information from previous phases.\n            trace_collector: Optional trace collector for gathering execution details.\n\n        Returns:\n            Dictionary containing:\n                output: The synthesized response.\n                context: Updated context with perspectives and synthesis.\n\n        Raises:\n            CollaborationError: If phase execution fails.\n        \"\"\"\n        start_time = time.time()\n\n        try:\n            # Get inputs from previous phases\n            inputs = self._get_inputs_from_context(context)\n\n            # Step 1: Generate responses from each perspective\n            perspective_responses = await self._generate_perspective_responses(\n                query, context, trace_collector\n            )\n\n            # Step 2: Synthesize the perspectives\n            synthesis = await self._synthesize_perspectives(\n                query, perspective_responses, context, trace_collector\n            )\n\n            execution_time = time.time() - start_time\n\n            # Log completion\n            logger.info(\n                f\"PerspectiveRotation phase '{self._phase_name}' completed in {execution_time:.2f}s\",\n                extra={\"perspectives\": len(self._perspectives)}\n            )\n\n            # Add phase trace if collector is provided\n            if trace_collector:\n                trace_collector.add_phase_trace(\n                    phase_name=self._phase_name,\n                    input_data={\"query\": query, \"context\": context},\n                    output_data={\n                        \"perspective_responses\": perspective_responses,\n                        \"synthesis\": synthesis\n                    },\n                    execution_time=execution_time,\n                    phase_parameters=self._config\n                )\n\n            # Calculate confidence score\n            confidence = synthesis.get(\"confidence\", 0.7)\n\n            # Return results\n            return {\n                \"output\": synthesis.get(\"text\", \"\"),\n                \"perspective_responses\": perspective_responses,\n                \"synthesis\": synthesis,\n                \"perspectives\": self._perspectives,\n                \"confidence\": confidence\n            }\n\n        except Exception as e:\n            raise CollaborationError(\n                f\"PerspectiveRotation phase '{self._phase_name}' failed: {str(e)}\"\n            )\n\n    async def _generate_perspective_responses(\n        self,\n        query: str,\n        context: Dict[str, Any],\n        trace_collector: Optional[TraceCollector] = None\n    ) -&gt; Dict[str, str]:\n        \"\"\"Generate responses from each perspective.\n\n        Args:\n            query: The user query.\n            context: Context information from previous phases.\n            trace_collector: Optional trace collector.\n\n        Returns:\n            Dictionary mapping perspective names to their responses.\n\n        Raises:\n            CollaborationError: If generation fails.\n        \"\"\"\n        logger.debug(f\"Generating responses from {len(self._perspectives)} perspectives\")\n\n        # Get inputs from previous phases\n        inputs = self._get_inputs_from_context(context)\n\n        # Distribute models across perspectives\n        if len(self._model_ids) &gt;= len(self._perspectives):\n            # If we have enough models, assign one per perspective\n            model_assignments = {\n                perspective: self._model_ids[i % len(self._model_ids)]\n                for i, perspective in enumerate(self._perspectives)\n            }\n        else:\n            # Otherwise, use available models in rotation\n            model_assignments = {\n                perspective: self._model_ids[i % len(self._model_ids)]\n                for i, perspective in enumerate(self._perspectives)\n            }\n\n        # Generate response for each perspective\n        perspective_responses = {}\n\n        for perspective in self._perspectives:\n            # Get template (try perspective-specific first, then default)\n            template_name = f\"{perspective}_perspective\"\n            if not self._config_manager.get_template(template_name):\n                template_name = self._perspective_template\n\n            # Format prompt\n            try:\n                context = {\n                    \"query\": query,\n                    \"perspective\": perspective,\n                    **inputs\n                }\n                perspective_prompt = self.render_template(template_name, context)\n            except (ConfigurationError, KeyError) as e:\n                logger.warning(f\"Failed to format prompt for perspective '{perspective}': {str(e)}\")\n                continue\n\n            # Get model for this perspective\n            model_id = model_assignments.get(perspective)\n            if not model_id:\n                logger.warning(f\"No model assigned for perspective '{perspective}'\")\n                continue\n\n            # Run model\n            try:\n                model_result = await self._model_manager.run_inference(\n                    model_id=model_id,\n                    prompt=perspective_prompt\n                )\n\n                # Extract response\n                perspective_responses[perspective] = model_result.get(\"text\", \"\")\n\n                # Add trace if collector is provided\n                if trace_collector:\n                    trace_collector.add_model_trace(\n                        model_id=model_id,\n                        input_prompt=perspective_prompt,\n                        output=model_result,\n                        execution_time=model_result.get(\"generation_time\", 0),\n                        parameters={\"perspective\": perspective}\n                    )\n\n            except Exception as e:\n                logger.warning(f\"Failed to generate response for perspective '{perspective}': {str(e)}\")\n\n        return perspective_responses\n\n    async def _synthesize_perspectives(\n        self,\n        query: str,\n        perspective_responses: Dict[str, str],\n        context: Dict[str, Any],\n        trace_collector: Optional[TraceCollector] = None\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Synthesize the perspectives into a final response.\n\n        Args:\n            query: The user query.\n            perspective_responses: Responses from different perspectives.\n            context: Context information from previous phases.\n            trace_collector: Optional trace collector.\n\n        Returns:\n            Dictionary containing synthesis results.\n\n        Raises:\n            CollaborationError: If synthesis fails.\n        \"\"\"\n        logger.debug(\"Synthesizing perspectives\")\n\n        # Check if synthesis model is available\n        if not self._synthesis_model:\n            raise CollaborationError(\"No synthesis model specified\")\n\n        # Format perspectives for synthesis\n        formatted_perspectives = \"\"\n        for perspective, response in perspective_responses.items():\n            formatted_perspectives += f\"\\n\\n## {perspective.capitalize()} Perspective\\n\\n{response}\"\n\n        # Format synthesis prompt\n        try:\n            context = {\n                \"query\": query,\n                \"perspectives\": formatted_perspectives\n            }\n            synthesis_prompt = self.render_template(self._synthesis_template, context)\n        except (ConfigurationError, KeyError) as e:\n            raise CollaborationError(f\"Failed to format synthesis prompt: {str(e)}\")\n\n        # Run synthesis model\n        try:\n            synthesis_result = await self._model_manager.run_inference(\n                model_id=self._synthesis_model,\n                prompt=synthesis_prompt\n            )\n\n            # Add trace if collector is provided\n            if trace_collector:\n                trace_collector.add_model_trace(\n                    model_id=self._synthesis_model,\n                    input_prompt=synthesis_prompt,\n                    output=synthesis_result,\n                    execution_time=synthesis_result.get(\"generation_time\", 0),\n                    parameters={\"role\": \"synthesizer\"}\n                )\n\n            return synthesis_result\n\n        except Exception as e:\n            raise CollaborationError(f\"Failed to synthesize perspectives: {str(e)}\")\n</code></pre>"},{"location":"api/collaboration/#ai_ensemble_suite.collaboration.PerspectiveRotation.__init__","title":"<code>__init__(model_manager, config_manager, phase_name)</code>","text":"<p>Initialize the perspective rotation phase.</p> <p>Parameters:</p> Name Type Description Default <code>model_manager</code> <code>ModelManager</code> <p>The ModelManager instance.</p> required <code>config_manager</code> <code>ConfigManager</code> <p>The ConfigManager instance.</p> required <code>phase_name</code> <code>str</code> <p>The name of the phase for configuration lookup.</p> required <p>Raises:</p> Type Description <code>ConfigurationError</code> <p>If phase configuration is invalid.</p> Source code in <code>src\\ai_ensemble_suite\\collaboration\\perspective_rotation.py</code> <pre><code>def __init__(\n    self, \n    model_manager: \"ModelManager\",\n    config_manager: \"ConfigManager\",\n    phase_name: str\n) -&gt; None:\n    \"\"\"Initialize the perspective rotation phase.\n\n    Args:\n        model_manager: The ModelManager instance.\n        config_manager: The ConfigManager instance.\n        phase_name: The name of the phase for configuration lookup.\n\n    Raises:\n        ConfigurationError: If phase configuration is invalid.\n    \"\"\"\n    super().__init__(model_manager, config_manager, phase_name)\n\n    # Get perspectives to rotate through\n    self._perspectives = self._config.get(\"perspectives\", [])\n\n    # If no perspectives defined, use default set\n    if not self._perspectives:\n        self._perspectives = [\n            \"technical\", \"ethical\", \"practical\", \"creative\", \"critical\"\n        ]\n        logger.debug(\n            f\"No perspectives specified for PerspectiveRotation phase '{phase_name}', \"\n            f\"using defaults: {self._perspectives}\"\n        )\n\n    # Get template name\n    self._perspective_template = self._config.get(\"perspective_template\", \"perspective\")\n\n    # Get synthesis template\n    self._synthesis_template = self._config.get(\"synthesis_template\", \"synthesis\")\n\n    # Get synthesis model (default to first model)\n    self._synthesis_model = self._config.get(\"synthesis_model\")\n    if not self._synthesis_model and self._model_ids:\n        self._synthesis_model = self._model_ids[0]\n\n    logger.debug(\n        f\"Initialized PerspectiveRotation phase '{phase_name}' with \"\n        f\"{len(self._perspectives)} perspectives\"\n    )\n</code></pre>"},{"location":"api/collaboration/#ai_ensemble_suite.collaboration.PerspectiveRotation.execute","title":"<code>execute(query, context, trace_collector=None)</code>  <code>async</code>","text":"<p>Execute the Perspective Rotation phase.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The user query to process.</p> required <code>context</code> <code>Dict[str, Any]</code> <p>Context information from previous phases.</p> required <code>trace_collector</code> <code>Optional[TraceCollector]</code> <p>Optional trace collector for gathering execution details.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary containing: output: The synthesized response. context: Updated context with perspectives and synthesis.</p> <p>Raises:</p> Type Description <code>CollaborationError</code> <p>If phase execution fails.</p> Source code in <code>src\\ai_ensemble_suite\\collaboration\\perspective_rotation.py</code> <pre><code>async def execute(\n    self, \n    query: str,\n    context: Dict[str, Any],\n    trace_collector: Optional[TraceCollector] = None\n) -&gt; Dict[str, Any]:\n    \"\"\"Execute the Perspective Rotation phase.\n\n    Args:\n        query: The user query to process.\n        context: Context information from previous phases.\n        trace_collector: Optional trace collector for gathering execution details.\n\n    Returns:\n        Dictionary containing:\n            output: The synthesized response.\n            context: Updated context with perspectives and synthesis.\n\n    Raises:\n        CollaborationError: If phase execution fails.\n    \"\"\"\n    start_time = time.time()\n\n    try:\n        # Get inputs from previous phases\n        inputs = self._get_inputs_from_context(context)\n\n        # Step 1: Generate responses from each perspective\n        perspective_responses = await self._generate_perspective_responses(\n            query, context, trace_collector\n        )\n\n        # Step 2: Synthesize the perspectives\n        synthesis = await self._synthesize_perspectives(\n            query, perspective_responses, context, trace_collector\n        )\n\n        execution_time = time.time() - start_time\n\n        # Log completion\n        logger.info(\n            f\"PerspectiveRotation phase '{self._phase_name}' completed in {execution_time:.2f}s\",\n            extra={\"perspectives\": len(self._perspectives)}\n        )\n\n        # Add phase trace if collector is provided\n        if trace_collector:\n            trace_collector.add_phase_trace(\n                phase_name=self._phase_name,\n                input_data={\"query\": query, \"context\": context},\n                output_data={\n                    \"perspective_responses\": perspective_responses,\n                    \"synthesis\": synthesis\n                },\n                execution_time=execution_time,\n                phase_parameters=self._config\n            )\n\n        # Calculate confidence score\n        confidence = synthesis.get(\"confidence\", 0.7)\n\n        # Return results\n        return {\n            \"output\": synthesis.get(\"text\", \"\"),\n            \"perspective_responses\": perspective_responses,\n            \"synthesis\": synthesis,\n            \"perspectives\": self._perspectives,\n            \"confidence\": confidence\n        }\n\n    except Exception as e:\n        raise CollaborationError(\n            f\"PerspectiveRotation phase '{self._phase_name}' failed: {str(e)}\"\n        )\n</code></pre>"},{"location":"api/collaboration/#ai_ensemble_suite.collaboration.RoleBasedDebate","title":"<code>RoleBasedDebate</code>","text":"<p>               Bases: <code>BaseDebate</code></p> <p>Role-Based Debate pattern.</p> <p>Models interact according to assigned specialized roles such as Critic, Advocate, Synthesizer, and Fact-Checker.</p> Source code in <code>src\\ai_ensemble_suite\\collaboration\\structured_debate\\role_based.py</code> <pre><code>class RoleBasedDebate(BaseDebate):\n    \"\"\"Role-Based Debate pattern.\n\n    Models interact according to assigned specialized roles such as\n    Critic, Advocate, Synthesizer, and Fact-Checker.\n    \"\"\"\n\n    def __init__(\n        self, \n        model_manager: \"ModelManager\",\n        config_manager: \"ConfigManager\",\n        phase_name: str\n    ) -&gt; None:\n        \"\"\"Initialize the role-based debate phase.\n\n        Args:\n            model_manager: The ModelManager instance.\n            config_manager: The ConfigManager instance.\n            phase_name: The name of the phase for configuration lookup.\n\n        Raises:\n            ConfigurationError: If phase configuration is invalid.\n        \"\"\"\n        super().__init__(model_manager, config_manager, phase_name)\n\n        # Get role assignments\n        self._role_assignments = self._config.get(\"role_assignments\", {})\n\n        # If no explicit role assignments, try to use model roles\n        if not self._role_assignments:\n            self._use_model_roles = True\n            logger.debug(\n                f\"No explicit role assignments for phase '{phase_name}', \"\n                \"will use model roles\"\n            )\n        else:\n            self._use_model_roles = False\n            logger.debug(\n                f\"Using explicit role assignments for phase '{phase_name}': \"\n                f\"{self._role_assignments}\"\n            )\n\n        # Get debate turn order (optional)\n        self._turn_order = self._config.get(\"turn_order\", [])\n\n        # Get standard roles if not explicitly configured\n        if not self._turn_order:\n            self._turn_order = [\"critic\", \"advocate\", \"synthesizer\", \"fact_checker\"]\n            logger.debug(f\"Using default turn order for phase '{phase_name}'\")\n\n        # Get role-specific prompt templates\n        self._role_templates = self._config.get(\"role_templates\", {})\n\n    async def _execute_debate_pattern(\n        self,\n        query: str,\n        initial_response: str,\n        inputs: Dict[str, Any],\n        trace_collector: Optional[TraceCollector] = None\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Execute the role-based debate pattern.\n\n        Args:\n            query: The user query to process.\n            initial_response: The initial response to debate.\n            inputs: Inputs from previous phases.\n            trace_collector: Optional trace collector.\n\n        Returns:\n            Dictionary containing debate results.\n\n        Raises:\n            CollaborationError: If execution fails.\n        \"\"\"\n        # Map models to roles\n        role_to_models = await self._map_roles_to_models()\n\n        # If no roles mapped to models, raise an error\n        if not role_to_models:\n            raise CollaborationError(\n                f\"Role-based debate '{self._phase_name}' has no valid role-model mappings\"\n            )\n\n        # Start with the initial context\n        debate_context = {\n            \"query\": query,\n            \"initial_response\": initial_response,\n            \"exchanges\": []\n        }\n\n        # Add inputs from previous phases\n        debate_context.update(inputs)\n\n        # Execute debate rounds\n        for round_num in range(self._rounds):\n            logger.debug(f\"Starting debate round {round_num + 1}/{self._rounds}\")\n\n            # Process each role in turn order\n            for role in self._turn_order:\n                # Skip if no models assigned to this role\n                if role not in role_to_models or not role_to_models[role]:\n                    logger.debug(f\"Skipping role '{role}' (no models assigned)\")\n                    continue\n\n                # Get the template for this role\n                template_name = self._get_template_for_role(role)\n                if not template_name:\n                    logger.warning(\n                        f\"No template found for role '{role}', using default template\"\n                    )\n                    template_name = self._prompt_template\n\n                # If still no template, skip this role\n                if not template_name:\n                    logger.warning(f\"Skipping role '{role}' (no template available)\")\n                    continue\n\n                # Format prompt for this role\n                try:\n                    role_prompt = self.render_template(template_name, debate_context)\n                except (ConfigurationError, KeyError) as e:\n                    logger.warning(f\"Failed to format prompt for role '{role}': {str(e)}\")\n                    continue\n\n                # Run models for this role\n                models_for_role = role_to_models[role]\n                role_results = await self._run_models(\n                    prompt=role_prompt,\n                    model_ids=models_for_role,\n                    trace_collector=trace_collector\n                )\n\n                # Process outputs\n                responses = {}\n                for model_id, result in role_results.items():\n                    responses[model_id] = result.get(\"text\", \"\")\n\n                # Combine responses if multiple models for this role\n                if len(responses) &gt; 1:\n                    combined_response = f\"# Combined {role.capitalize()} Perspective\\n\\n\"\n                    for model_id, response in responses.items():\n                        combined_response += f\"## From {model_id}\\n\\n{response}\\n\\n\"\n                    role_response = combined_response\n                elif responses:\n                    # Just use the single response\n                    role_response = list(responses.values())[0]\n                else:\n                    role_response = \"\"\n\n                # Add to exchanges\n                if role_response:\n                    debate_context[\"exchanges\"].append({\n                        \"role\": role,\n                        \"content\": role_response,\n                        \"round\": round_num + 1\n                    })\n\n                    # Also add role-specific key in context\n                    debate_context[f\"{role}_perspective\"] = role_response\n\n                    # Update for the next role\n                    debate_context[\"latest_exchange\"] = role_response\n\n        # Determine the final output\n        final_perspective = \"\"\n\n        # Prioritize the synthesizer role if present\n        if \"synthesizer_perspective\" in debate_context:\n            final_perspective = debate_context[\"synthesizer_perspective\"]\n        # Otherwise use the last exchange\n        elif debate_context[\"exchanges\"]:\n            final_perspective = debate_context[\"exchanges\"][-1][\"content\"]\n\n        # Calculate confidence (average from all model results across all roles)\n        all_confidences = []\n        for role, role_models in role_to_models.items():\n            for model_id in role_models:\n                # Find all results for this model\n                for exchange in debate_context.get(\"exchanges\", []):\n                    if exchange[\"role\"] == role:\n                        # Look for confidence in the raw results\n                        for round_results in exchange.get(\"raw_results\", {}).values():\n                            if isinstance(round_results, dict) and \"confidence\" in round_results:\n                                if isinstance(round_results[\"confidence\"], dict) and \"combined\" in round_results[\"confidence\"]:\n                                    all_confidences.append(round_results[\"confidence\"][\"combined\"])\n                                elif isinstance(round_results[\"confidence\"], (float, int)):\n                                    all_confidences.append(round_results[\"confidence\"])\n\n        confidence = sum(all_confidences) / len(all_confidences) if all_confidences else 0.7\n\n        # Return results\n        return {\n            \"output\": final_perspective,\n            \"final_perspective\": final_perspective,\n            \"exchanges\": debate_context[\"exchanges\"],\n            \"initial_response\": initial_response,\n            \"role_perspectives\": {\n                role: debate_context.get(f\"{role}_perspective\", \"\")\n                for role in role_to_models.keys()\n            },\n            \"confidence\": confidence\n        }\n\n    async def _map_roles_to_models(self) -&gt; Dict[str, List[str]]:\n        \"\"\"Map roles to model IDs based on configuration.\n\n        Returns:\n            Dictionary mapping role names to lists of model IDs.\n\n        Raises:\n            CollaborationError: If role mapping fails.\n        \"\"\"\n        role_to_models: Dict[str, List[str]] = {}\n\n        # Check if using explicit role assignments\n        if not self._use_model_roles:\n            # Use explicit assignments from configuration\n            for role, model_ids in self._role_assignments.items():\n                if isinstance(model_ids, str):\n                    # Single model ID\n                    role_to_models[role] = [model_ids]\n                elif isinstance(model_ids, list):\n                    # List of model IDs\n                    role_to_models[role] = model_ids\n\n            return role_to_models\n\n        # Otherwise use model roles\n        for model_id in self._model_ids:\n            try:\n                model = self._model_manager.get_model(model_id)\n                role = model.get_role()\n\n                if role:\n                    if role not in role_to_models:\n                        role_to_models[role] = []\n                    role_to_models[role].append(model_id)\n            except Exception as e:\n                logger.warning(f\"Failed to get role for model '{model_id}': {str(e)}\")\n\n        return role_to_models\n\n    def _get_template_for_role(self, role: str) -&gt; Optional[str]:\n        \"\"\"Get the template name for a specific role.\n\n        Args:\n            role: The role to get template for.\n\n        Returns:\n            Template name or None if not found.\n        \"\"\"\n        # First check role-specific templates in this phase's config\n        if self._role_templates and role in self._role_templates:\n            return self._role_templates[role]\n\n        # Then check for a template with the role name\n        role_template_name = f\"{role}_template\"\n        if role_template_name in self._config:\n            return self._config[role_template_name]\n\n        # Then look for a standard template with the role name pattern\n        standard_template = f\"role_{role}\"\n        template = self._config_manager.get_template(standard_template)\n        if template is not None:\n            return standard_template\n\n        # Fall back to the default prompt template\n        return self._prompt_template\n</code></pre>"},{"location":"api/collaboration/#ai_ensemble_suite.collaboration.RoleBasedDebate.__init__","title":"<code>__init__(model_manager, config_manager, phase_name)</code>","text":"<p>Initialize the role-based debate phase.</p> <p>Parameters:</p> Name Type Description Default <code>model_manager</code> <code>ModelManager</code> <p>The ModelManager instance.</p> required <code>config_manager</code> <code>ConfigManager</code> <p>The ConfigManager instance.</p> required <code>phase_name</code> <code>str</code> <p>The name of the phase for configuration lookup.</p> required <p>Raises:</p> Type Description <code>ConfigurationError</code> <p>If phase configuration is invalid.</p> Source code in <code>src\\ai_ensemble_suite\\collaboration\\structured_debate\\role_based.py</code> <pre><code>def __init__(\n    self, \n    model_manager: \"ModelManager\",\n    config_manager: \"ConfigManager\",\n    phase_name: str\n) -&gt; None:\n    \"\"\"Initialize the role-based debate phase.\n\n    Args:\n        model_manager: The ModelManager instance.\n        config_manager: The ConfigManager instance.\n        phase_name: The name of the phase for configuration lookup.\n\n    Raises:\n        ConfigurationError: If phase configuration is invalid.\n    \"\"\"\n    super().__init__(model_manager, config_manager, phase_name)\n\n    # Get role assignments\n    self._role_assignments = self._config.get(\"role_assignments\", {})\n\n    # If no explicit role assignments, try to use model roles\n    if not self._role_assignments:\n        self._use_model_roles = True\n        logger.debug(\n            f\"No explicit role assignments for phase '{phase_name}', \"\n            \"will use model roles\"\n        )\n    else:\n        self._use_model_roles = False\n        logger.debug(\n            f\"Using explicit role assignments for phase '{phase_name}': \"\n            f\"{self._role_assignments}\"\n        )\n\n    # Get debate turn order (optional)\n    self._turn_order = self._config.get(\"turn_order\", [])\n\n    # Get standard roles if not explicitly configured\n    if not self._turn_order:\n        self._turn_order = [\"critic\", \"advocate\", \"synthesizer\", \"fact_checker\"]\n        logger.debug(f\"Using default turn order for phase '{phase_name}'\")\n\n    # Get role-specific prompt templates\n    self._role_templates = self._config.get(\"role_templates\", {})\n</code></pre>"},{"location":"api/collaboration/#ai_ensemble_suite.collaboration.RoleBasedWorkflow","title":"<code>RoleBasedWorkflow</code>","text":"<p>               Bases: <code>BaseCollaborationPhase</code></p> <p>Role-Based Workflow collaboration phase.</p> <p>Models function in specialized roles like researcher, analyst, and writer to create a structured workflow for complex tasks.</p> Source code in <code>src\\ai_ensemble_suite\\collaboration\\role_based_workflow.py</code> <pre><code>class RoleBasedWorkflow(BaseCollaborationPhase):\n    \"\"\"Role-Based Workflow collaboration phase.\n\n    Models function in specialized roles like researcher, analyst,\n    and writer to create a structured workflow for complex tasks.\n    \"\"\"\n\n    def __init__(\n        self, \n        model_manager: \"ModelManager\",\n        config_manager: \"ConfigManager\",\n        phase_name: str\n    ) -&gt; None:\n        \"\"\"Initialize the role-based workflow phase.\n\n        Args:\n            model_manager: The ModelManager instance.\n            config_manager: The ConfigManager instance.\n            phase_name: The name of the phase for configuration lookup.\n\n        Raises:\n            ConfigurationError: If phase configuration is invalid.\n        \"\"\"\n        super().__init__(model_manager, config_manager, phase_name)\n\n        # Get workflow steps\n        self._workflow_steps = self._config.get(\"workflow_steps\", [])\n\n        # If no workflow steps defined, use default workflow\n        if not self._workflow_steps:\n            self._workflow_steps = [\n                {\n                    \"role\": \"researcher\",\n                    \"task\": \"research\",\n                    \"description\": \"Gather comprehensive information on the topic\",\n                    \"template\": \"role_researcher\"\n                },\n                {\n                    \"role\": \"analyst\",\n                    \"task\": \"analyze\",\n                    \"description\": \"Analyze the research and identify key insights\",\n                    \"template\": \"role_analyst\"\n                },\n                {\n                    \"role\": \"writer\",\n                    \"task\": \"write\",\n                    \"description\": \"Create a well-structured response based on the analysis\",\n                    \"template\": \"role_writer\"\n                },\n                {\n                    \"role\": \"editor\",\n                    \"task\": \"edit\",\n                    \"description\": \"Review and refine the written content for clarity and accuracy\",\n                    \"template\": \"role_editor\"\n                }\n            ]\n            logger.debug(\n                f\"No workflow steps defined for phase '{phase_name}', using default workflow\"\n            )\n\n        # Validate workflow steps\n        for i, step in enumerate(self._workflow_steps):\n            if \"role\" not in step:\n                step[\"role\"] = f\"role_{i+1}\"\n                logger.warning(f\"Step {i+1} missing role, using '{step['role']}'\")\n\n            if \"task\" not in step:\n                step[\"task\"] = f\"task_{i+1}\"\n                logger.warning(f\"Step {i+1} missing task, using '{step['task']}'\")\n\n            if \"template\" not in step:\n                step[\"template\"] = f\"role_{step['role']}\"\n                logger.warning(f\"Step {i+1} missing template, using '{step['template']}'\")\n\n        logger.debug(\n            f\"Initialized RoleBasedWorkflow phase '{phase_name}' with \"\n            f\"{len(self._workflow_steps)} workflow steps\"\n        )\n\n    async def execute(\n            self,\n            query: str,\n            context: Dict[str, Any],\n            trace_collector: Optional[TraceCollector] = None\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Execute the Role-Based Workflow phase.\"\"\"\n        start_time = time.time()\n\n        try:\n            # Get inputs from previous phases\n            inputs = self._get_inputs_from_context(context)\n\n            # Map models to roles\n            role_assignments = self._assign_models_to_roles()\n\n            # Initialize workflow context\n            workflow_context = {\n                \"query\": query,\n                \"steps\": {},\n                \"final_output\": \"\",\n                **inputs\n            }\n\n            # Execute workflow steps in sequence\n            for step_idx, step in enumerate(self._workflow_steps):\n                role = step[\"role\"]\n                task = step[\"task\"]\n                template_name = step[\"template\"]\n\n                logger.debug(f\"Executing workflow step {step_idx + 1}: {role} - {task}\")\n\n                # Get model for this role\n                model_id = role_assignments.get(role)\n                if not model_id:\n                    logger.warning(f\"No model assigned for role '{role}', skipping step\")\n                    continue\n\n                # Format prompt for this step - IMPROVED VARIABLE HANDLING\n                try:\n                    # Start with a clean set of variables for this step\n                    prompt_vars = {}\n\n                    # Always include the query\n                    prompt_vars[\"query\"] = query\n\n                    # Include inputs from previous phases\n                    prompt_vars.update(inputs)\n\n                    # Handle input_from parameter for this step\n                    input_from_list = step.get(\"input_from\", [])\n                    if isinstance(input_from_list, str):\n                        input_from_list = [input_from_list]\n\n                    # If no input_from specified but not first step, use all previous steps\n                    if not input_from_list and step_idx &gt; 0:\n                        input_from_list = [prev_step[\"task\"] for prev_step in self._workflow_steps[:step_idx]]\n\n                    # Add the specified inputs from workflow context AS DIRECT VARIABLES\n                    for input_task in input_from_list:\n                        if input_task in workflow_context[\"steps\"]:\n                            prompt_vars[input_task] = workflow_context[\"steps\"][input_task]\n                        else:\n                            logger.warning(f\"Step '{task}' requires input from '{input_task}', but it is not available\")\n\n                    # Format the prompt with the prepared variables\n                    step_prompt = self.render_template(template_name, prompt_vars)\n\n                except (ConfigurationError, KeyError) as e:\n                    logger.warning(f\"Failed to format prompt for step '{task}': {str(e)}\")\n                    continue\n\n                # Run model with the formatted prompt\n                try:\n                    step_result = await self._model_manager.run_inference(\n                        model_id=model_id,\n                        prompt=step_prompt\n                    )\n\n                    # Add trace if collector is provided\n                    if trace_collector:\n                        trace_collector.add_model_trace(\n                            model_id=model_id,\n                            input_prompt=step_prompt,\n                            output=step_result,\n                            execution_time=step_result.get(\"generation_time\", 0),\n                            parameters={\"role\": role, \"task\": task}\n                        )\n\n                    # Extract output\n                    step_output = step_result.get(\"text\", \"\")\n\n                    # Store step results in workflow context by task name\n                    workflow_context[\"steps\"][task] = step_output\n\n                    # Also store by role name for easier reference\n                    workflow_context[role] = step_output\n\n                    # Update final output (always use the latest step)\n                    workflow_context[\"final_output\"] = step_output\n\n                except Exception as e:\n                    logger.warning(f\"Failed to execute step '{task}': {str(e)}\")\n\n            execution_time = time.time() - start_time\n\n            # Log completion\n            logger.info(\n                f\"RoleBasedWorkflow phase '{self._phase_name}' completed in {execution_time:.2f}s\",\n                extra={\"steps\": len(self._workflow_steps)}\n            )\n\n            # Add phase trace if collector is provided\n            if trace_collector:\n                trace_collector.add_phase_trace(\n                    phase_name=self._phase_name,\n                    input_data={\"query\": query, \"inputs\": inputs},\n                    output_data=workflow_context,\n                    execution_time=execution_time,\n                    phase_parameters=self._config\n                )\n\n            # Get final output\n            final_output = workflow_context.get(\"final_output\", \"\")\n            if not final_output and workflow_context[\"steps\"]:\n                # Use the last step's output\n                final_step = list(workflow_context[\"steps\"].keys())[-1]\n                final_output = workflow_context[\"steps\"][final_step]\n\n            # Calculate confidence (average from step results that have confidence)\n            confidence_values = []\n            for step_name, step_result in workflow_context.get(\"steps\", {}).items():\n                if isinstance(step_result, dict) and \"confidence\" in step_result:\n                    if isinstance(step_result[\"confidence\"], dict) and \"combined\" in step_result[\"confidence\"]:\n                        confidence_values.append(step_result[\"confidence\"][\"combined\"])\n                    elif isinstance(step_result[\"confidence\"], (float, int)):\n                        confidence_values.append(step_result[\"confidence\"])\n\n            confidence = sum(confidence_values) / len(confidence_values) if confidence_values else 0.7\n\n            # Return results\n            return {\n                \"output\": final_output,\n                \"workflow_steps\": workflow_context[\"steps\"],\n                \"confidence\": confidence\n            }\n\n        except Exception as e:\n            raise CollaborationError(\n                f\"RoleBasedWorkflow phase '{self._phase_name}' failed: {str(e)}\"\n            )\n\n    def _assign_models_to_roles(self) -&gt; Dict[str, str]:\n        \"\"\"Assign models to roles in the workflow.\n\n        Returns:\n            Dictionary mapping roles to model IDs.\n        \"\"\"\n        # Check for explicit role assignments in configuration\n        role_assignments = self._config.get(\"role_assignments\", {})\n\n        # If explicit assignments provided, use them\n        if role_assignments:\n            return role_assignments\n\n        # Otherwise, try to use model roles\n        role_to_model: Dict[str, str] = {}\n\n        # First try to match roles based on model metadata\n        for model_id in self._model_ids:\n            try:\n                model = self._model_manager.get_model(model_id)\n                model_role = model.get_role()\n\n                if model_role:\n                    # Check if this role is used in the workflow\n                    for step in self._workflow_steps:\n                        if step[\"role\"] == model_role and model_role not in role_to_model:\n                            role_to_model[model_role] = model_id\n                            break\n            except Exception:\n                pass\n\n        # For any unassigned roles, distribute remaining models\n        available_models = [m for m in self._model_ids if m not in role_to_model.values()]\n        model_idx = 0\n\n        for step in self._workflow_steps:\n            role = step[\"role\"]\n\n            if role not in role_to_model and available_models:\n                role_to_model[role] = available_models[model_idx % len(available_models)]\n                model_idx += 1\n\n        # Log assignments\n        logger.debug(f\"Role assignments: {role_to_model}\")\n\n        return role_to_model\n</code></pre>"},{"location":"api/collaboration/#ai_ensemble_suite.collaboration.RoleBasedWorkflow.__init__","title":"<code>__init__(model_manager, config_manager, phase_name)</code>","text":"<p>Initialize the role-based workflow phase.</p> <p>Parameters:</p> Name Type Description Default <code>model_manager</code> <code>ModelManager</code> <p>The ModelManager instance.</p> required <code>config_manager</code> <code>ConfigManager</code> <p>The ConfigManager instance.</p> required <code>phase_name</code> <code>str</code> <p>The name of the phase for configuration lookup.</p> required <p>Raises:</p> Type Description <code>ConfigurationError</code> <p>If phase configuration is invalid.</p> Source code in <code>src\\ai_ensemble_suite\\collaboration\\role_based_workflow.py</code> <pre><code>def __init__(\n    self, \n    model_manager: \"ModelManager\",\n    config_manager: \"ConfigManager\",\n    phase_name: str\n) -&gt; None:\n    \"\"\"Initialize the role-based workflow phase.\n\n    Args:\n        model_manager: The ModelManager instance.\n        config_manager: The ConfigManager instance.\n        phase_name: The name of the phase for configuration lookup.\n\n    Raises:\n        ConfigurationError: If phase configuration is invalid.\n    \"\"\"\n    super().__init__(model_manager, config_manager, phase_name)\n\n    # Get workflow steps\n    self._workflow_steps = self._config.get(\"workflow_steps\", [])\n\n    # If no workflow steps defined, use default workflow\n    if not self._workflow_steps:\n        self._workflow_steps = [\n            {\n                \"role\": \"researcher\",\n                \"task\": \"research\",\n                \"description\": \"Gather comprehensive information on the topic\",\n                \"template\": \"role_researcher\"\n            },\n            {\n                \"role\": \"analyst\",\n                \"task\": \"analyze\",\n                \"description\": \"Analyze the research and identify key insights\",\n                \"template\": \"role_analyst\"\n            },\n            {\n                \"role\": \"writer\",\n                \"task\": \"write\",\n                \"description\": \"Create a well-structured response based on the analysis\",\n                \"template\": \"role_writer\"\n            },\n            {\n                \"role\": \"editor\",\n                \"task\": \"edit\",\n                \"description\": \"Review and refine the written content for clarity and accuracy\",\n                \"template\": \"role_editor\"\n            }\n        ]\n        logger.debug(\n            f\"No workflow steps defined for phase '{phase_name}', using default workflow\"\n        )\n\n    # Validate workflow steps\n    for i, step in enumerate(self._workflow_steps):\n        if \"role\" not in step:\n            step[\"role\"] = f\"role_{i+1}\"\n            logger.warning(f\"Step {i+1} missing role, using '{step['role']}'\")\n\n        if \"task\" not in step:\n            step[\"task\"] = f\"task_{i+1}\"\n            logger.warning(f\"Step {i+1} missing task, using '{step['task']}'\")\n\n        if \"template\" not in step:\n            step[\"template\"] = f\"role_{step['role']}\"\n            logger.warning(f\"Step {i+1} missing template, using '{step['template']}'\")\n\n    logger.debug(\n        f\"Initialized RoleBasedWorkflow phase '{phase_name}' with \"\n        f\"{len(self._workflow_steps)} workflow steps\"\n    )\n</code></pre>"},{"location":"api/collaboration/#ai_ensemble_suite.collaboration.RoleBasedWorkflow.execute","title":"<code>execute(query, context, trace_collector=None)</code>  <code>async</code>","text":"<p>Execute the Role-Based Workflow phase.</p> Source code in <code>src\\ai_ensemble_suite\\collaboration\\role_based_workflow.py</code> <pre><code>async def execute(\n        self,\n        query: str,\n        context: Dict[str, Any],\n        trace_collector: Optional[TraceCollector] = None\n) -&gt; Dict[str, Any]:\n    \"\"\"Execute the Role-Based Workflow phase.\"\"\"\n    start_time = time.time()\n\n    try:\n        # Get inputs from previous phases\n        inputs = self._get_inputs_from_context(context)\n\n        # Map models to roles\n        role_assignments = self._assign_models_to_roles()\n\n        # Initialize workflow context\n        workflow_context = {\n            \"query\": query,\n            \"steps\": {},\n            \"final_output\": \"\",\n            **inputs\n        }\n\n        # Execute workflow steps in sequence\n        for step_idx, step in enumerate(self._workflow_steps):\n            role = step[\"role\"]\n            task = step[\"task\"]\n            template_name = step[\"template\"]\n\n            logger.debug(f\"Executing workflow step {step_idx + 1}: {role} - {task}\")\n\n            # Get model for this role\n            model_id = role_assignments.get(role)\n            if not model_id:\n                logger.warning(f\"No model assigned for role '{role}', skipping step\")\n                continue\n\n            # Format prompt for this step - IMPROVED VARIABLE HANDLING\n            try:\n                # Start with a clean set of variables for this step\n                prompt_vars = {}\n\n                # Always include the query\n                prompt_vars[\"query\"] = query\n\n                # Include inputs from previous phases\n                prompt_vars.update(inputs)\n\n                # Handle input_from parameter for this step\n                input_from_list = step.get(\"input_from\", [])\n                if isinstance(input_from_list, str):\n                    input_from_list = [input_from_list]\n\n                # If no input_from specified but not first step, use all previous steps\n                if not input_from_list and step_idx &gt; 0:\n                    input_from_list = [prev_step[\"task\"] for prev_step in self._workflow_steps[:step_idx]]\n\n                # Add the specified inputs from workflow context AS DIRECT VARIABLES\n                for input_task in input_from_list:\n                    if input_task in workflow_context[\"steps\"]:\n                        prompt_vars[input_task] = workflow_context[\"steps\"][input_task]\n                    else:\n                        logger.warning(f\"Step '{task}' requires input from '{input_task}', but it is not available\")\n\n                # Format the prompt with the prepared variables\n                step_prompt = self.render_template(template_name, prompt_vars)\n\n            except (ConfigurationError, KeyError) as e:\n                logger.warning(f\"Failed to format prompt for step '{task}': {str(e)}\")\n                continue\n\n            # Run model with the formatted prompt\n            try:\n                step_result = await self._model_manager.run_inference(\n                    model_id=model_id,\n                    prompt=step_prompt\n                )\n\n                # Add trace if collector is provided\n                if trace_collector:\n                    trace_collector.add_model_trace(\n                        model_id=model_id,\n                        input_prompt=step_prompt,\n                        output=step_result,\n                        execution_time=step_result.get(\"generation_time\", 0),\n                        parameters={\"role\": role, \"task\": task}\n                    )\n\n                # Extract output\n                step_output = step_result.get(\"text\", \"\")\n\n                # Store step results in workflow context by task name\n                workflow_context[\"steps\"][task] = step_output\n\n                # Also store by role name for easier reference\n                workflow_context[role] = step_output\n\n                # Update final output (always use the latest step)\n                workflow_context[\"final_output\"] = step_output\n\n            except Exception as e:\n                logger.warning(f\"Failed to execute step '{task}': {str(e)}\")\n\n        execution_time = time.time() - start_time\n\n        # Log completion\n        logger.info(\n            f\"RoleBasedWorkflow phase '{self._phase_name}' completed in {execution_time:.2f}s\",\n            extra={\"steps\": len(self._workflow_steps)}\n        )\n\n        # Add phase trace if collector is provided\n        if trace_collector:\n            trace_collector.add_phase_trace(\n                phase_name=self._phase_name,\n                input_data={\"query\": query, \"inputs\": inputs},\n                output_data=workflow_context,\n                execution_time=execution_time,\n                phase_parameters=self._config\n            )\n\n        # Get final output\n        final_output = workflow_context.get(\"final_output\", \"\")\n        if not final_output and workflow_context[\"steps\"]:\n            # Use the last step's output\n            final_step = list(workflow_context[\"steps\"].keys())[-1]\n            final_output = workflow_context[\"steps\"][final_step]\n\n        # Calculate confidence (average from step results that have confidence)\n        confidence_values = []\n        for step_name, step_result in workflow_context.get(\"steps\", {}).items():\n            if isinstance(step_result, dict) and \"confidence\" in step_result:\n                if isinstance(step_result[\"confidence\"], dict) and \"combined\" in step_result[\"confidence\"]:\n                    confidence_values.append(step_result[\"confidence\"][\"combined\"])\n                elif isinstance(step_result[\"confidence\"], (float, int)):\n                    confidence_values.append(step_result[\"confidence\"])\n\n        confidence = sum(confidence_values) / len(confidence_values) if confidence_values else 0.7\n\n        # Return results\n        return {\n            \"output\": final_output,\n            \"workflow_steps\": workflow_context[\"steps\"],\n            \"confidence\": confidence\n        }\n\n    except Exception as e:\n        raise CollaborationError(\n            f\"RoleBasedWorkflow phase '{self._phase_name}' failed: {str(e)}\"\n        )\n</code></pre>"},{"location":"api/collaboration/#ai_ensemble_suite.collaboration.StackedGeneralization","title":"<code>StackedGeneralization</code>","text":"<p>               Bases: <code>BaseCollaborationPhase</code></p> <p>Stacked Generalization collaboration phase.</p> <p>Base models first process the input, then a meta-model combines their outputs to produce a more accurate result. This learns the strengths and weaknesses of each base model to produce optimal combinations.</p> Source code in <code>src\\ai_ensemble_suite\\collaboration\\stackedgeneralization.py</code> <pre><code>class StackedGeneralization(BaseCollaborationPhase):\n    \"\"\"Stacked Generalization collaboration phase.\n\n    Base models first process the input, then a meta-model combines their outputs\n    to produce a more accurate result. This learns the strengths and weaknesses\n    of each base model to produce optimal combinations.\n    \"\"\"\n\n    def __init__(\n            self,\n            model_manager: \"ModelManager\",\n            config_manager: \"ConfigManager\",\n            phase_name: str\n    ) -&gt; None:\n        \"\"\"Initialize the Stacked Generalization collaboration phase.\n\n        Args:\n            model_manager: The ModelManager instance.\n            config_manager: The ConfigManager instance.\n            phase_name: The name of the phase for configuration lookup.\n        \"\"\"\n        super().__init__(model_manager, config_manager, phase_name)\n\n        # Load stacking-specific configuration\n        self._base_models = self._config.get(\"base_models\", [])\n        self._meta_model = self._config.get(\"meta_model\", \"\")\n        self._combination_strategy = self._config.get(\"combination_strategy\", \"weighted\")\n        self._max_rounds = self._config.get(\"max_rounds\", 1)\n        self._use_feedback_loop = self._config.get(\"use_feedback_loop\", False)\n\n        # Validate configuration\n        if not self._base_models:\n            raise ConfigurationError(f\"Stacked Generalization phase '{phase_name}' requires at least one base model\")\n\n        if not self._meta_model:\n            raise ConfigurationError(f\"Stacked Generalization phase '{phase_name}' requires a meta model\")\n\n        # Ensure meta model is in the list of model_ids\n        if self._meta_model not in self._model_ids:\n            raise ConfigurationError(f\"Meta model '{self._meta_model}' not found in available models\")\n\n        # Ensure all base models are in the list of model_ids\n        for model in self._base_models:\n            if model not in self._model_ids:\n                raise ConfigurationError(f\"Base model '{model}' not found in available models\")\n\n        logger.debug(\n            f\"Configured Stacked Generalization phase with {len(self._base_models)} base models and '{self._meta_model}' as the meta model\",\n            extra={\n                \"combination_strategy\": self._combination_strategy,\n                \"max_rounds\": self._max_rounds,\n                \"use_feedback_loop\": self._use_feedback_loop\n            }\n        )\n\n    async def _run_base_models(\n            self,\n            prompt: str,\n            trace_collector: Optional[TraceCollector] = None\n    ) -&gt; Dict[str, Dict[str, Any]]:\n        \"\"\"Run all base models on the prompt.\n\n        Args:\n            prompt: The input prompt.\n            trace_collector: Optional trace collector.\n\n        Returns:\n            Dictionary mapping model IDs to their outputs.\n        \"\"\"\n        # Run each base model on the prompt\n        result = await self._run_models(\n            prompt=prompt,\n            model_ids=self._base_models,\n            trace_collector=trace_collector\n        )\n\n        # Add trace if collector is provided\n        if trace_collector:\n            for model_id in self._base_models:\n                trace_collector.add_model_trace(\n                    model_id=f\"{model_id}_base\",\n                    input_prompt=prompt,\n                    output=result[model_id],\n                    execution_time=result[model_id].get(\"generation_time\", 0),\n                    parameters={\"role\": \"base_model\"}\n                )\n\n        return result\n\n    async def _run_meta_model(\n            self,\n            original_prompt: str,\n            base_outputs: Dict[str, Dict[str, Any]],\n            round_num: int = 1,\n            trace_collector: Optional[TraceCollector] = None\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Run the meta model to combine base model outputs.\n\n        Args:\n            original_prompt: The original input prompt.\n            base_outputs: Dictionary mapping model IDs to their outputs.\n            round_num: Current round number for multi-round stacking.\n            trace_collector: Optional trace collector.\n\n        Returns:\n            Meta model's output.\n        \"\"\"\n        # Extract text outputs from each model\n        base_text_outputs = {model_id: output.get(\"text\", \"\")\n                             for model_id, output in base_outputs.items()}\n\n        # Format the meta-prompt that includes base model outputs\n        meta_prompt_template = self._config.get(\"meta_prompt_template\", \"\")\n        if not meta_prompt_template:\n            # Default meta prompt if none is specified\n            meta_prompt = f\"\"\"As a meta-model, your task is to combine and synthesize the outputs from multiple AI models \ninto a single, coherent, and accurate response. Here is the original query and the outputs from each model:\n\nORIGINAL QUERY:\n{original_prompt}\n\nMODEL OUTPUTS:\n\"\"\"\n            for model_id, output in base_text_outputs.items():\n                meta_prompt += f\"\\n--- {model_id} ---\\n{output}\\n\"\n\n            meta_prompt += \"\"\"\nPlease analyze these outputs and create a comprehensive response that:\n1. Integrates the strengths of each model's response\n2. Resolves any contradictions between models\n3. Provides a coherent, well-structured answer to the original query\n4. Improves upon any weaknesses or gaps in the individual responses\n\nYour synthesized response:\"\"\"\n        else:\n            # Format custom meta prompt template\n            context = {\n                \"original_prompt\": original_prompt,\n                \"base_outputs\": base_text_outputs,\n                \"round\": round_num,\n            }\n            meta_prompt = self.render_template(meta_prompt_template, context)\n\n        # Run the meta model\n        result = await self._run_models(\n            prompt=meta_prompt,\n            model_ids=[self._meta_model],\n            trace_collector=trace_collector\n        )\n\n        # Add trace if collector is provided\n        if trace_collector:\n            trace_collector.add_model_trace(\n                model_id=f\"{self._meta_model}_meta_round{round_num}\",\n                input_prompt=meta_prompt,\n                output=result[self._meta_model],\n                execution_time=result[self._meta_model].get(\"generation_time\", 0),\n                parameters={\"role\": \"meta_model\", \"round\": round_num}\n            )\n\n        return result[self._meta_model]\n\n    async def execute(\n            self,\n            query: str,\n            context: Dict[str, Any],\n            trace_collector: Optional[TraceCollector] = None\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Execute the Stacked Generalization phase.\n\n        Args:\n            query: The user query to process.\n            context: Context information from previous phases.\n            trace_collector: Optional trace collector for gathering execution details.\n\n        Returns:\n            Dictionary containing:\n                output: The final meta-model output.\n                base_outputs: Dictionary mapping base model IDs to their outputs.\n                meta_output: The meta model's output.\n                rounds: Number of rounds performed.\n                context: Updated context for the next phase.\n\n        Raises:\n            CollaborationError: If phase execution fails.\n        \"\"\"\n        start_time = time.time()\n\n        try:\n            # Prepare the base prompt\n            base_prompt = query\n            if self._prompt_template:\n                try:\n                    inputs = self._get_inputs_from_context(context)\n                    context = {\"query\": query, **inputs}\n                    base_prompt = self.render_template(self._prompt_template, context)\n                except (ConfigurationError, KeyError) as e:\n                    raise CollaborationError(f\"Failed to format prompt: {str(e)}\")\n\n            # Run the base models\n            base_results = await self._run_base_models(base_prompt, trace_collector)\n\n            # Extract text outputs for easier processing\n            base_text_outputs = {model_id: result.get(\"text\", \"\")\n                                 for model_id, result in base_results.items()}\n\n            # Initialize meta output and round counter\n            meta_result = None\n            meta_output = \"\"\n            rounds = 0\n            all_round_outputs = {}\n            all_round_results = {}\n\n            # Iterative refinement through multiple rounds if configured\n            current_base_results = base_results\n            for round_num in range(1, self._max_rounds + 1):\n                rounds = round_num\n\n                # Run the meta model\n                meta_result = await self._run_meta_model(\n                    original_prompt=base_prompt,\n                    base_outputs=current_base_results,\n                    round_num=round_num,\n                    trace_collector=trace_collector\n                )\n\n                meta_output = meta_result.get(\"text\", \"\")\n                all_round_outputs[f\"round_{round_num}\"] = meta_output\n                all_round_results[f\"round_{round_num}\"] = meta_result\n\n                # If feedback loop is enabled and we're not at the last round,\n                # use the meta output to inform the next round\n                if self._use_feedback_loop and round_num &lt; self._max_rounds:\n                    # For feedback loop, we provide the meta model's output back to base models\n                    feedback_prompt = f\"{base_prompt}\\n\\nPrevious synthesis: {meta_output}\"\n\n                    # Re-run base models with feedback from meta model\n                    current_base_results = await self._run_base_models(feedback_prompt, trace_collector)\n                else:\n                    # We're done with all rounds\n                    break\n\n            execution_time = time.time() - start_time\n\n            # Calculate confidence metrics\n            confidence = 0.0\n\n            # Confidence could be based on agreement between base models\n            if len(base_text_outputs) &gt; 1:\n                # Simple agreement metric - count words that appear in multiple outputs\n                word_sets = [set(output.lower().split()) for output in base_text_outputs.values()]\n                union_words = set().union(*word_sets)\n                if union_words:\n                    # Count how many outputs contain each word\n                    word_counts = {}\n                    for word in union_words:\n                        word_counts[word] = sum(1 for words in word_sets if word in words)\n\n                    # Average agreement across all words\n                    avg_agreement = sum(word_counts.values()) / (len(word_counts) * len(base_text_outputs))\n                    confidence = avg_agreement\n            else:\n                confidence = 1.0\n\n            # Log completion\n            logger.info(\n                f\"Stacked Generalization phase '{self._phase_name}' completed in {execution_time:.2f}s\",\n                extra={\n                    \"base_model_count\": len(self._base_models),\n                    \"rounds\": rounds,\n                    \"phase\": self._phase_name,\n                    \"confidence\": confidence\n                }\n            )\n\n            # Add phase trace if collector is provided\n            if trace_collector:\n                trace_collector.add_phase_trace(\n                    phase_name=self._phase_name,\n                    input_data={\"query\": query, \"base_prompt\": base_prompt},\n                    output_data={\n                        \"base_outputs\": base_text_outputs,\n                        \"meta_output\": meta_output,\n                        \"all_round_outputs\": all_round_outputs\n                    },\n                    execution_time=execution_time,\n                    phase_parameters={\n                        \"base_models\": self._base_models,\n                        \"meta_model\": self._meta_model,\n                        \"combination_strategy\": self._combination_strategy,\n                        \"max_rounds\": self._max_rounds,\n                        \"rounds_completed\": rounds,\n                        \"use_feedback_loop\": self._use_feedback_loop\n                    }\n                )\n\n            # Return results\n            return {\n                \"output\": meta_output,  # The final output\n                \"base_outputs\": base_text_outputs,  # All base model outputs\n                \"base_results\": base_results,  # Raw base model results\n                \"meta_output\": meta_output,  # Meta model output\n                \"meta_result\": meta_result,  # Raw meta model result\n                \"all_round_outputs\": all_round_outputs,  # Outputs from each round\n                \"all_round_results\": all_round_results,  # Raw results from each round\n                \"rounds\": rounds,  # Number of rounds performed\n                \"confidence\": confidence,  # Agreement-based confidence\n                \"execution_time\": execution_time  # Execution time\n            }\n\n        except Exception as e:\n            raise CollaborationError(\n                f\"Stacked Generalization phase '{self._phase_name}' failed: {str(e)}\"\n            )\n</code></pre>"},{"location":"api/collaboration/#ai_ensemble_suite.collaboration.StackedGeneralization.__init__","title":"<code>__init__(model_manager, config_manager, phase_name)</code>","text":"<p>Initialize the Stacked Generalization collaboration phase.</p> <p>Parameters:</p> Name Type Description Default <code>model_manager</code> <code>ModelManager</code> <p>The ModelManager instance.</p> required <code>config_manager</code> <code>ConfigManager</code> <p>The ConfigManager instance.</p> required <code>phase_name</code> <code>str</code> <p>The name of the phase for configuration lookup.</p> required Source code in <code>src\\ai_ensemble_suite\\collaboration\\stackedgeneralization.py</code> <pre><code>def __init__(\n        self,\n        model_manager: \"ModelManager\",\n        config_manager: \"ConfigManager\",\n        phase_name: str\n) -&gt; None:\n    \"\"\"Initialize the Stacked Generalization collaboration phase.\n\n    Args:\n        model_manager: The ModelManager instance.\n        config_manager: The ConfigManager instance.\n        phase_name: The name of the phase for configuration lookup.\n    \"\"\"\n    super().__init__(model_manager, config_manager, phase_name)\n\n    # Load stacking-specific configuration\n    self._base_models = self._config.get(\"base_models\", [])\n    self._meta_model = self._config.get(\"meta_model\", \"\")\n    self._combination_strategy = self._config.get(\"combination_strategy\", \"weighted\")\n    self._max_rounds = self._config.get(\"max_rounds\", 1)\n    self._use_feedback_loop = self._config.get(\"use_feedback_loop\", False)\n\n    # Validate configuration\n    if not self._base_models:\n        raise ConfigurationError(f\"Stacked Generalization phase '{phase_name}' requires at least one base model\")\n\n    if not self._meta_model:\n        raise ConfigurationError(f\"Stacked Generalization phase '{phase_name}' requires a meta model\")\n\n    # Ensure meta model is in the list of model_ids\n    if self._meta_model not in self._model_ids:\n        raise ConfigurationError(f\"Meta model '{self._meta_model}' not found in available models\")\n\n    # Ensure all base models are in the list of model_ids\n    for model in self._base_models:\n        if model not in self._model_ids:\n            raise ConfigurationError(f\"Base model '{model}' not found in available models\")\n\n    logger.debug(\n        f\"Configured Stacked Generalization phase with {len(self._base_models)} base models and '{self._meta_model}' as the meta model\",\n        extra={\n            \"combination_strategy\": self._combination_strategy,\n            \"max_rounds\": self._max_rounds,\n            \"use_feedback_loop\": self._use_feedback_loop\n        }\n    )\n</code></pre>"},{"location":"api/collaboration/#ai_ensemble_suite.collaboration.StackedGeneralization.execute","title":"<code>execute(query, context, trace_collector=None)</code>  <code>async</code>","text":"<p>Execute the Stacked Generalization phase.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The user query to process.</p> required <code>context</code> <code>Dict[str, Any]</code> <p>Context information from previous phases.</p> required <code>trace_collector</code> <code>Optional[TraceCollector]</code> <p>Optional trace collector for gathering execution details.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary containing: output: The final meta-model output. base_outputs: Dictionary mapping base model IDs to their outputs. meta_output: The meta model's output. rounds: Number of rounds performed. context: Updated context for the next phase.</p> <p>Raises:</p> Type Description <code>CollaborationError</code> <p>If phase execution fails.</p> Source code in <code>src\\ai_ensemble_suite\\collaboration\\stackedgeneralization.py</code> <pre><code>async def execute(\n        self,\n        query: str,\n        context: Dict[str, Any],\n        trace_collector: Optional[TraceCollector] = None\n) -&gt; Dict[str, Any]:\n    \"\"\"Execute the Stacked Generalization phase.\n\n    Args:\n        query: The user query to process.\n        context: Context information from previous phases.\n        trace_collector: Optional trace collector for gathering execution details.\n\n    Returns:\n        Dictionary containing:\n            output: The final meta-model output.\n            base_outputs: Dictionary mapping base model IDs to their outputs.\n            meta_output: The meta model's output.\n            rounds: Number of rounds performed.\n            context: Updated context for the next phase.\n\n    Raises:\n        CollaborationError: If phase execution fails.\n    \"\"\"\n    start_time = time.time()\n\n    try:\n        # Prepare the base prompt\n        base_prompt = query\n        if self._prompt_template:\n            try:\n                inputs = self._get_inputs_from_context(context)\n                context = {\"query\": query, **inputs}\n                base_prompt = self.render_template(self._prompt_template, context)\n            except (ConfigurationError, KeyError) as e:\n                raise CollaborationError(f\"Failed to format prompt: {str(e)}\")\n\n        # Run the base models\n        base_results = await self._run_base_models(base_prompt, trace_collector)\n\n        # Extract text outputs for easier processing\n        base_text_outputs = {model_id: result.get(\"text\", \"\")\n                             for model_id, result in base_results.items()}\n\n        # Initialize meta output and round counter\n        meta_result = None\n        meta_output = \"\"\n        rounds = 0\n        all_round_outputs = {}\n        all_round_results = {}\n\n        # Iterative refinement through multiple rounds if configured\n        current_base_results = base_results\n        for round_num in range(1, self._max_rounds + 1):\n            rounds = round_num\n\n            # Run the meta model\n            meta_result = await self._run_meta_model(\n                original_prompt=base_prompt,\n                base_outputs=current_base_results,\n                round_num=round_num,\n                trace_collector=trace_collector\n            )\n\n            meta_output = meta_result.get(\"text\", \"\")\n            all_round_outputs[f\"round_{round_num}\"] = meta_output\n            all_round_results[f\"round_{round_num}\"] = meta_result\n\n            # If feedback loop is enabled and we're not at the last round,\n            # use the meta output to inform the next round\n            if self._use_feedback_loop and round_num &lt; self._max_rounds:\n                # For feedback loop, we provide the meta model's output back to base models\n                feedback_prompt = f\"{base_prompt}\\n\\nPrevious synthesis: {meta_output}\"\n\n                # Re-run base models with feedback from meta model\n                current_base_results = await self._run_base_models(feedback_prompt, trace_collector)\n            else:\n                # We're done with all rounds\n                break\n\n        execution_time = time.time() - start_time\n\n        # Calculate confidence metrics\n        confidence = 0.0\n\n        # Confidence could be based on agreement between base models\n        if len(base_text_outputs) &gt; 1:\n            # Simple agreement metric - count words that appear in multiple outputs\n            word_sets = [set(output.lower().split()) for output in base_text_outputs.values()]\n            union_words = set().union(*word_sets)\n            if union_words:\n                # Count how many outputs contain each word\n                word_counts = {}\n                for word in union_words:\n                    word_counts[word] = sum(1 for words in word_sets if word in words)\n\n                # Average agreement across all words\n                avg_agreement = sum(word_counts.values()) / (len(word_counts) * len(base_text_outputs))\n                confidence = avg_agreement\n        else:\n            confidence = 1.0\n\n        # Log completion\n        logger.info(\n            f\"Stacked Generalization phase '{self._phase_name}' completed in {execution_time:.2f}s\",\n            extra={\n                \"base_model_count\": len(self._base_models),\n                \"rounds\": rounds,\n                \"phase\": self._phase_name,\n                \"confidence\": confidence\n            }\n        )\n\n        # Add phase trace if collector is provided\n        if trace_collector:\n            trace_collector.add_phase_trace(\n                phase_name=self._phase_name,\n                input_data={\"query\": query, \"base_prompt\": base_prompt},\n                output_data={\n                    \"base_outputs\": base_text_outputs,\n                    \"meta_output\": meta_output,\n                    \"all_round_outputs\": all_round_outputs\n                },\n                execution_time=execution_time,\n                phase_parameters={\n                    \"base_models\": self._base_models,\n                    \"meta_model\": self._meta_model,\n                    \"combination_strategy\": self._combination_strategy,\n                    \"max_rounds\": self._max_rounds,\n                    \"rounds_completed\": rounds,\n                    \"use_feedback_loop\": self._use_feedback_loop\n                }\n            )\n\n        # Return results\n        return {\n            \"output\": meta_output,  # The final output\n            \"base_outputs\": base_text_outputs,  # All base model outputs\n            \"base_results\": base_results,  # Raw base model results\n            \"meta_output\": meta_output,  # Meta model output\n            \"meta_result\": meta_result,  # Raw meta model result\n            \"all_round_outputs\": all_round_outputs,  # Outputs from each round\n            \"all_round_results\": all_round_results,  # Raw results from each round\n            \"rounds\": rounds,  # Number of rounds performed\n            \"confidence\": confidence,  # Agreement-based confidence\n            \"execution_time\": execution_time  # Execution time\n        }\n\n    except Exception as e:\n        raise CollaborationError(\n            f\"Stacked Generalization phase '{self._phase_name}' failed: {str(e)}\"\n        )\n</code></pre>"},{"location":"api/collaboration/#ai_ensemble_suite.collaboration.StructuredCritique","title":"<code>StructuredCritique</code>","text":"<p>               Bases: <code>BaseDebate</code></p> <p>Structured Critique debate pattern.</p> <p>Models evaluate others' responses using structured formats with specific critique dimensions like accuracy, clarity, and completeness.</p> Source code in <code>src\\ai_ensemble_suite\\collaboration\\structured_debate\\critique.py</code> <pre><code>class StructuredCritique(BaseDebate):\n    \"\"\"Structured Critique debate pattern.\n\n    Models evaluate others' responses using structured formats with\n    specific critique dimensions like accuracy, clarity, and completeness.\n    \"\"\"\n\n    async def _execute_debate_pattern(\n        self,\n        query: str,\n        initial_response: str,\n        inputs: Dict[str, Any],\n        trace_collector: Optional[TraceCollector] = None\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Execute the structured critique debate pattern.\n\n        Args:\n            query: The user query to process.\n            initial_response: The initial response to critique.\n            inputs: Inputs from previous phases.\n            trace_collector: Optional trace collector.\n\n        Returns:\n            Dictionary containing debate results.\n\n        Raises:\n            CollaborationError: If execution fails.\n        \"\"\"\n        # Get prompt template\n        if not self._prompt_template:\n            raise CollaborationError(\n                f\"Critique phase '{self._phase_name}' missing prompt template\"\n            )\n\n        # Format prompt\n        try:\n            context = {\n                \"query\": query,\n                \"response\": initial_response,\n                **inputs\n            }\n            critique_prompt = self.render_template(self._prompt_template, context)\n        except (ConfigurationError, KeyError) as e:\n            raise CollaborationError(f\"Failed to format critique prompt: {str(e)}\")\n\n        # Run critique models\n        model_results = await self._run_models(\n            prompt=critique_prompt,\n            trace_collector=trace_collector\n        )\n\n        # Process outputs\n        critiques = {}\n        for model_id, result in model_results.items():\n            critiques[model_id] = result.get(\"text\", \"\")\n\n        # Combine critiques if multiple critics\n        if len(critiques) &gt; 1:\n            combined_critique = \"# Combined Critiques\\n\\n\"\n            for model_id, critique in critiques.items():\n                combined_critique += f\"## Critique from {model_id}\\n\\n{critique}\\n\\n\"\n            critique_text = combined_critique\n        elif critiques:\n            # Just use the single critique\n            critique_text = list(critiques.values())[0]\n        else:\n            critique_text = \"\"\n\n        # Extract key points from the critique\n        key_points = self._extract_key_points(critique_text)\n\n        # Calculate confidence score (average from all models)\n        confidence_values = []\n        for result in model_results.values():\n            if \"confidence\" in result:\n                if isinstance(result[\"confidence\"], dict) and \"combined\" in result[\"confidence\"]:\n                    confidence_values.append(result[\"confidence\"][\"combined\"])\n                elif isinstance(result[\"confidence\"], (float, int)):\n                    confidence_values.append(result[\"confidence\"])\n\n        confidence = sum(confidence_values) / len(confidence_values) if confidence_values else 0.7\n\n        # Return results\n        return {\n            \"output\": critique_text,\n            \"critique\": critique_text,\n            \"initial_response\": initial_response,\n            \"key_points\": key_points,\n            \"model_critiques\": critiques,\n            \"raw_results\": model_results,\n            \"confidence\": confidence\n        }\n</code></pre>"},{"location":"api/collaboration/#ai_ensemble_suite.collaboration.SynthesisOriented","title":"<code>SynthesisOriented</code>","text":"<p>               Bases: <code>BaseDebate</code></p> <p>Synthesis-Oriented debate pattern.</p> <p>Models focus on finding common ground and integrating perspectives rather than critique. This pattern is more collaborative than adversarial.</p> Source code in <code>src\\ai_ensemble_suite\\collaboration\\structured_debate\\synthesis.py</code> <pre><code>class SynthesisOriented(BaseDebate):\n    \"\"\"Synthesis-Oriented debate pattern.\n\n    Models focus on finding common ground and integrating perspectives\n    rather than critique. This pattern is more collaborative than adversarial.\n    \"\"\"\n\n    async def _execute_debate_pattern(\n        self,\n        query: str,\n        initial_response: str,\n        inputs: Dict[str, Any],\n        trace_collector: Optional[TraceCollector] = None\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Execute the synthesis-oriented debate pattern.\n\n        Args:\n            query: The user query to process.\n            initial_response: The initial response to synthesize with.\n            inputs: Inputs from previous phases.\n            trace_collector: Optional trace collector.\n\n        Returns:\n            Dictionary containing debate results.\n\n        Raises:\n            CollaborationError: If execution fails.\n        \"\"\"\n        # Get prompt template\n        if not self._prompt_template:\n            raise CollaborationError(\n                f\"Synthesis phase '{self._phase_name}' missing prompt template\"\n            )\n\n        # Format prompt\n        try:\n            context = {\n                \"query\": query,\n                \"response\": initial_response,\n                **inputs\n            }\n            synthesis_prompt = self.render_template(self._prompt_template, context)\n        except (ConfigurationError, KeyError) as e:\n            raise CollaborationError(f\"Failed to format synthesis prompt: {str(e)}\")\n\n        # Run synthesis models\n        model_results = await self._run_models(\n            prompt=synthesis_prompt,\n            trace_collector=trace_collector\n        )\n\n        # Process outputs\n        synthesis_outputs = {}\n        for model_id, result in model_results.items():\n            synthesis_outputs[model_id] = result.get(\"text\", \"\")\n\n        # Combine syntheses if multiple models\n        if len(synthesis_outputs) &gt; 1:\n            # Determine the primary model (first in model_ids list)\n            primary_model = self._model_ids[0] if self._model_ids else None\n\n            # Use the primary model's synthesis if available\n            if primary_model and primary_model in synthesis_outputs:\n                synthesis_text = synthesis_outputs[primary_model]\n            else:\n                # Or combine all syntheses\n                combined_synthesis = \"# Integrated Perspectives\\n\\n\"\n                for model_id, synthesis in synthesis_outputs.items():\n                    combined_synthesis += f\"## Perspective from {model_id}\\n\\n{synthesis}\\n\\n\"\n                synthesis_text = combined_synthesis\n        elif synthesis_outputs:\n            # Just use the single synthesis\n            synthesis_text = list(synthesis_outputs.values())[0]\n        else:\n            synthesis_text = \"\"\n\n        # Calculate confidence score (average from all models)\n        confidence_values = []\n        for result in model_results.values():\n            if \"confidence\" in result:\n                if isinstance(result[\"confidence\"], dict) and \"combined\" in result[\"confidence\"]:\n                    confidence_values.append(result[\"confidence\"][\"combined\"])\n                elif isinstance(result[\"confidence\"], (float, int)):\n                    confidence_values.append(result[\"confidence\"])\n\n        confidence = sum(confidence_values) / len(confidence_values) if confidence_values else 0.7\n\n        # Return results\n        return {\n            \"output\": synthesis_text,\n            \"synthesis\": synthesis_text,\n            \"initial_response\": initial_response,\n            \"individual_syntheses\": synthesis_outputs,\n            \"raw_results\": model_results,\n            \"confidence\": confidence\n        }\n</code></pre>"},{"location":"api/collaboration/#ai_ensemble_suite.collaboration.UncertaintyBasedCollaboration","title":"<code>UncertaintyBasedCollaboration</code>","text":"<p>               Bases: <code>BaseCollaborationPhase</code></p> <p>Uncertainty-Based Collaboration phase.</p> <p>This collaborative phase uses uncertainty measurements to guide model interactions. Initial model outputs are analyzed for uncertainty, then the collaboration adapts by using more confident models to refine uncertain outputs or by selecting the most reliable outputs from multiple models.</p> Source code in <code>src\\ai_ensemble_suite\\collaboration\\uncertaintybased.py</code> <pre><code>class UncertaintyBasedCollaboration(BaseCollaborationPhase):\n    \"\"\"Uncertainty-Based Collaboration phase.\n\n    This collaborative phase uses uncertainty measurements to guide model interactions.\n    Initial model outputs are analyzed for uncertainty, then the collaboration adapts\n    by using more confident models to refine uncertain outputs or by selecting the\n    most reliable outputs from multiple models.\n    \"\"\"\n\n    def __init__(\n            self,\n            model_manager: \"ModelManager\",\n            config_manager: \"ConfigManager\",\n            phase_name: str\n    ) -&gt; None:\n        \"\"\"Initialize the Uncertainty-Based Collaboration phase.\"\"\"\n        super().__init__(model_manager, config_manager, phase_name)\n\n        # Load uncertainty-specific configuration\n        self._uncertainty_metric = self._config.get(\"uncertainty_metric\", \"disagreement\")\n        self._selection_method = self._config.get(\"selection_method\", \"least_uncertain\")\n        self._confidence_threshold = self._config.get(\"confidence_threshold\", 0.6)\n        self._fallback_model = self._config.get(\"fallback_model\", None)\n        self._refinement_iterations = self._config.get(\"refinement_iterations\", 1)\n        self._adaptive_selection = self._config.get(\"adaptive_selection\", True)\n\n        logger.debug(\n            f\"Configured UncertaintyBasedCollaboration phase with metric: {self._uncertainty_metric}\",\n            extra={\n                \"selection_method\": self._selection_method,\n                \"confidence_threshold\": self._confidence_threshold,\n                \"refinement_iterations\": self._refinement_iterations\n            }\n        )\n\n    def _calculate_uncertainty_metrics(self, outputs: List[str]) -&gt; Dict[str, float]:\n        \"\"\"Calculate uncertainty metrics between multiple outputs.\"\"\"\n        metrics = {\n            \"disagreement\": 0.0,\n            \"entropy\": 0.0,\n            \"variance\": 0.0,\n            \"confidence\": 1.0  # Start with high confidence\n        }\n\n        if not outputs or len(outputs) &lt; 2:\n            # Can't calculate uncertainty with fewer than 2 outputs\n            return metrics\n\n        # Calculate word-level disagreement\n        word_sets = []\n        for output in outputs:\n            # Create sets of significant words for comparison\n            words = set(output.lower().split())\n            word_sets.append(words)\n\n        # Calculate average Jaccard distance between all pairs\n        jaccard_distances = []\n        for i in range(len(word_sets)):\n            for j in range(i + 1, len(word_sets)):\n                # Jaccard distance = 1 - (intersection / union)\n                intersection = len(word_sets[i].intersection(word_sets[j]))\n                union = len(word_sets[i].union(word_sets[j]))\n                if union == 0:\n                    distance = 0\n                else:\n                    distance = 1.0 - (intersection / union)\n                jaccard_distances.append(distance)\n\n        if jaccard_distances:\n            metrics[\"disagreement\"] = sum(jaccard_distances) / len(jaccard_distances)\n\n        # Calculate word distribution entropy\n        all_words = []\n        for output in outputs:\n            all_words.extend(output.lower().split())\n\n        word_counts = Counter(all_words)\n        total_words = len(all_words)\n\n        if total_words &gt; 0:\n            # Calculate normalized entropy\n            probs = [count / total_words for count in word_counts.values()]\n            try:\n                entropy = -sum(p * np.log2(p) for p in probs)\n                # Normalize by maximum possible entropy\n                max_entropy = np.log2(len(word_counts)) if word_counts else 0\n                if max_entropy &gt; 0:\n                    metrics[\"entropy\"] = min(1.0, entropy / max_entropy)\n            except (ValueError, TypeError):\n                # Handle numerical issues\n                metrics[\"entropy\"] = 0.5\n\n        # Calculate output length variance (normalized)\n        if len(outputs) &gt; 1:\n            lengths = [len(output) for output in outputs]\n            mean_len = sum(lengths) / len(lengths)\n            if mean_len &gt; 0:\n                variance = sum((length - mean_len) ** 2 for length in lengths) / len(lengths)\n                # Normalize variance by mean squared to get scale-invariant measure\n                metrics[\"variance\"] = min(1.0, variance / (mean_len ** 2)) if mean_len &gt; 0 else 0\n\n        # Calculate overall confidence score (inverse of uncertainty)\n        # Weight the metrics based on importance\n        weights = {\n            \"disagreement\": 0.6,  # Disagreement has highest weight\n            \"entropy\": 0.3,  # Entropy next\n            \"variance\": 0.1  # Length variance least important\n        }\n\n        uncertainty_score = sum(metrics[metric] * weight for metric, weight in weights.items())\n        metrics[\"confidence\"] = 1.0 - min(1.0, uncertainty_score)\n\n        return metrics\n\n    def _calculate_output_disagreement(self, target_output: str, other_outputs: List[str]) -&gt; float:\n        \"\"\"Calculate how much a specific output disagrees with others.\"\"\"\n        if not other_outputs:\n            return 0.0\n\n        target_words = set(target_output.lower().split())\n        all_disagreements = []\n\n        for other in other_outputs:\n            other_words = set(other.lower().split())\n            union = len(target_words.union(other_words))\n\n            if union == 0:  # Both empty\n                all_disagreements.append(0.0)\n            else:\n                intersection = len(target_words.intersection(other_words))\n                disagreement = 1.0 - (intersection / union)  # Jaccard distance\n                all_disagreements.append(disagreement)\n\n        # Return average disagreement with all other outputs\n        return sum(all_disagreements) / len(all_disagreements) if all_disagreements else 0.0\n\n    def _get_output_text(self, output_value: Any) -&gt; str:\n        \"\"\"Safely extract text from an output value, handling different formats.\"\"\"\n        if isinstance(output_value, dict) and \"text\" in output_value:\n            return output_value[\"text\"]\n        elif isinstance(output_value, str):\n            return output_value\n        else:\n            logger.warning(f\"Unexpected output format: {type(output_value)}\")\n            return str(output_value)\n\n    def _normalize_outputs(self, outputs: Dict[str, Any]) -&gt; Dict[str, Dict[str, Any]]:\n        \"\"\"Normalize outputs to a consistent format with 'text' key.\"\"\"\n        normalized = {}\n        for key, value in outputs.items():\n            if isinstance(value, dict) and \"text\" in value:\n                normalized[key] = value\n            elif isinstance(value, str):\n                normalized[key] = {\"text\": value, \"confidence\": 0.5}\n            else:\n                logger.warning(f\"Unexpected output format for key {key}: {type(value)}\")\n                normalized[key] = {\"text\": str(value), \"confidence\": 0.3}\n        return normalized\n\n    def _select_model_subset(\n            self,\n            model_outputs: Dict[str, Any],\n            uncertainty_metrics: Dict[str, float]\n    ) -&gt; List[str]:\n        \"\"\"Select a subset of models for refinement based on uncertainty metrics.\"\"\"\n        normalized_outputs = self._normalize_outputs(model_outputs)\n\n        if len(normalized_outputs) &lt;= 1:\n            return list(normalized_outputs.keys())\n\n        # Calculate per-model disagreement\n        model_scores = {}\n        for model_id, output_data in normalized_outputs.items():\n            output_text = output_data[\"text\"]\n            other_outputs = [\n                normalized_outputs[m_id][\"text\"] for m_id in normalized_outputs\n                if m_id != model_id\n            ]\n            disagreement = self._calculate_output_disagreement(output_text, other_outputs)\n            agreement = 1.0 - disagreement\n            model_scores[model_id] = agreement\n\n        # If overall confidence is high, use all models\n        if uncertainty_metrics[\"confidence\"] &gt;= self._confidence_threshold:\n            return list(normalized_outputs.keys())\n\n        # Otherwise, select top models based on agreement\n        sorted_models = sorted(model_scores.keys(), key=lambda m: model_scores[m], reverse=True)\n\n        # Take top half of models (at least 1)\n        num_to_select = max(1, len(sorted_models) // 2)\n        return sorted_models[:num_to_select]\n\n    def _create_refinement_prompt(\n            self,\n            base_prompt: str,\n            model_outputs: Dict[str, Any],\n            uncertainty_metrics: Dict[str, float]\n    ) -&gt; str:\n        \"\"\"Create a prompt for the refinement models, incorporating uncertainty information.\"\"\"\n        normalized_outputs = self._normalize_outputs(model_outputs)\n\n        refinement_prompt = (\n            f\"The following question needs a refined answer:\\n\\n\"\n            f\"{base_prompt}\\n\\n\"\n            f\"Initial responses have been generated with a confidence of {uncertainty_metrics['confidence']:.2f}.\\n\\n\"\n        )\n\n        # Add information about areas of disagreement\n        if uncertainty_metrics[\"disagreement\"] &gt; 0.3:\n            refinement_prompt += \"There is significant disagreement between initial responses. \"\n\n        if uncertainty_metrics[\"entropy\"] &gt; 0.5:\n            refinement_prompt += \"The responses show high variability in their content. \"\n\n        refinement_prompt += \"Here are the previous responses:\\n\\n\"\n\n        # Add model outputs\n        for i, (model_id, output_data) in enumerate(normalized_outputs.items()):\n            output_text = output_data[\"text\"]\n            # Limit text length to avoid excessively long prompts\n            if len(output_text) &gt; 500:\n                output_text = output_text[:497] + \"...\"\n            refinement_prompt += f\"Response {i+1}:\\n{output_text}\\n\\n\"\n\n        refinement_prompt += (\n            \"Please provide a refined, accurate response that addresses any inconsistencies \"\n            \"or uncertainties in the previous responses. Focus on generating a comprehensive \"\n            \"and confident answer.\"\n        )\n\n        return refinement_prompt\n\n    def _select_best_output(self, outputs: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Select the best output from a set of outputs based on confidence or other metrics.\n\n        Args:\n            outputs: Dictionary of outputs to select from\n\n        Returns:\n            Dictionary containing the selected output information including model_id and text\n        \"\"\"\n        if not outputs:\n            return {\"model_id\": None, \"text\": \"No outputs available.\"}\n\n        normalized = self._normalize_outputs(outputs)\n\n        # If only one output, return it\n        if len(normalized) == 1:\n            model_id = next(iter(normalized))\n            return {\n                \"model_id\": model_id,\n                \"text\": normalized[model_id][\"text\"],\n                \"confidence\": normalized[model_id].get(\"confidence\", 0.5)\n            }\n\n        # Otherwise, select based on confidence\n        best_model_id = None\n        best_confidence = -1\n\n        for model_id, output_data in normalized.items():\n            confidence = output_data.get(\"confidence\", 0.5)\n            if confidence &gt; best_confidence:\n                best_confidence = confidence\n                best_model_id = model_id\n\n        # Return best output, or first one if all confidences are equal\n        if best_model_id is None:\n            best_model_id = next(iter(normalized))\n\n        return {\n            \"model_id\": best_model_id,\n            \"text\": normalized[best_model_id][\"text\"],\n            \"confidence\": normalized[best_model_id].get(\"confidence\", 0.5)\n        }\n\n    async def execute(\n        self,\n        query: str,\n        context: Dict[str, Any],\n        trace_collector: Optional[TraceCollector] = None\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Execute the Uncertainty-Based Collaboration phase.\"\"\"\n        start_time = time.time()\n\n        try:\n            # Prepare the base prompt\n            base_prompt = query\n            if self._prompt_template:\n                try:\n                    inputs = self._get_inputs_from_context(context)\n                    context_dict = {\"query\": query, **inputs}\n                    # import json\n                    # print(\"DEBUG CONTEXT STRUCTURE:\", json.dumps(context_dict, default=str, indent=2))\n                    base_prompt = self.render_template(self._prompt_template, context_dict)\n                except (ConfigurationError, KeyError) as e:\n                    raise CollaborationError(f\"Failed to format prompt: {str(e)}\")\n\n            # Check if we have inputs from previous phases to use\n            initial_outputs = {}\n            if 'input_from' in self._config and context:\n                for phase_name in self._config['input_from']:\n                    if phase_name in context:\n                        phase_data = context[phase_name]\n                        if 'outputs' in phase_data:\n                            # Use outputs from previous phase as initial outputs\n                            initial_outputs = phase_data['outputs']\n                            logger.debug(f\"Using {len(initial_outputs)} outputs from phase '{phase_name}'\")\n                        elif 'output' in phase_data:\n                            # If only a single output is available\n                            initial_outputs = {'previous_phase': phase_data['output']}\n                            logger.debug(f\"Using single output from phase '{phase_name}'\")\n\n            # If no previous outputs, get initial outputs from all models\n            if not initial_outputs:\n                initial_results = await self._run_models(\n                    prompt=base_prompt,\n                    model_ids=self._model_ids,\n                    trace_collector=trace_collector\n                )\n\n                # Extract text outputs\n                for model_id, result in initial_results.items():\n                    if \"text\" in result:\n                        initial_outputs[model_id] = {\n                            \"text\": result[\"text\"],\n                            \"confidence\": result.get(\"confidence\", 0.5),\n                            \"generation_time\": result.get(\"generation_time\", 0)\n                        }\n\n            # If still no outputs, raise an error\n            if not initial_outputs:\n                raise CollaborationError(\"No outputs available from models or previous phases\")\n\n            # Calculate uncertainty metrics for initial outputs - handle both dict and string outputs\n            all_outputs = []\n            for key, value in initial_outputs.items():\n                all_outputs.append(self._get_output_text(value))\n\n            uncertainty_metrics = self._calculate_uncertainty_metrics(all_outputs)\n\n            # Store iterations for tracing\n            iterations = [\n                {\n                    \"iteration\": 0,\n                    \"outputs\": initial_outputs,\n                    \"uncertainty_metrics\": uncertainty_metrics\n                }\n            ]\n\n            # Normalize outputs for consistent processing\n            current_outputs = self._normalize_outputs(initial_outputs)\n            current_metrics = uncertainty_metrics\n\n            # Determine if refinement is needed based on confidence\n            needs_refinement = current_metrics[\"confidence\"] &lt; self._confidence_threshold\n            refinement_count = 0\n            refinement_success = False\n\n            # Perform iterative refinement if needed\n            while (needs_refinement and\n                   refinement_count &lt; self._refinement_iterations):\n\n                refinement_count += 1\n                logger.info(\n                    f\"Starting refinement iteration {refinement_count} due to low confidence \"\n                    f\"({current_metrics['confidence']:.2f})\",\n                    extra={\"threshold\": self._confidence_threshold}\n                )\n\n                try:\n                    # Select models for refinement\n                    if self._adaptive_selection:\n                        # Get model IDs that correspond to actual loaded models\n                        available_models = []\n                        for model_id in self._model_ids:\n                            if model_id in current_outputs or model_id in self._model_manager.get_loaded_models():\n                                available_models.append(model_id)\n\n                        if not available_models:\n                            logger.warning(\"No models available for refinement, using all configured models\")\n                            available_models = self._model_ids\n\n                        refinement_models = self._select_model_subset(current_outputs, current_metrics)\n                        # Ensure selected models are available\n                        refinement_models = [m for m in refinement_models if m in available_models]\n                    else:\n                        # Use all models\n                        refinement_models = self._model_ids\n\n                    # Skip refinement if no models available\n                    if not refinement_models:\n                        logger.warning(\"No models available for refinement, skipping\")\n                        break\n\n                    # Create refinement prompt with uncertainty information\n                    refinement_prompt = self._create_refinement_prompt(\n                        base_prompt, current_outputs, current_metrics\n                    )\n\n                    # Run selected models with refinement prompt\n                    refinement_results = await self._run_models(\n                        prompt=refinement_prompt,\n                        model_ids=refinement_models,\n                        trace_collector=trace_collector\n                    )\n\n                    # Extract refined outputs - check if we got any results\n                    refined_outputs = {}\n                    for model_id, result in refinement_results.items():\n                        if \"text\" in result:\n                            refined_outputs[model_id] = {\n                                \"text\": result[\"text\"],\n                                \"confidence\": result.get(\"confidence\", 0.5),\n                                \"generation_time\": result.get(\"generation_time\", 0),\n                                \"is_refinement\": True\n                            }\n\n                    # If we got no refined outputs, stop refinement\n                    if not refined_outputs:\n                        logger.warning(\"No refined outputs produced in this iteration, stopping refinement\")\n                        break\n\n                    # Calculate uncertainty metrics for refined outputs\n                    refined_texts = [output[\"text\"] for output in refined_outputs.values()]\n                    refined_metrics = self._calculate_uncertainty_metrics(refined_texts)\n\n                    # Store this iteration\n                    iterations.append({\n                        \"iteration\": refinement_count,\n                        \"outputs\": refined_outputs,\n                        \"uncertainty_metrics\": refined_metrics,\n                        \"refinement_models\": refinement_models\n                    })\n\n                    # Update current state\n                    current_outputs = refined_outputs\n                    current_metrics = refined_metrics\n                    refinement_success = True\n\n                    # Check if further refinement is needed\n                    needs_refinement = (\n                        current_metrics[\"confidence\"] &lt; self._confidence_threshold and\n                        refinement_count &lt; self._refinement_iterations\n                    )\n\n                except Exception as e:\n                    logger.error(f\"Error during refinement iteration {refinement_count}: {e}\")\n                    # Continue with the next iteration if available, or exit the loop\n                    if refinement_count &gt;= self._refinement_iterations:\n                        break\n\n            # Select final output based on uncertainty metrics across all iterations\n            best_iteration = 0\n            best_confidence = iterations[0][\"uncertainty_metrics\"][\"confidence\"]\n\n            for i, iteration_data in enumerate(iterations):\n                confidence = iteration_data[\"uncertainty_metrics\"][\"confidence\"]\n                if confidence &gt; best_confidence:\n                    best_confidence = confidence\n                    best_iteration = i\n\n            # Get outputs from best iteration\n            best_iteration_data = iterations[best_iteration]\n            best_outputs = best_iteration_data[\"outputs\"]\n\n            # Select best output\n            best_output_info = self._select_best_output(best_outputs)\n\n            # Get final output information\n            best_model_id = best_output_info[\"model_id\"]\n            best_output = best_output_info[\"text\"]\n            best_metrics = best_iteration_data[\"uncertainty_metrics\"]\n\n            execution_time = time.time() - start_time\n\n            # Log completion\n            logger.info(\n                f\"Uncertainty-based collaboration phase completed in {execution_time:.2f}s \"\n                f\"with {refinement_count} refinement iterations\",\n                extra={\n                    \"confidence\": best_metrics[\"confidence\"],\n                    \"best_iteration\": best_iteration,\n                    \"best_model\": best_model_id,\n                    \"refinement_success\": refinement_success\n                }\n            )\n\n            # Prepare result with all outputs and metrics\n            final_result = {\n                \"output\": best_output,\n                \"confidence\": best_metrics[\"confidence\"],\n                \"best_model\": best_model_id,\n                \"best_iteration\": best_iteration,\n                \"refinement_count\": refinement_count,\n                \"refinement_success\": refinement_success,\n                \"uncertainty_metrics\": best_metrics,\n                \"iterations\": iterations\n            }\n\n            # Add phase trace if collector is provided\n            if trace_collector:\n                trace_collector.add_phase_trace(\n                    phase_name=self._phase_name,\n                    input_data={\"query\": query, \"base_prompt\": base_prompt},\n                    output_data=final_result,\n                    execution_time=execution_time,\n                    phase_parameters={\n                        \"uncertainty_metric\": self._uncertainty_metric,\n                        \"selection_method\": self._selection_method,\n                        \"confidence_threshold\": self._confidence_threshold,\n                        \"refinement_iterations\": self._refinement_iterations\n                    }\n                )\n\n            return final_result\n\n        except Exception as e:\n            logger.error(f\"Error in uncertainty-based collaboration: {e}\", exc_info=True)\n            raise CollaborationError(f\"Uncertainty-based collaboration failed: {str(e)}\")\n</code></pre>"},{"location":"api/collaboration/#ai_ensemble_suite.collaboration.UncertaintyBasedCollaboration.__init__","title":"<code>__init__(model_manager, config_manager, phase_name)</code>","text":"<p>Initialize the Uncertainty-Based Collaboration phase.</p> Source code in <code>src\\ai_ensemble_suite\\collaboration\\uncertaintybased.py</code> <pre><code>def __init__(\n        self,\n        model_manager: \"ModelManager\",\n        config_manager: \"ConfigManager\",\n        phase_name: str\n) -&gt; None:\n    \"\"\"Initialize the Uncertainty-Based Collaboration phase.\"\"\"\n    super().__init__(model_manager, config_manager, phase_name)\n\n    # Load uncertainty-specific configuration\n    self._uncertainty_metric = self._config.get(\"uncertainty_metric\", \"disagreement\")\n    self._selection_method = self._config.get(\"selection_method\", \"least_uncertain\")\n    self._confidence_threshold = self._config.get(\"confidence_threshold\", 0.6)\n    self._fallback_model = self._config.get(\"fallback_model\", None)\n    self._refinement_iterations = self._config.get(\"refinement_iterations\", 1)\n    self._adaptive_selection = self._config.get(\"adaptive_selection\", True)\n\n    logger.debug(\n        f\"Configured UncertaintyBasedCollaboration phase with metric: {self._uncertainty_metric}\",\n        extra={\n            \"selection_method\": self._selection_method,\n            \"confidence_threshold\": self._confidence_threshold,\n            \"refinement_iterations\": self._refinement_iterations\n        }\n    )\n</code></pre>"},{"location":"api/collaboration/#ai_ensemble_suite.collaboration.UncertaintyBasedCollaboration.execute","title":"<code>execute(query, context, trace_collector=None)</code>  <code>async</code>","text":"<p>Execute the Uncertainty-Based Collaboration phase.</p> Source code in <code>src\\ai_ensemble_suite\\collaboration\\uncertaintybased.py</code> <pre><code>async def execute(\n    self,\n    query: str,\n    context: Dict[str, Any],\n    trace_collector: Optional[TraceCollector] = None\n) -&gt; Dict[str, Any]:\n    \"\"\"Execute the Uncertainty-Based Collaboration phase.\"\"\"\n    start_time = time.time()\n\n    try:\n        # Prepare the base prompt\n        base_prompt = query\n        if self._prompt_template:\n            try:\n                inputs = self._get_inputs_from_context(context)\n                context_dict = {\"query\": query, **inputs}\n                # import json\n                # print(\"DEBUG CONTEXT STRUCTURE:\", json.dumps(context_dict, default=str, indent=2))\n                base_prompt = self.render_template(self._prompt_template, context_dict)\n            except (ConfigurationError, KeyError) as e:\n                raise CollaborationError(f\"Failed to format prompt: {str(e)}\")\n\n        # Check if we have inputs from previous phases to use\n        initial_outputs = {}\n        if 'input_from' in self._config and context:\n            for phase_name in self._config['input_from']:\n                if phase_name in context:\n                    phase_data = context[phase_name]\n                    if 'outputs' in phase_data:\n                        # Use outputs from previous phase as initial outputs\n                        initial_outputs = phase_data['outputs']\n                        logger.debug(f\"Using {len(initial_outputs)} outputs from phase '{phase_name}'\")\n                    elif 'output' in phase_data:\n                        # If only a single output is available\n                        initial_outputs = {'previous_phase': phase_data['output']}\n                        logger.debug(f\"Using single output from phase '{phase_name}'\")\n\n        # If no previous outputs, get initial outputs from all models\n        if not initial_outputs:\n            initial_results = await self._run_models(\n                prompt=base_prompt,\n                model_ids=self._model_ids,\n                trace_collector=trace_collector\n            )\n\n            # Extract text outputs\n            for model_id, result in initial_results.items():\n                if \"text\" in result:\n                    initial_outputs[model_id] = {\n                        \"text\": result[\"text\"],\n                        \"confidence\": result.get(\"confidence\", 0.5),\n                        \"generation_time\": result.get(\"generation_time\", 0)\n                    }\n\n        # If still no outputs, raise an error\n        if not initial_outputs:\n            raise CollaborationError(\"No outputs available from models or previous phases\")\n\n        # Calculate uncertainty metrics for initial outputs - handle both dict and string outputs\n        all_outputs = []\n        for key, value in initial_outputs.items():\n            all_outputs.append(self._get_output_text(value))\n\n        uncertainty_metrics = self._calculate_uncertainty_metrics(all_outputs)\n\n        # Store iterations for tracing\n        iterations = [\n            {\n                \"iteration\": 0,\n                \"outputs\": initial_outputs,\n                \"uncertainty_metrics\": uncertainty_metrics\n            }\n        ]\n\n        # Normalize outputs for consistent processing\n        current_outputs = self._normalize_outputs(initial_outputs)\n        current_metrics = uncertainty_metrics\n\n        # Determine if refinement is needed based on confidence\n        needs_refinement = current_metrics[\"confidence\"] &lt; self._confidence_threshold\n        refinement_count = 0\n        refinement_success = False\n\n        # Perform iterative refinement if needed\n        while (needs_refinement and\n               refinement_count &lt; self._refinement_iterations):\n\n            refinement_count += 1\n            logger.info(\n                f\"Starting refinement iteration {refinement_count} due to low confidence \"\n                f\"({current_metrics['confidence']:.2f})\",\n                extra={\"threshold\": self._confidence_threshold}\n            )\n\n            try:\n                # Select models for refinement\n                if self._adaptive_selection:\n                    # Get model IDs that correspond to actual loaded models\n                    available_models = []\n                    for model_id in self._model_ids:\n                        if model_id in current_outputs or model_id in self._model_manager.get_loaded_models():\n                            available_models.append(model_id)\n\n                    if not available_models:\n                        logger.warning(\"No models available for refinement, using all configured models\")\n                        available_models = self._model_ids\n\n                    refinement_models = self._select_model_subset(current_outputs, current_metrics)\n                    # Ensure selected models are available\n                    refinement_models = [m for m in refinement_models if m in available_models]\n                else:\n                    # Use all models\n                    refinement_models = self._model_ids\n\n                # Skip refinement if no models available\n                if not refinement_models:\n                    logger.warning(\"No models available for refinement, skipping\")\n                    break\n\n                # Create refinement prompt with uncertainty information\n                refinement_prompt = self._create_refinement_prompt(\n                    base_prompt, current_outputs, current_metrics\n                )\n\n                # Run selected models with refinement prompt\n                refinement_results = await self._run_models(\n                    prompt=refinement_prompt,\n                    model_ids=refinement_models,\n                    trace_collector=trace_collector\n                )\n\n                # Extract refined outputs - check if we got any results\n                refined_outputs = {}\n                for model_id, result in refinement_results.items():\n                    if \"text\" in result:\n                        refined_outputs[model_id] = {\n                            \"text\": result[\"text\"],\n                            \"confidence\": result.get(\"confidence\", 0.5),\n                            \"generation_time\": result.get(\"generation_time\", 0),\n                            \"is_refinement\": True\n                        }\n\n                # If we got no refined outputs, stop refinement\n                if not refined_outputs:\n                    logger.warning(\"No refined outputs produced in this iteration, stopping refinement\")\n                    break\n\n                # Calculate uncertainty metrics for refined outputs\n                refined_texts = [output[\"text\"] for output in refined_outputs.values()]\n                refined_metrics = self._calculate_uncertainty_metrics(refined_texts)\n\n                # Store this iteration\n                iterations.append({\n                    \"iteration\": refinement_count,\n                    \"outputs\": refined_outputs,\n                    \"uncertainty_metrics\": refined_metrics,\n                    \"refinement_models\": refinement_models\n                })\n\n                # Update current state\n                current_outputs = refined_outputs\n                current_metrics = refined_metrics\n                refinement_success = True\n\n                # Check if further refinement is needed\n                needs_refinement = (\n                    current_metrics[\"confidence\"] &lt; self._confidence_threshold and\n                    refinement_count &lt; self._refinement_iterations\n                )\n\n            except Exception as e:\n                logger.error(f\"Error during refinement iteration {refinement_count}: {e}\")\n                # Continue with the next iteration if available, or exit the loop\n                if refinement_count &gt;= self._refinement_iterations:\n                    break\n\n        # Select final output based on uncertainty metrics across all iterations\n        best_iteration = 0\n        best_confidence = iterations[0][\"uncertainty_metrics\"][\"confidence\"]\n\n        for i, iteration_data in enumerate(iterations):\n            confidence = iteration_data[\"uncertainty_metrics\"][\"confidence\"]\n            if confidence &gt; best_confidence:\n                best_confidence = confidence\n                best_iteration = i\n\n        # Get outputs from best iteration\n        best_iteration_data = iterations[best_iteration]\n        best_outputs = best_iteration_data[\"outputs\"]\n\n        # Select best output\n        best_output_info = self._select_best_output(best_outputs)\n\n        # Get final output information\n        best_model_id = best_output_info[\"model_id\"]\n        best_output = best_output_info[\"text\"]\n        best_metrics = best_iteration_data[\"uncertainty_metrics\"]\n\n        execution_time = time.time() - start_time\n\n        # Log completion\n        logger.info(\n            f\"Uncertainty-based collaboration phase completed in {execution_time:.2f}s \"\n            f\"with {refinement_count} refinement iterations\",\n            extra={\n                \"confidence\": best_metrics[\"confidence\"],\n                \"best_iteration\": best_iteration,\n                \"best_model\": best_model_id,\n                \"refinement_success\": refinement_success\n            }\n        )\n\n        # Prepare result with all outputs and metrics\n        final_result = {\n            \"output\": best_output,\n            \"confidence\": best_metrics[\"confidence\"],\n            \"best_model\": best_model_id,\n            \"best_iteration\": best_iteration,\n            \"refinement_count\": refinement_count,\n            \"refinement_success\": refinement_success,\n            \"uncertainty_metrics\": best_metrics,\n            \"iterations\": iterations\n        }\n\n        # Add phase trace if collector is provided\n        if trace_collector:\n            trace_collector.add_phase_trace(\n                phase_name=self._phase_name,\n                input_data={\"query\": query, \"base_prompt\": base_prompt},\n                output_data=final_result,\n                execution_time=execution_time,\n                phase_parameters={\n                    \"uncertainty_metric\": self._uncertainty_metric,\n                    \"selection_method\": self._selection_method,\n                    \"confidence_threshold\": self._confidence_threshold,\n                    \"refinement_iterations\": self._refinement_iterations\n                }\n            )\n\n        return final_result\n\n    except Exception as e:\n        logger.error(f\"Error in uncertainty-based collaboration: {e}\", exc_info=True)\n        raise CollaborationError(f\"Uncertainty-based collaboration failed: {str(e)}\")\n</code></pre>"},{"location":"api/configuration/","title":"Configuration API","text":"<p>API reference for configuration options. </p> <p>Configuration management for ai-ensemble-suite.</p>"},{"location":"api/configuration/#ai_ensemble_suite.config.ConfigManager","title":"<code>ConfigManager</code>","text":"<p>Manages configuration for the ai-ensemble-suite.</p> <p>Handles loading, validation, and access to configuration values.</p> Source code in <code>src\\ai_ensemble_suite\\config\\config_manager.py</code> <pre><code>class ConfigManager:\n    \"\"\"Manages configuration for the ai-ensemble-suite.\n\n    Handles loading, validation, and access to configuration values.\n    \"\"\"\n\n    def __init__(\n        self,\n        config_path: Optional[str] = None,\n        config_dict: Optional[Dict[str, Any]] = None\n    ) -&gt; None:\n        \"\"\"Initialize the ConfigManager.\n\n        Args:\n            config_path: Path to a YAML configuration file.\n            config_dict: Dictionary containing configuration values.\n\n        Raises:\n            ConfigurationError: If both config_path and config_dict are provided\n              or if configuration is invalid.\n        \"\"\"\n        self.config: Dict[str, Any] = {}\n\n        # Start with default configuration\n        self._load_defaults()\n\n        # Load configuration if provided\n        try:\n            if config_path or config_dict:\n                self.load(config_path, config_dict)\n        except (ConfigurationError, ValidationError) as e:\n             # Catch validation errors during initial load and re-raise\n             logger.error(f\"Initial configuration loading failed: {e}\", exc_info=True)\n             raise ConfigurationError(f\"Initial configuration loading failed: {e}\") from e\n\n\n    def _load_defaults(self) -&gt; None:\n        \"\"\"Load default configuration values.\"\"\"\n        self.config = copy.deepcopy(DEFAULT_CONFIG)\n\n        # Add all template variations from the templates module\n        # Ensure ALL_TEMPLATES is a dict {name: string}\n        if isinstance(ALL_TEMPLATES, dict):\n            self.config.setdefault(\"templates\", {}).update(ALL_TEMPLATES)\n        else:\n             logger.warning(\"ALL_TEMPLATES from templates module is not a dictionary, cannot load default templates.\")\n\n\n    def load(\n        self,\n        config_path: Optional[str] = None,\n        config_dict: Optional[Dict[str, Any]] = None\n    ) -&gt; None:\n        \"\"\"Load configuration from a file or dictionary, merging with defaults.\n\n        Args:\n            config_path: Path to a YAML configuration file.\n            config_dict: Dictionary containing configuration values.\n\n        Raises:\n            ConfigurationError: If both config_path and config_dict are provided,\n              or if file loading/parsing fails.\n            ValidationError: If the merged configuration is invalid.\n        \"\"\"\n        if config_path and config_dict:\n            raise ConfigurationError(\n                \"Cannot specify both config_path and config_dict\"\n            )\n\n        user_config: Optional[Dict[str, Any]] = None\n\n        # Load from file if specified\n        if config_path:\n            try:\n                # Ensure path is absolute or resolved correctly relative to execution context\n                resolved_path = Path(config_path).resolve()\n                logger.debug(f\"Attempting to load configuration from: {resolved_path}\")\n                if not resolved_path.is_file():\n                    raise FileNotFoundError(f\"Configuration file not found or is not a file at: {resolved_path}\")\n                with open(resolved_path, \"r\", encoding=\"utf-8\") as file:\n                    loaded_content = yaml.safe_load(file)\n                    if not isinstance(loaded_content, dict):\n                        # Allow empty file (None) but not non-dict types\n                        if loaded_content is not None:\n                             raise ConfigurationError(\"Configuration file content must be a YAML dictionary (mapping).\")\n                        user_config = {} # Treat empty file as empty config\n                    else:\n                        user_config = loaded_content\n            except (yaml.YAMLError, FileNotFoundError, OSError) as e:\n                raise ConfigurationError(f\"Failed to load or parse configuration file '{config_path}': {str(e)}\")\n        # Use provided dictionary if specified\n        elif config_dict:\n            if not isinstance(config_dict, dict):\n                raise ConfigurationError(\"config_dict must be a dictionary\")\n            user_config = config_dict\n\n        # Merge user config into existing (default) config if loaded\n        if user_config is not None:\n            try:\n                # Deep update modifies self.config in place\n                self._update_config_recursive(self.config, user_config)\n                logger.debug(\"User configuration merged with defaults.\")\n            except Exception as e:\n                 # Should not happen with basic dict merge, but safety catch\n                 raise ConfigurationError(f\"Internal error merging configuration: {str(e)}\")\n\n        # Always validate the final merged configuration\n        try:\n            self.validate()\n            logger.info(\"Configuration loaded and validated successfully.\")\n        except ValidationError as e:\n            # If validation fails after loading/merging, it's a fatal config error.\n            # We don't revert here; the constructor or update method handles reversion.\n            logger.error(f\"Configuration validation failed: {e}\")\n            raise ValidationError(f\"Invalid configuration after loading/merging: {str(e)}\") from e\n\n\n    def _update_config_recursive(self, config: Dict[str, Any], updates: Dict[str, Any]) -&gt; None:\n        \"\"\"Recursively update configuration dictionary 'config' with 'updates'.\n\n        Args:\n            config: Existing configuration dictionary to update (modified in place).\n            updates: New values to apply to the configuration.\n        \"\"\"\n        for key, value in updates.items():\n            if key in config and isinstance(config[key], dict) and isinstance(value, dict):\n                # Recursively update dictionaries\n                self._update_config_recursive(config[key], value)\n            # Allow replacing lists entirely, don't merge them element-wise by default\n            # elif key in config and isinstance(config[key], list) and isinstance(value, list):\n                 # List merging logic could go here if needed, but replacement is simpler\n                 # config[key] = copy.deepcopy(value)\n            elif isinstance(value, (dict, list)):\n                # Use deepcopy for new/overwritten nested structures to avoid shared references\n                config[key] = copy.deepcopy(value)\n            else:\n                # Replace or add the value (basic types are copied by assignment/immutable)\n                config[key] = value\n\n    def validate(self) -&gt; None:\n        \"\"\"Validate the current configuration against the schema.\n\n        Raises:\n            ValidationError: If the configuration is invalid.\n        \"\"\"\n        logger.debug(\"Validating configuration...\")\n\n        # Check for required top-level sections and their types\n        required_sections = {\n            \"models\": dict, # Models section is crucial even if empty\n            \"collaboration\": dict,\n            \"aggregation\": dict,\n            \"templates\": dict\n        }\n        for section, expected_type in required_sections.items():\n            if section not in self.config:\n                raise ValidationError(f\"Missing required configuration section: '{section}'\")\n            if not isinstance(self.config[section], expected_type):\n                 raise ValidationError(f\"Configuration section '{section}' must be a {expected_type.__name__}.\")\n\n        # --- Get IDs and Names for Cross-Validation ---\n        # Get all model IDs\n        model_ids = set(self.get_model_ids()) # Uses self.config['models']\n\n        # Get all phase names\n        phase_names = set()\n        # Ensure collaboration and phases exist and are the correct type first\n        if \"collaboration\" in self.config and isinstance(self.config[\"collaboration\"], dict) and \\\n           \"phases\" in self.config[\"collaboration\"] and isinstance(self.config[\"collaboration\"][\"phases\"], list):\n            phases_list = self.config[\"collaboration\"][\"phases\"]\n            for phase in phases_list:\n                # Ensure phase is dict and has a string name\n                if isinstance(phase, dict) and \"name\" in phase and isinstance(phase[\"name\"], str):\n                    phase_names.add(phase[\"name\"])\n                # else: Handled by collaboration config validation below\n\n        # --- Run Schema Validations ---\n        # Validate models configuration\n        # Models section existence/type already checked, safe to access\n        for model_id, model_config in self.config[\"models\"].items():\n            try:\n                ConfigSchema.validate_model_config(model_id, model_config)\n            except ValidationError as e:\n                raise ValidationError(f\"Invalid model configuration for '{model_id}': {str(e)}\")\n\n        # Validate collaboration configuration\n        # Section existence/type checked, safe to access\n        try:\n            ConfigSchema.validate_collaboration_config(self.config[\"collaboration\"], model_ids)\n            # Re-extract phase names AFTER validation, in case validation modified format (unlikely)\n            # Or rely on the initial extraction based on checked types. Using initial extraction is fine.\n        except ValidationError as e:\n            raise ValidationError(f\"Invalid collaboration configuration: {str(e)}\")\n\n        try:\n            # Pass model_ids and phase_names for cross-reference validation\n            ConfigSchema.validate_aggregation_config(self.config[\"aggregation\"], phase_names, model_ids)\n        except ValidationError as e:\n            raise ValidationError(f\"Invalid aggregation configuration: {str(e)}\")\n\n\n        # Validate confidence configuration if provided\n        if \"confidence\" in self.config:\n             if not isinstance(self.config[\"confidence\"], dict):\n                  raise ValidationError(\"Configuration section 'confidence' must be a dictionary.\")\n             try:\n                 ConfigSchema.validate_confidence_config(self.config[\"confidence\"])\n             except ValidationError as e:\n                 raise ValidationError(f\"Invalid confidence configuration: {str(e)}\")\n\n        # Validate logging configuration if provided\n        if \"logging\" in self.config:\n             if not isinstance(self.config[\"logging\"], dict):\n                  raise ValidationError(\"Configuration section 'logging' must be a dictionary.\")\n             try:\n                 ConfigSchema.validate_logging_config(self.config[\"logging\"])\n             except ValidationError as e:\n                 raise ValidationError(f\"Invalid logging configuration: {str(e)}\")\n\n        # Validate templates: check if required templates exist\n        # Section existence/type checked, safe to access\n        try:\n            required_templates = self._get_required_templates()\n            ConfigSchema.validate_templates(self.config[\"templates\"], required_templates)\n        except ValidationError as e:\n            raise ValidationError(f\"Invalid templates configuration: {str(e)}\")\n\n        logger.debug(\"Configuration validation successful.\")\n\n\n    def _get_required_templates(self) -&gt; Set[str]:\n        \"\"\"Determine the set of template names required by the current configuration.\n\n        Scans collaboration phases and aggregation settings for template references.\n\n        Returns:\n            Set of required template names (strings).\n        \"\"\"\n        required_templates: Set[str] = set()\n\n        # --- Templates from Collaboration Phases ---\n        if \"collaboration\" in self.config and isinstance(self.config[\"collaboration\"], dict) and \\\n           \"phases\" in self.config[\"collaboration\"] and isinstance(self.config[\"collaboration\"][\"phases\"], list):\n            phases_list = self.config[\"collaboration\"][\"phases\"]\n            for phase in phases_list:\n                if isinstance(phase, dict):\n                    # Add all known template keys that might exist in a phase config\n                    template_keys_in_phase = [\n                        \"prompt_template\", \"initial_template\", \"branch_template\",\n                        \"evaluation_template\", \"critique_template\", \"improvement_template\",\n                        \"draft_template\", \"competitor_template\", \"perspective_template\",\n                        \"synthesis_template\"\n                    ]\n                    for key in template_keys_in_phase:\n                         template_name = phase.get(key)\n                         if isinstance(template_name, str) and template_name:\n                              required_templates.add(template_name)\n\n                    # Check within nested structures like workflow_steps or review_levels\n                    if \"workflow_steps\" in phase and isinstance(phase[\"workflow_steps\"], list):\n                        for step in phase[\"workflow_steps\"]:\n                            if isinstance(step, dict):\n                                 template_name = step.get(\"template\")\n                                 if isinstance(template_name, str) and template_name:\n                                     required_templates.add(template_name)\n                    if \"review_levels\" in phase and isinstance(phase[\"review_levels\"], list):\n                        for level in phase[\"review_levels\"]:\n                            if isinstance(level, dict):\n                                 template_name = level.get(\"template\")\n                                 if isinstance(template_name, str) and template_name:\n                                     required_templates.add(level[\"template\"])\n\n\n        # --- Templates from Aggregation Strategy ---\n        def collect_aggregation_templates(agg_conf: Dict[str, Any], req_set: Set[str]):\n            \"\"\"Helper to collect template refs from an aggregation config dict.\"\"\"\n            if not isinstance(agg_conf, dict): return\n\n            template_keys_in_agg = [\n                \"fusion_template\", \"evaluation_template\", \"selector_prompt_template\"\n            ]\n            for key in template_keys_in_agg:\n                 template_name = agg_conf.get(key)\n                 if isinstance(template_name, str) and template_name:\n                      req_set.add(template_name)\n\n            # Recursively check fallback\n            fallback_config = agg_conf.get(\"fallback\")\n            if isinstance(fallback_config, dict):\n                collect_aggregation_templates(fallback_config, req_set)\n\n            # Recursively check adaptive_selection sub-strategies\n            if agg_conf.get(\"strategy\") == \"adaptive_selection\":\n                sub_strategies = agg_conf.get(\"strategies\")\n                if isinstance(sub_strategies, dict):\n                     for sub_conf in sub_strategies.values():\n                         collect_aggregation_templates(sub_conf, req_set)\n\n        if \"aggregation\" in self.config and isinstance(self.config[\"aggregation\"], dict):\n             collect_aggregation_templates(self.config[\"aggregation\"], required_templates)\n\n\n        # --- Templates from Confidence Estimation ---\n        if \"confidence\" in self.config and isinstance(self.config[\"confidence\"], dict):\n            conf_method = self.config[\"confidence\"].get(\"default_method\")\n            needs_self_eval = False\n            if isinstance(conf_method, str) and conf_method in [\"self_eval\", \"combined\"]:\n                needs_self_eval = True\n            elif isinstance(conf_method, list) and \"self_eval\" in conf_method:\n                 needs_self_eval = True\n\n            if needs_self_eval:\n                 # Default name, or get from config if specified? Assume default for now.\n                 self_eval_template_name = self.config[\"confidence\"].get(\"self_eval_template\", \"self_evaluation\")\n                 if isinstance(self_eval_template_name, str) and self_eval_template_name:\n                      required_templates.add(self_eval_template_name)\n\n\n        logger.debug(f\"Required templates identified: {required_templates or 'None'}\")\n        return required_templates\n\n\n    # --- Getters and Setters ---\n\n    def get(\n        self,\n        key: str,\n        default: Any = None\n    ) -&gt; Any:\n        \"\"\"Get a configuration value, using dot notation for nested keys.\n\n        Args:\n            key: The configuration key (e.g., 'aggregation.strategy', 'models.model1.path').\n            default: Default value if the key is not found.\n\n        Returns:\n            A deep copy of the configuration value if it's a dict or list,\n            otherwise the value itself. Returns default if key not found.\n        \"\"\"\n        try:\n            if \".\" in key:\n                parts = key.split(\".\")\n                current = self.config\n                for part in parts:\n                    # Check if current level is a dict and part exists\n                    if not isinstance(current, dict) or part not in current:\n                        return default\n                    current = current[part]\n                # Return a deep copy for mutable types to prevent accidental modification\n                return copy.deepcopy(current) if isinstance(current, (dict, list)) else current\n            else:\n                 # Top-level key\n                 value = self.config.get(key, default)\n                 # Deep copy if mutable\n                 return copy.deepcopy(value) if isinstance(value, (dict, list)) else value\n        except Exception as e:\n             # Should not happen with .get and type checks, but safety first\n             logger.warning(f\"Error accessing config key '{key}': {e}. Returning default.\")\n             return default\n\n    def update(\n        self,\n        config_dict: Dict[str, Any]\n    ) -&gt; None:\n        \"\"\"Update the current configuration with values from config_dict.\n\n        Validates the configuration after merging. Reverts on validation failure.\n\n        Args:\n            config_dict: Dictionary containing configuration values to update.\n\n        Raises:\n            ConfigurationError: If config_dict is not a dictionary.\n            ValidationError: If validation fails after merging.\n        \"\"\"\n        if not isinstance(config_dict, dict):\n             raise ConfigurationError(\"Update value must be a dictionary.\")\n\n        # Store the current config in case validation fails and we need to revert\n        current_config_backup = copy.deepcopy(self.config)\n        logger.debug(\"Attempting configuration update...\")\n\n        try:\n            # Update the configuration recursively (modifies self.config)\n            self._update_config_recursive(self.config, config_dict)\n            # Validate the updated configuration\n            self.validate()\n            logger.info(\"Configuration updated and validated successfully.\")\n        except (ValidationError, ConfigurationError) as e: # Catch expected validation/merge errors\n            # Restore the previous configuration if validation fails\n            self.config = current_config_backup\n            logger.error(f\"Configuration update failed due to validation/merge error: {e}. Configuration reverted.\")\n            # Re-raise the error to signal failure\n            raise ValidationError(f\"Invalid configuration update: {str(e)}\") from e\n        except Exception as e:\n             # Catch unexpected errors during update/validation\n             self.config = current_config_backup\n             logger.error(f\"Unexpected error during configuration update: {e}. Configuration reverted.\", exc_info=True)\n             raise ConfigurationError(f\"Unexpected error applying configuration update: {str(e)}\")\n\n\n    def get_model_config(\n        self,\n        model_id: str\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Get the fully resolved configuration for a specific model, including defaults.\n\n        Args:\n            model_id: The ID of the model.\n\n        Returns:\n            Dictionary containing the model's merged configuration.\n\n        Raises:\n            ConfigurationError: If the model ID does not exist.\n        \"\"\"\n        # Ensure models section exists and model_id is present\n        if \"models\" not in self.config or not isinstance(self.config[\"models\"], dict) or \\\n           model_id not in self.config[\"models\"]:\n            raise ConfigurationError(f\"Model not found in configuration: '{model_id}'\")\n\n        # Get the model's specific config - deep copy first\n        model_specific_config = copy.deepcopy(self.config[\"models\"][model_id])\n        if not isinstance(model_specific_config, dict):\n             # Should be caught by validation, but safety check\n             raise ConfigurationError(f\"Configuration for model '{model_id}' is not a dictionary.\")\n\n        # Apply default parameters ('defaults.model_parameters')\n        default_params = self.get_default_model_parameters() # Gets a deep copy\n        if default_params:\n             # Ensure 'parameters' key exists in model_specific_config\n             if \"parameters\" not in model_specific_config or not isinstance(model_specific_config[\"parameters\"], dict):\n                 model_specific_config[\"parameters\"] = {}\n             # Apply defaults only if the parameter is NOT already set in the specific model's config\n             for param, value in default_params.items():\n                 model_specific_config[\"parameters\"].setdefault(param, value)\n\n        # Apply global model parameters ('defaults.global_model_parameters')\n        # These override defaults but can be overridden by specific model params\n        global_params = self.get(\"defaults.global_model_parameters\", {})\n        if isinstance(global_params, dict):\n            if \"parameters\" not in model_specific_config or not isinstance(model_specific_config[\"parameters\"], dict):\n                model_specific_config[\"parameters\"] = {}\n            # Update defaults with global, then update with specific\n            merged_params = {**default_params, **global_params, **model_specific_config[\"parameters\"]}\n            model_specific_config[\"parameters\"] = merged_params\n\n\n        return model_specific_config\n\n\n    def get_collaboration_config(\n        self,\n        phase_name: Optional[str] = None\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Get config for a specific collaboration phase or the entire collaboration section.\n\n        Args:\n            phase_name: Name of the phase, or None for the whole section.\n\n        Returns:\n            Deep copy of the requested configuration dictionary.\n\n        Raises:\n            ConfigurationError: If collaboration section is missing or phase not found.\n        \"\"\"\n        # Section existence/type checked during validation, access should be safe\n        collab_section = self.config[\"collaboration\"]\n\n        if phase_name is None:\n            # Return a deep copy of the entire collaboration configuration\n            return copy.deepcopy(collab_section)\n\n        # Find the phase with the specified name\n        phases_list = collab_section.get(\"phases\", []) # Should be a list due to validation\n        if isinstance(phases_list, list):\n             for phase in phases_list:\n                 # Should be dict with str name due to validation\n                 if isinstance(phase, dict) and phase.get(\"name\") == phase_name:\n                      # Return a deep copy of the phase config\n                     return copy.deepcopy(phase)\n\n        # Phase not found\n        raise ConfigurationError(f\"Phase not found in collaboration configuration: '{phase_name}'\")\n\n\n    def get_aggregation_config(\n        self,\n        strategy_name: Optional[str] = None\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Get config for a specific aggregation strategy or the entire aggregation section.\n\n        Handles lookup for active strategy, fallback, or adaptive sub-strategies.\n\n        Args:\n            strategy_name: Name of the strategy, or None for the whole section.\n\n        Returns:\n            Deep copy of the requested configuration dictionary.\n\n        Raises:\n            ConfigurationError: If aggregation section missing or strategy details not found.\n        \"\"\"\n        # Section existence/type checked during validation, access should be safe\n        agg_section = self.config[\"aggregation\"]\n\n        if strategy_name is None:\n            # Return a deep copy of the entire aggregation configuration\n            return copy.deepcopy(agg_section)\n\n        # Normalize the requested name\n        strategy_name_lower = strategy_name.lower()\n        active_strategy = agg_section.get(\"strategy\") # Should be str due to validation\n\n        # 1. Check if it matches the active strategy name\n        if isinstance(active_strategy, str) and strategy_name_lower == active_strategy.lower():\n            # Return the main config section, as it contains details for the active strategy\n            return copy.deepcopy(agg_section)\n\n        # 2. Check if it matches the fallback strategy name\n        fallback_config = agg_section.get(\"fallback\")\n        if isinstance(fallback_config, dict):\n            fallback_strategy_name = fallback_config.get(\"strategy\") # Should be str due to validation\n            if isinstance(fallback_strategy_name, str) and strategy_name_lower == fallback_strategy_name.lower():\n                # Return the fallback's specific config dictionary\n                return copy.deepcopy(fallback_config)\n\n        # 3. Check if it's a sub-strategy under adaptive_selection\n        if active_strategy == \"adaptive_selection\":\n            adaptive_strategies_dict = agg_section.get(\"strategies\") # Should be dict due to validation\n            if isinstance(adaptive_strategies_dict, dict):\n                 for sub_name, sub_config in adaptive_strategies_dict.items():\n                      if isinstance(sub_name, str) and strategy_name_lower == sub_name.lower():\n                           # Sub-config should be a dict due to validation\n                           config_copy = copy.deepcopy(sub_config) if isinstance(sub_config, dict) else {}\n                           # Ensure the strategy name is present in the returned dict for consistency\n                           config_copy.setdefault(\"strategy\", sub_name) # Use the key as name if missing\n                           return config_copy\n\n\n        # If not found in active, fallback, or adaptive sub-strategies\n        raise ConfigurationError(\n            f\"Configuration details not found for aggregation strategy: '{strategy_name}'. \"\n            f\"Checked active ('{active_strategy}'), fallback, and adaptive sub-strategies (if applicable).\"\n        )\n\n\n    def get_model_ids(self) -&gt; List[str]:\n        \"\"\"Get all model IDs defined in the configuration 'models' section.\n\n        Returns:\n            List of model ID strings. Returns empty list if 'models' is missing/invalid.\n        \"\"\"\n        models_section = self.config.get(\"models\")\n        if not isinstance(models_section, dict):\n            return []\n        return list(models_section.keys())\n\n\n    def get_all_models_config(self) -&gt; Dict[str, Dict[str, Any]]:\n        \"\"\"Get fully resolved configuration for all models, applying defaults.\n\n        Returns:\n            Dictionary mapping model IDs to their merged configurations.\n        \"\"\"\n        all_configs = {}\n        model_ids = self.get_model_ids() # Get list of valid model IDs\n\n        for model_id in model_ids:\n            try:\n                # get_model_config already handles defaults and returns a deep copy\n                all_configs[model_id] = self.get_model_config(model_id)\n            except ConfigurationError as e:\n                 # Should not happen if get_model_ids is consistent, but log if it does\n                 logger.warning(f\"Error retrieving config for model ID '{model_id}' listed in get_model_ids(): {e}\")\n                 continue # Skip this model\n\n        return all_configs\n\n    # --- Simple Getters for Top-Level Info ---\n\n    def get_collaboration_mode(self) -&gt; str:\n        \"\"\"Get the active collaboration mode string.\"\"\"\n        # Validation ensures this path exists and is a string\n        return self.config[\"collaboration\"][\"mode\"]\n\n    def get_aggregation_strategy(self) -&gt; str:\n        \"\"\"Get the name of the active aggregation strategy string.\"\"\"\n        # Validation ensures this path exists and is a string\n        return self.config[\"aggregation\"][\"strategy\"]\n\n    def get_default_model_parameters(self) -&gt; Dict[str, Any]:\n        \"\"\"Get default model parameters from 'defaults.model_parameters'.\"\"\"\n        # Use self.get for safe access, returns {} if path doesn't exist or not dict\n        params = self.get(\"defaults.model_parameters\", {})\n        # Ensure it's a dict before returning deep copy\n        return copy.deepcopy(params) if isinstance(params, dict) else {}\n\n    def get_template(self, template_name: str) -&gt; Optional[str]:\n        \"\"\"Get a specific prompt template string by name.\n\n        Args:\n            template_name: The name of the template.\n\n        Returns:\n            The template string, or None if not found or not a string.\n        \"\"\"\n        # Validation ensures 'templates' exists and is a dict\n        template_value = self.config[\"templates\"].get(template_name)\n\n        # Return only if it's a string\n        return template_value if isinstance(template_value, str) else None\n\n\n    def get_all_templates(self) -&gt; Dict[str, str]:\n        \"\"\"Get a dictionary of all configured templates.\"\"\"\n        # Validation ensures 'templates' exists and is a dict\n        # Return a deep copy to prevent modification\n        return copy.deepcopy(self.config[\"templates\"])\n\n\n    def get_confidence_config(self) -&gt; Dict[str, Any]:\n        \"\"\"Get the confidence estimation configuration section.\"\"\"\n        # Use self.get for safe access, returns {} if not found/not dict\n        conf_config = self.get(\"confidence\", {})\n        return copy.deepcopy(conf_config) if isinstance(conf_config, dict) else {}\n\n    def get_logging_config(self) -&gt; Dict[str, Any]:\n        \"\"\"Get the logging configuration section.\"\"\"\n        log_config = self.get(\"logging\", {})\n        return copy.deepcopy(log_config) if isinstance(log_config, dict) else {}\n\n\n    # --- Formatting Utility ---\n\n    # def format_prompt(self, template_name: str, **kwargs: Any) -&gt; str:\n    #     \"\"\"Format a prompt template using its name and provided values.\n    #\n    #     Args:\n    #         template_name: The name of the template to retrieve and format.\n    #         **kwargs: Values to substitute into the template placeholders.\n    #\n    #     Returns:\n    #         The formatted prompt string.\n    #\n    #     Raises:\n    #         ConfigurationError: If the template name does not exist.\n    #         ValidationError: If a required placeholder is missing in kwargs.\n    #     \"\"\"\n    #     template_string = self.get_template(template_name)\n    #     if template_string is None:\n    #         # Check if it exists at all vs having wrong type (get_template returns None for both)\n    #         if template_name not in self.config.get(\"templates\", {}):\n    #              raise ConfigurationError(f\"Prompt template '{template_name}' not found in configuration.\")\n    #         else:\n    #              raise ConfigurationError(f\"Template '{template_name}' is not a string in configuration.\")\n    #\n    #\n    #     try:\n    #         # Use the utility function, applying strict validation by default\n    #         # Pass strict=True to ensure errors are raised for missing keys\n    #         formatted = _format_prompt_util(template_string, strict=True, **kwargs)\n    #         return formatted\n    #     except ValidationError as e:\n    #         # Re-raise validation errors related to missing keys/format issues\n    #         raise ValidationError(f\"Error formatting template '{template_name}': {str(e)}\")\n    #     except Exception as e:\n    #          # Catch other potential formatting errors (less likely with strict=True)\n    #          logger.error(f\"Unexpected error formatting template '{template_name}': {e}\", exc_info=True)\n    #          raise ConfigurationError(f\"Unexpected error formatting template '{template_name}': {str(e)}\")\n\n    def get_full_config(self) -&gt; Dict[str, Any]:\n        \"\"\"Returns a deep copy of the entire current configuration dictionary.\"\"\"\n        # Assumes self.config exists and is a dictionary (which validation ensures)\n        # Return a deep copy to prevent external modification of the internal state.\n        return copy.deepcopy(self.config)\n</code></pre>"},{"location":"api/configuration/#ai_ensemble_suite.config.ConfigManager.__init__","title":"<code>__init__(config_path=None, config_dict=None)</code>","text":"<p>Initialize the ConfigManager.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>Optional[str]</code> <p>Path to a YAML configuration file.</p> <code>None</code> <code>config_dict</code> <code>Optional[Dict[str, Any]]</code> <p>Dictionary containing configuration values.</p> <code>None</code> <p>Raises:</p> Type Description <code>ConfigurationError</code> <p>If both config_path and config_dict are provided or if configuration is invalid.</p> Source code in <code>src\\ai_ensemble_suite\\config\\config_manager.py</code> <pre><code>def __init__(\n    self,\n    config_path: Optional[str] = None,\n    config_dict: Optional[Dict[str, Any]] = None\n) -&gt; None:\n    \"\"\"Initialize the ConfigManager.\n\n    Args:\n        config_path: Path to a YAML configuration file.\n        config_dict: Dictionary containing configuration values.\n\n    Raises:\n        ConfigurationError: If both config_path and config_dict are provided\n          or if configuration is invalid.\n    \"\"\"\n    self.config: Dict[str, Any] = {}\n\n    # Start with default configuration\n    self._load_defaults()\n\n    # Load configuration if provided\n    try:\n        if config_path or config_dict:\n            self.load(config_path, config_dict)\n    except (ConfigurationError, ValidationError) as e:\n         # Catch validation errors during initial load and re-raise\n         logger.error(f\"Initial configuration loading failed: {e}\", exc_info=True)\n         raise ConfigurationError(f\"Initial configuration loading failed: {e}\") from e\n</code></pre>"},{"location":"api/configuration/#ai_ensemble_suite.config.ConfigManager.get","title":"<code>get(key, default=None)</code>","text":"<p>Get a configuration value, using dot notation for nested keys.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The configuration key (e.g., 'aggregation.strategy', 'models.model1.path').</p> required <code>default</code> <code>Any</code> <p>Default value if the key is not found.</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>A deep copy of the configuration value if it's a dict or list,</p> <code>Any</code> <p>otherwise the value itself. Returns default if key not found.</p> Source code in <code>src\\ai_ensemble_suite\\config\\config_manager.py</code> <pre><code>def get(\n    self,\n    key: str,\n    default: Any = None\n) -&gt; Any:\n    \"\"\"Get a configuration value, using dot notation for nested keys.\n\n    Args:\n        key: The configuration key (e.g., 'aggregation.strategy', 'models.model1.path').\n        default: Default value if the key is not found.\n\n    Returns:\n        A deep copy of the configuration value if it's a dict or list,\n        otherwise the value itself. Returns default if key not found.\n    \"\"\"\n    try:\n        if \".\" in key:\n            parts = key.split(\".\")\n            current = self.config\n            for part in parts:\n                # Check if current level is a dict and part exists\n                if not isinstance(current, dict) or part not in current:\n                    return default\n                current = current[part]\n            # Return a deep copy for mutable types to prevent accidental modification\n            return copy.deepcopy(current) if isinstance(current, (dict, list)) else current\n        else:\n             # Top-level key\n             value = self.config.get(key, default)\n             # Deep copy if mutable\n             return copy.deepcopy(value) if isinstance(value, (dict, list)) else value\n    except Exception as e:\n         # Should not happen with .get and type checks, but safety first\n         logger.warning(f\"Error accessing config key '{key}': {e}. Returning default.\")\n         return default\n</code></pre>"},{"location":"api/configuration/#ai_ensemble_suite.config.ConfigManager.get_aggregation_config","title":"<code>get_aggregation_config(strategy_name=None)</code>","text":"<p>Get config for a specific aggregation strategy or the entire aggregation section.</p> <p>Handles lookup for active strategy, fallback, or adaptive sub-strategies.</p> <p>Parameters:</p> Name Type Description Default <code>strategy_name</code> <code>Optional[str]</code> <p>Name of the strategy, or None for the whole section.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Deep copy of the requested configuration dictionary.</p> <p>Raises:</p> Type Description <code>ConfigurationError</code> <p>If aggregation section missing or strategy details not found.</p> Source code in <code>src\\ai_ensemble_suite\\config\\config_manager.py</code> <pre><code>def get_aggregation_config(\n    self,\n    strategy_name: Optional[str] = None\n) -&gt; Dict[str, Any]:\n    \"\"\"Get config for a specific aggregation strategy or the entire aggregation section.\n\n    Handles lookup for active strategy, fallback, or adaptive sub-strategies.\n\n    Args:\n        strategy_name: Name of the strategy, or None for the whole section.\n\n    Returns:\n        Deep copy of the requested configuration dictionary.\n\n    Raises:\n        ConfigurationError: If aggregation section missing or strategy details not found.\n    \"\"\"\n    # Section existence/type checked during validation, access should be safe\n    agg_section = self.config[\"aggregation\"]\n\n    if strategy_name is None:\n        # Return a deep copy of the entire aggregation configuration\n        return copy.deepcopy(agg_section)\n\n    # Normalize the requested name\n    strategy_name_lower = strategy_name.lower()\n    active_strategy = agg_section.get(\"strategy\") # Should be str due to validation\n\n    # 1. Check if it matches the active strategy name\n    if isinstance(active_strategy, str) and strategy_name_lower == active_strategy.lower():\n        # Return the main config section, as it contains details for the active strategy\n        return copy.deepcopy(agg_section)\n\n    # 2. Check if it matches the fallback strategy name\n    fallback_config = agg_section.get(\"fallback\")\n    if isinstance(fallback_config, dict):\n        fallback_strategy_name = fallback_config.get(\"strategy\") # Should be str due to validation\n        if isinstance(fallback_strategy_name, str) and strategy_name_lower == fallback_strategy_name.lower():\n            # Return the fallback's specific config dictionary\n            return copy.deepcopy(fallback_config)\n\n    # 3. Check if it's a sub-strategy under adaptive_selection\n    if active_strategy == \"adaptive_selection\":\n        adaptive_strategies_dict = agg_section.get(\"strategies\") # Should be dict due to validation\n        if isinstance(adaptive_strategies_dict, dict):\n             for sub_name, sub_config in adaptive_strategies_dict.items():\n                  if isinstance(sub_name, str) and strategy_name_lower == sub_name.lower():\n                       # Sub-config should be a dict due to validation\n                       config_copy = copy.deepcopy(sub_config) if isinstance(sub_config, dict) else {}\n                       # Ensure the strategy name is present in the returned dict for consistency\n                       config_copy.setdefault(\"strategy\", sub_name) # Use the key as name if missing\n                       return config_copy\n\n\n    # If not found in active, fallback, or adaptive sub-strategies\n    raise ConfigurationError(\n        f\"Configuration details not found for aggregation strategy: '{strategy_name}'. \"\n        f\"Checked active ('{active_strategy}'), fallback, and adaptive sub-strategies (if applicable).\"\n    )\n</code></pre>"},{"location":"api/configuration/#ai_ensemble_suite.config.ConfigManager.get_aggregation_strategy","title":"<code>get_aggregation_strategy()</code>","text":"<p>Get the name of the active aggregation strategy string.</p> Source code in <code>src\\ai_ensemble_suite\\config\\config_manager.py</code> <pre><code>def get_aggregation_strategy(self) -&gt; str:\n    \"\"\"Get the name of the active aggregation strategy string.\"\"\"\n    # Validation ensures this path exists and is a string\n    return self.config[\"aggregation\"][\"strategy\"]\n</code></pre>"},{"location":"api/configuration/#ai_ensemble_suite.config.ConfigManager.get_all_models_config","title":"<code>get_all_models_config()</code>","text":"<p>Get fully resolved configuration for all models, applying defaults.</p> <p>Returns:</p> Type Description <code>Dict[str, Dict[str, Any]]</code> <p>Dictionary mapping model IDs to their merged configurations.</p> Source code in <code>src\\ai_ensemble_suite\\config\\config_manager.py</code> <pre><code>def get_all_models_config(self) -&gt; Dict[str, Dict[str, Any]]:\n    \"\"\"Get fully resolved configuration for all models, applying defaults.\n\n    Returns:\n        Dictionary mapping model IDs to their merged configurations.\n    \"\"\"\n    all_configs = {}\n    model_ids = self.get_model_ids() # Get list of valid model IDs\n\n    for model_id in model_ids:\n        try:\n            # get_model_config already handles defaults and returns a deep copy\n            all_configs[model_id] = self.get_model_config(model_id)\n        except ConfigurationError as e:\n             # Should not happen if get_model_ids is consistent, but log if it does\n             logger.warning(f\"Error retrieving config for model ID '{model_id}' listed in get_model_ids(): {e}\")\n             continue # Skip this model\n\n    return all_configs\n</code></pre>"},{"location":"api/configuration/#ai_ensemble_suite.config.ConfigManager.get_all_templates","title":"<code>get_all_templates()</code>","text":"<p>Get a dictionary of all configured templates.</p> Source code in <code>src\\ai_ensemble_suite\\config\\config_manager.py</code> <pre><code>def get_all_templates(self) -&gt; Dict[str, str]:\n    \"\"\"Get a dictionary of all configured templates.\"\"\"\n    # Validation ensures 'templates' exists and is a dict\n    # Return a deep copy to prevent modification\n    return copy.deepcopy(self.config[\"templates\"])\n</code></pre>"},{"location":"api/configuration/#ai_ensemble_suite.config.ConfigManager.get_collaboration_config","title":"<code>get_collaboration_config(phase_name=None)</code>","text":"<p>Get config for a specific collaboration phase or the entire collaboration section.</p> <p>Parameters:</p> Name Type Description Default <code>phase_name</code> <code>Optional[str]</code> <p>Name of the phase, or None for the whole section.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Deep copy of the requested configuration dictionary.</p> <p>Raises:</p> Type Description <code>ConfigurationError</code> <p>If collaboration section is missing or phase not found.</p> Source code in <code>src\\ai_ensemble_suite\\config\\config_manager.py</code> <pre><code>def get_collaboration_config(\n    self,\n    phase_name: Optional[str] = None\n) -&gt; Dict[str, Any]:\n    \"\"\"Get config for a specific collaboration phase or the entire collaboration section.\n\n    Args:\n        phase_name: Name of the phase, or None for the whole section.\n\n    Returns:\n        Deep copy of the requested configuration dictionary.\n\n    Raises:\n        ConfigurationError: If collaboration section is missing or phase not found.\n    \"\"\"\n    # Section existence/type checked during validation, access should be safe\n    collab_section = self.config[\"collaboration\"]\n\n    if phase_name is None:\n        # Return a deep copy of the entire collaboration configuration\n        return copy.deepcopy(collab_section)\n\n    # Find the phase with the specified name\n    phases_list = collab_section.get(\"phases\", []) # Should be a list due to validation\n    if isinstance(phases_list, list):\n         for phase in phases_list:\n             # Should be dict with str name due to validation\n             if isinstance(phase, dict) and phase.get(\"name\") == phase_name:\n                  # Return a deep copy of the phase config\n                 return copy.deepcopy(phase)\n\n    # Phase not found\n    raise ConfigurationError(f\"Phase not found in collaboration configuration: '{phase_name}'\")\n</code></pre>"},{"location":"api/configuration/#ai_ensemble_suite.config.ConfigManager.get_collaboration_mode","title":"<code>get_collaboration_mode()</code>","text":"<p>Get the active collaboration mode string.</p> Source code in <code>src\\ai_ensemble_suite\\config\\config_manager.py</code> <pre><code>def get_collaboration_mode(self) -&gt; str:\n    \"\"\"Get the active collaboration mode string.\"\"\"\n    # Validation ensures this path exists and is a string\n    return self.config[\"collaboration\"][\"mode\"]\n</code></pre>"},{"location":"api/configuration/#ai_ensemble_suite.config.ConfigManager.get_confidence_config","title":"<code>get_confidence_config()</code>","text":"<p>Get the confidence estimation configuration section.</p> Source code in <code>src\\ai_ensemble_suite\\config\\config_manager.py</code> <pre><code>def get_confidence_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Get the confidence estimation configuration section.\"\"\"\n    # Use self.get for safe access, returns {} if not found/not dict\n    conf_config = self.get(\"confidence\", {})\n    return copy.deepcopy(conf_config) if isinstance(conf_config, dict) else {}\n</code></pre>"},{"location":"api/configuration/#ai_ensemble_suite.config.ConfigManager.get_default_model_parameters","title":"<code>get_default_model_parameters()</code>","text":"<p>Get default model parameters from 'defaults.model_parameters'.</p> Source code in <code>src\\ai_ensemble_suite\\config\\config_manager.py</code> <pre><code>def get_default_model_parameters(self) -&gt; Dict[str, Any]:\n    \"\"\"Get default model parameters from 'defaults.model_parameters'.\"\"\"\n    # Use self.get for safe access, returns {} if path doesn't exist or not dict\n    params = self.get(\"defaults.model_parameters\", {})\n    # Ensure it's a dict before returning deep copy\n    return copy.deepcopy(params) if isinstance(params, dict) else {}\n</code></pre>"},{"location":"api/configuration/#ai_ensemble_suite.config.ConfigManager.get_full_config","title":"<code>get_full_config()</code>","text":"<p>Returns a deep copy of the entire current configuration dictionary.</p> Source code in <code>src\\ai_ensemble_suite\\config\\config_manager.py</code> <pre><code>def get_full_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns a deep copy of the entire current configuration dictionary.\"\"\"\n    # Assumes self.config exists and is a dictionary (which validation ensures)\n    # Return a deep copy to prevent external modification of the internal state.\n    return copy.deepcopy(self.config)\n</code></pre>"},{"location":"api/configuration/#ai_ensemble_suite.config.ConfigManager.get_logging_config","title":"<code>get_logging_config()</code>","text":"<p>Get the logging configuration section.</p> Source code in <code>src\\ai_ensemble_suite\\config\\config_manager.py</code> <pre><code>def get_logging_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Get the logging configuration section.\"\"\"\n    log_config = self.get(\"logging\", {})\n    return copy.deepcopy(log_config) if isinstance(log_config, dict) else {}\n</code></pre>"},{"location":"api/configuration/#ai_ensemble_suite.config.ConfigManager.get_model_config","title":"<code>get_model_config(model_id)</code>","text":"<p>Get the fully resolved configuration for a specific model, including defaults.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The ID of the model.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary containing the model's merged configuration.</p> <p>Raises:</p> Type Description <code>ConfigurationError</code> <p>If the model ID does not exist.</p> Source code in <code>src\\ai_ensemble_suite\\config\\config_manager.py</code> <pre><code>def get_model_config(\n    self,\n    model_id: str\n) -&gt; Dict[str, Any]:\n    \"\"\"Get the fully resolved configuration for a specific model, including defaults.\n\n    Args:\n        model_id: The ID of the model.\n\n    Returns:\n        Dictionary containing the model's merged configuration.\n\n    Raises:\n        ConfigurationError: If the model ID does not exist.\n    \"\"\"\n    # Ensure models section exists and model_id is present\n    if \"models\" not in self.config or not isinstance(self.config[\"models\"], dict) or \\\n       model_id not in self.config[\"models\"]:\n        raise ConfigurationError(f\"Model not found in configuration: '{model_id}'\")\n\n    # Get the model's specific config - deep copy first\n    model_specific_config = copy.deepcopy(self.config[\"models\"][model_id])\n    if not isinstance(model_specific_config, dict):\n         # Should be caught by validation, but safety check\n         raise ConfigurationError(f\"Configuration for model '{model_id}' is not a dictionary.\")\n\n    # Apply default parameters ('defaults.model_parameters')\n    default_params = self.get_default_model_parameters() # Gets a deep copy\n    if default_params:\n         # Ensure 'parameters' key exists in model_specific_config\n         if \"parameters\" not in model_specific_config or not isinstance(model_specific_config[\"parameters\"], dict):\n             model_specific_config[\"parameters\"] = {}\n         # Apply defaults only if the parameter is NOT already set in the specific model's config\n         for param, value in default_params.items():\n             model_specific_config[\"parameters\"].setdefault(param, value)\n\n    # Apply global model parameters ('defaults.global_model_parameters')\n    # These override defaults but can be overridden by specific model params\n    global_params = self.get(\"defaults.global_model_parameters\", {})\n    if isinstance(global_params, dict):\n        if \"parameters\" not in model_specific_config or not isinstance(model_specific_config[\"parameters\"], dict):\n            model_specific_config[\"parameters\"] = {}\n        # Update defaults with global, then update with specific\n        merged_params = {**default_params, **global_params, **model_specific_config[\"parameters\"]}\n        model_specific_config[\"parameters\"] = merged_params\n\n\n    return model_specific_config\n</code></pre>"},{"location":"api/configuration/#ai_ensemble_suite.config.ConfigManager.get_model_ids","title":"<code>get_model_ids()</code>","text":"<p>Get all model IDs defined in the configuration 'models' section.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of model ID strings. Returns empty list if 'models' is missing/invalid.</p> Source code in <code>src\\ai_ensemble_suite\\config\\config_manager.py</code> <pre><code>def get_model_ids(self) -&gt; List[str]:\n    \"\"\"Get all model IDs defined in the configuration 'models' section.\n\n    Returns:\n        List of model ID strings. Returns empty list if 'models' is missing/invalid.\n    \"\"\"\n    models_section = self.config.get(\"models\")\n    if not isinstance(models_section, dict):\n        return []\n    return list(models_section.keys())\n</code></pre>"},{"location":"api/configuration/#ai_ensemble_suite.config.ConfigManager.get_template","title":"<code>get_template(template_name)</code>","text":"<p>Get a specific prompt template string by name.</p> <p>Parameters:</p> Name Type Description Default <code>template_name</code> <code>str</code> <p>The name of the template.</p> required <p>Returns:</p> Type Description <code>Optional[str]</code> <p>The template string, or None if not found or not a string.</p> Source code in <code>src\\ai_ensemble_suite\\config\\config_manager.py</code> <pre><code>def get_template(self, template_name: str) -&gt; Optional[str]:\n    \"\"\"Get a specific prompt template string by name.\n\n    Args:\n        template_name: The name of the template.\n\n    Returns:\n        The template string, or None if not found or not a string.\n    \"\"\"\n    # Validation ensures 'templates' exists and is a dict\n    template_value = self.config[\"templates\"].get(template_name)\n\n    # Return only if it's a string\n    return template_value if isinstance(template_value, str) else None\n</code></pre>"},{"location":"api/configuration/#ai_ensemble_suite.config.ConfigManager.load","title":"<code>load(config_path=None, config_dict=None)</code>","text":"<p>Load configuration from a file or dictionary, merging with defaults.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>Optional[str]</code> <p>Path to a YAML configuration file.</p> <code>None</code> <code>config_dict</code> <code>Optional[Dict[str, Any]]</code> <p>Dictionary containing configuration values.</p> <code>None</code> <p>Raises:</p> Type Description <code>ConfigurationError</code> <p>If both config_path and config_dict are provided, or if file loading/parsing fails.</p> <code>ValidationError</code> <p>If the merged configuration is invalid.</p> Source code in <code>src\\ai_ensemble_suite\\config\\config_manager.py</code> <pre><code>def load(\n    self,\n    config_path: Optional[str] = None,\n    config_dict: Optional[Dict[str, Any]] = None\n) -&gt; None:\n    \"\"\"Load configuration from a file or dictionary, merging with defaults.\n\n    Args:\n        config_path: Path to a YAML configuration file.\n        config_dict: Dictionary containing configuration values.\n\n    Raises:\n        ConfigurationError: If both config_path and config_dict are provided,\n          or if file loading/parsing fails.\n        ValidationError: If the merged configuration is invalid.\n    \"\"\"\n    if config_path and config_dict:\n        raise ConfigurationError(\n            \"Cannot specify both config_path and config_dict\"\n        )\n\n    user_config: Optional[Dict[str, Any]] = None\n\n    # Load from file if specified\n    if config_path:\n        try:\n            # Ensure path is absolute or resolved correctly relative to execution context\n            resolved_path = Path(config_path).resolve()\n            logger.debug(f\"Attempting to load configuration from: {resolved_path}\")\n            if not resolved_path.is_file():\n                raise FileNotFoundError(f\"Configuration file not found or is not a file at: {resolved_path}\")\n            with open(resolved_path, \"r\", encoding=\"utf-8\") as file:\n                loaded_content = yaml.safe_load(file)\n                if not isinstance(loaded_content, dict):\n                    # Allow empty file (None) but not non-dict types\n                    if loaded_content is not None:\n                         raise ConfigurationError(\"Configuration file content must be a YAML dictionary (mapping).\")\n                    user_config = {} # Treat empty file as empty config\n                else:\n                    user_config = loaded_content\n        except (yaml.YAMLError, FileNotFoundError, OSError) as e:\n            raise ConfigurationError(f\"Failed to load or parse configuration file '{config_path}': {str(e)}\")\n    # Use provided dictionary if specified\n    elif config_dict:\n        if not isinstance(config_dict, dict):\n            raise ConfigurationError(\"config_dict must be a dictionary\")\n        user_config = config_dict\n\n    # Merge user config into existing (default) config if loaded\n    if user_config is not None:\n        try:\n            # Deep update modifies self.config in place\n            self._update_config_recursive(self.config, user_config)\n            logger.debug(\"User configuration merged with defaults.\")\n        except Exception as e:\n             # Should not happen with basic dict merge, but safety catch\n             raise ConfigurationError(f\"Internal error merging configuration: {str(e)}\")\n\n    # Always validate the final merged configuration\n    try:\n        self.validate()\n        logger.info(\"Configuration loaded and validated successfully.\")\n    except ValidationError as e:\n        # If validation fails after loading/merging, it's a fatal config error.\n        # We don't revert here; the constructor or update method handles reversion.\n        logger.error(f\"Configuration validation failed: {e}\")\n        raise ValidationError(f\"Invalid configuration after loading/merging: {str(e)}\") from e\n</code></pre>"},{"location":"api/configuration/#ai_ensemble_suite.config.ConfigManager.update","title":"<code>update(config_dict)</code>","text":"<p>Update the current configuration with values from config_dict.</p> <p>Validates the configuration after merging. Reverts on validation failure.</p> <p>Parameters:</p> Name Type Description Default <code>config_dict</code> <code>Dict[str, Any]</code> <p>Dictionary containing configuration values to update.</p> required <p>Raises:</p> Type Description <code>ConfigurationError</code> <p>If config_dict is not a dictionary.</p> <code>ValidationError</code> <p>If validation fails after merging.</p> Source code in <code>src\\ai_ensemble_suite\\config\\config_manager.py</code> <pre><code>def update(\n    self,\n    config_dict: Dict[str, Any]\n) -&gt; None:\n    \"\"\"Update the current configuration with values from config_dict.\n\n    Validates the configuration after merging. Reverts on validation failure.\n\n    Args:\n        config_dict: Dictionary containing configuration values to update.\n\n    Raises:\n        ConfigurationError: If config_dict is not a dictionary.\n        ValidationError: If validation fails after merging.\n    \"\"\"\n    if not isinstance(config_dict, dict):\n         raise ConfigurationError(\"Update value must be a dictionary.\")\n\n    # Store the current config in case validation fails and we need to revert\n    current_config_backup = copy.deepcopy(self.config)\n    logger.debug(\"Attempting configuration update...\")\n\n    try:\n        # Update the configuration recursively (modifies self.config)\n        self._update_config_recursive(self.config, config_dict)\n        # Validate the updated configuration\n        self.validate()\n        logger.info(\"Configuration updated and validated successfully.\")\n    except (ValidationError, ConfigurationError) as e: # Catch expected validation/merge errors\n        # Restore the previous configuration if validation fails\n        self.config = current_config_backup\n        logger.error(f\"Configuration update failed due to validation/merge error: {e}. Configuration reverted.\")\n        # Re-raise the error to signal failure\n        raise ValidationError(f\"Invalid configuration update: {str(e)}\") from e\n    except Exception as e:\n         # Catch unexpected errors during update/validation\n         self.config = current_config_backup\n         logger.error(f\"Unexpected error during configuration update: {e}. Configuration reverted.\", exc_info=True)\n         raise ConfigurationError(f\"Unexpected error applying configuration update: {str(e)}\")\n</code></pre>"},{"location":"api/configuration/#ai_ensemble_suite.config.ConfigManager.validate","title":"<code>validate()</code>","text":"<p>Validate the current configuration against the schema.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>If the configuration is invalid.</p> Source code in <code>src\\ai_ensemble_suite\\config\\config_manager.py</code> <pre><code>def validate(self) -&gt; None:\n    \"\"\"Validate the current configuration against the schema.\n\n    Raises:\n        ValidationError: If the configuration is invalid.\n    \"\"\"\n    logger.debug(\"Validating configuration...\")\n\n    # Check for required top-level sections and their types\n    required_sections = {\n        \"models\": dict, # Models section is crucial even if empty\n        \"collaboration\": dict,\n        \"aggregation\": dict,\n        \"templates\": dict\n    }\n    for section, expected_type in required_sections.items():\n        if section not in self.config:\n            raise ValidationError(f\"Missing required configuration section: '{section}'\")\n        if not isinstance(self.config[section], expected_type):\n             raise ValidationError(f\"Configuration section '{section}' must be a {expected_type.__name__}.\")\n\n    # --- Get IDs and Names for Cross-Validation ---\n    # Get all model IDs\n    model_ids = set(self.get_model_ids()) # Uses self.config['models']\n\n    # Get all phase names\n    phase_names = set()\n    # Ensure collaboration and phases exist and are the correct type first\n    if \"collaboration\" in self.config and isinstance(self.config[\"collaboration\"], dict) and \\\n       \"phases\" in self.config[\"collaboration\"] and isinstance(self.config[\"collaboration\"][\"phases\"], list):\n        phases_list = self.config[\"collaboration\"][\"phases\"]\n        for phase in phases_list:\n            # Ensure phase is dict and has a string name\n            if isinstance(phase, dict) and \"name\" in phase and isinstance(phase[\"name\"], str):\n                phase_names.add(phase[\"name\"])\n            # else: Handled by collaboration config validation below\n\n    # --- Run Schema Validations ---\n    # Validate models configuration\n    # Models section existence/type already checked, safe to access\n    for model_id, model_config in self.config[\"models\"].items():\n        try:\n            ConfigSchema.validate_model_config(model_id, model_config)\n        except ValidationError as e:\n            raise ValidationError(f\"Invalid model configuration for '{model_id}': {str(e)}\")\n\n    # Validate collaboration configuration\n    # Section existence/type checked, safe to access\n    try:\n        ConfigSchema.validate_collaboration_config(self.config[\"collaboration\"], model_ids)\n        # Re-extract phase names AFTER validation, in case validation modified format (unlikely)\n        # Or rely on the initial extraction based on checked types. Using initial extraction is fine.\n    except ValidationError as e:\n        raise ValidationError(f\"Invalid collaboration configuration: {str(e)}\")\n\n    try:\n        # Pass model_ids and phase_names for cross-reference validation\n        ConfigSchema.validate_aggregation_config(self.config[\"aggregation\"], phase_names, model_ids)\n    except ValidationError as e:\n        raise ValidationError(f\"Invalid aggregation configuration: {str(e)}\")\n\n\n    # Validate confidence configuration if provided\n    if \"confidence\" in self.config:\n         if not isinstance(self.config[\"confidence\"], dict):\n              raise ValidationError(\"Configuration section 'confidence' must be a dictionary.\")\n         try:\n             ConfigSchema.validate_confidence_config(self.config[\"confidence\"])\n         except ValidationError as e:\n             raise ValidationError(f\"Invalid confidence configuration: {str(e)}\")\n\n    # Validate logging configuration if provided\n    if \"logging\" in self.config:\n         if not isinstance(self.config[\"logging\"], dict):\n              raise ValidationError(\"Configuration section 'logging' must be a dictionary.\")\n         try:\n             ConfigSchema.validate_logging_config(self.config[\"logging\"])\n         except ValidationError as e:\n             raise ValidationError(f\"Invalid logging configuration: {str(e)}\")\n\n    # Validate templates: check if required templates exist\n    # Section existence/type checked, safe to access\n    try:\n        required_templates = self._get_required_templates()\n        ConfigSchema.validate_templates(self.config[\"templates\"], required_templates)\n    except ValidationError as e:\n        raise ValidationError(f\"Invalid templates configuration: {str(e)}\")\n\n    logger.debug(\"Configuration validation successful.\")\n</code></pre>"},{"location":"api/ensemble/","title":"Ensemble API","text":"<p>API reference for the Ensemble module. </p> <p>Main Ensemble class for ai-ensemble-suite.</p>"},{"location":"api/ensemble/#ai_ensemble_suite.ensemble.Ensemble","title":"<code>Ensemble</code>","text":"<p>Coordinates the collaboration of multiple AI models for complex tasks.</p> <p>Manages configuration, model loading, phase execution, and result aggregation based on defined collaboration modes and aggregation strategies.</p> Source code in <code>src\\ai_ensemble_suite\\ensemble.py</code> <pre><code>class Ensemble:\n    \"\"\"Coordinates the collaboration of multiple AI models for complex tasks.\n\n    Manages configuration, model loading, phase execution, and result aggregation\n    based on defined collaboration modes and aggregation strategies.\n    \"\"\"\n\n    # Registry for collaboration phase types (maps lowercase type name to class)\n    # Registry for collaboration phase types (maps lowercase type name to class)\n    _COLLABORATION_TYPES: Dict[str, Type[BaseCollaborationPhase]] = {\n        # Simple Phases\n        \"async_thinking\": AsyncThinking,\n        \"integration\": Integration,\n        \"expert_committee\": ExpertCommittee,\n        \"hierarchical_review\": HierarchicalReview,\n\n        # More Complex Phases\n        \"competitive_evaluation\": CompetitiveEvaluation,\n        \"perspective_rotation\": PerspectiveRotation,\n        \"chain_of_thought\": ChainOfThoughtBranching,  # Correct class name confirmed\n        \"adversarial_improvement\": AdversarialImprovement,\n        \"role_based_workflow\": RoleBasedWorkflow,\n\n        # Debate (needs special handling via subtype)\n        \"structured_debate\": BaseDebate,  # Base class maps here, subtype determines actual class\n\n        # Direct mapping for debate subtypes for clarity/flexibility in config\n        \"critique\": StructuredCritique,  # Can be used directly or via structured_debate subtype\n        \"synthesis\": SynthesisOriented,  # Can be used directly or via ...\n        \"role_based_debate\": RoleBasedDebate,  # Can be used directly or via ...\n\n        # Add this line to include Bagging\n        \"bagging\": Bagging,\n        \"uncertainty_based\": UncertaintyBasedCollaboration,\n        \"stacked_generalization\": StackedGeneralization,\n    }\n\n    # Specific mapping for structured_debate subtypes (used by _get_phase_class)\n    _DEBATE_SUBTYPES: Dict[str, Type[BaseDebate]] = {\n        \"critique\": StructuredCritique,\n        \"synthesis\": SynthesisOriented,\n        \"role_based_debate\": RoleBasedDebate,\n    }\n\n    # Registry for aggregation strategy types (maps lowercase type name to class)\n    _AGGREGATION_TYPES: Dict[str, Type[BaseAggregator]] = {\n        \"weighted_voting\": WeightedVoting,\n        \"sequential_refinement\": SequentialRefinement,\n        \"confidence_based\": ConfidenceBased,\n        \"multidimensional_voting\": MultidimensionalVoting,\n        \"ensemble_fusion\": EnsembleFusion,\n        \"adaptive_selection\": AdaptiveSelection,\n    }\n\n    def __init__(\n            self,\n            config_path: Optional[str] = None,\n            config_dict: Optional[Dict[str, Any]] = None\n    ) -&gt; None:\n        \"\"\"Initialize the Ensemble orchestration layer.\n\n        Args:\n            config_path: Optional path to a YAML configuration file.\n            config_dict: Optional dictionary containing configuration values.\n                         If both are None, default configuration is used.\n                         If both are provided, raises ConfigurationError.\n\n        Raises:\n            ConfigurationError: If configuration loading or validation fails,\n                                or if both config_path and config_dict are given.\n            AiEnsembleSuiteError: For other unexpected initialization errors.\n        \"\"\"\n        logger.info(\"Initializing Ensemble...\")\n        self.config_manager: ConfigManager\n        self.model_manager: ModelManager\n        self._initialized: bool = False\n        self._initialization_lock = asyncio.Lock()  # Lock for initialization process\n\n        try:\n            # Initialize the configuration manager\n            # ConfigManager handles defaults and loading priority internally\n            self.config_manager = ConfigManager(config_path, config_dict)\n            logger.info(\"ConfigManager initialized.\")\n\n            # Initialize the template manager\n            self.template_manager = TemplateManager(self.config_manager)\n            logger.info(\"TemplateManager initialized.\")\n\n            # Initialize the model manager, passing the config manager and self-reference\n            self.model_manager = ModelManager(self.config_manager, ensemble=self)\n            logger.info(\"ModelManager initialized.\")\n\n        except (ConfigurationError, ValidationError) as e:\n            logger.error(f\"Ensemble initialization failed due to configuration error: {e}\", exc_info=True)\n            raise ConfigurationError(f\"Configuration failed: {e}\") from e  # Re-raise specific config errors\n        except Exception as e:\n            logger.error(f\"Unexpected error during Ensemble setup: {e}\", exc_info=True)\n            # Wrap unexpected errors\n            raise AiEnsembleSuiteError(f\"Failed to set up Ensemble components: {e}\") from e\n\n        logger.info(\"Ensemble instance created. Call initialize() or use async context manager before 'ask'.\")\n\n    async def initialize(self) -&gt; None:\n        \"\"\"Load models and prepare the ensemble for processing queries.\n\n        This method is idempotent; it only performs initialization once.\n        Must be called before `ask` if not using the async context manager (`async with`).\n\n        Raises:\n            ModelError: If model loading fails during ModelManager initialization.\n            ConfigurationError: If configuration issues prevent model loading.\n            AiEnsembleSuiteError: For unexpected errors during initialization.\n        \"\"\"\n        async with self._initialization_lock:\n            if self._initialized:\n                logger.debug(\"Ensemble already initialized. Skipping.\")\n                return\n\n            logger.info(\"Initializing ensemble resources (loading models)...\")\n            init_start_time = time.time()\n\n            try:\n                # Initialize the model manager (this loads the models)\n                await self.model_manager.initialize()\n\n                self._initialized = True\n                init_duration = time.time() - init_start_time\n                logger.info(f\"Ensemble initialization complete in {init_duration:.2f}s. Ready for queries.\")\n\n            except (ModelError, ConfigurationError) as e:\n                # ModelManager.initialize handles its own logging and potential shutdown on failure\n                logger.error(f\"Ensemble initialization failed: {e}\")\n                self._initialized = False  # Ensure state reflects failure\n                raise  # Re-raise caught known errors\n            except Exception as e:\n                logger.error(f\"Unexpected error during ensemble initialization: {e}\", exc_info=True)\n                # Attempt graceful shutdown if possible, though model_manager might have already tried\n                try:\n                    if self.model_manager: await self.model_manager.shutdown()\n                except Exception as shutdown_e:\n                    logger.error(f\"Error during shutdown attempt after initialization failure: {shutdown_e}\")\n                self._initialized = False\n                raise AiEnsembleSuiteError(f\"Unexpected error during ensemble initialization: {e}\") from e\n\n    async def shutdown(self) -&gt; None:\n        \"\"\"Release resources used by the ensemble (unload models, shutdown threads).\n\n        This method is idempotent. Should be called when done with the ensemble\n        if not using the async context manager.\n        \"\"\"\n        # Use the lock to prevent race conditions with initialization or concurrent shutdowns\n        async with self._initialization_lock:\n            # Check if already effectively shut down or never initialized properly\n            if not self._initialized and (not hasattr(self,\n                                                      'model_manager') or self.model_manager is None or not self.model_manager.initialized):\n                logger.info(\"Ensemble already shut down or was not successfully initialized.\")\n                return\n\n            logger.info(\"Shutting down ensemble resources...\")\n            shutdown_start_time = time.time()\n\n            try:\n                # Shutdown the model manager (this unloads models and stops executor)\n                if hasattr(self, 'model_manager') and self.model_manager:\n                    await self.model_manager.shutdown()\n\n            except Exception as e:\n                # Log error but proceed to reset state\n                logger.error(f\"Error during ensemble shutdown: {e}\", exc_info=True)\n\n            finally:\n                # Reset state regardless of shutdown errors\n                self._initialized = False\n                # Setting managers to None might prevent reuse, decide based on desired behavior.\n                # Clearing might be safer if re-initialization with new config is possible.\n                # self.model_manager = None\n                # self.config_manager = None\n                shutdown_duration = time.time() - shutdown_start_time\n                logger.info(f\"Ensemble shutdown completed in {shutdown_duration:.2f}s.\")\n\n    async def ask(\n            self,\n            query: str,\n            **kwargs: Any\n    ) -&gt; Union[str, Dict[str, Any]]:\n        \"\"\"Process a query through the configured collaboration and aggregation pipeline.\n\n        Args:\n            query: The user query or task description string.\n            **kwargs: Optional overrides and parameters for the execution:\n                trace (bool): If True, return a detailed trace dictionary along\n                              with the response. Defaults to False.\n                collaboration_mode (str): Override the collaboration mode from config.\n                aggregation_strategy (str): Override the aggregation strategy from config.\n                # Context can also be passed via kwargs to provide initial state\n                initial_context (Dict[str, Any]): A dictionary to merge into the starting context.\n\n        Returns:\n            - If `trace=False` (default): The final aggregated response string.\n            - If `trace=True`: A dictionary containing:\n                - 'response': The final aggregated response string.\n                - 'trace': A dictionary containing detailed execution trace data.\n                - 'execution_time': Total time taken for the query processing.\n                - 'confidence': Aggregated confidence score (if available).\n\n        Raises:\n            ModelError: If model inference fails.\n            CollaborationError: If a collaboration phase encounters an error.\n            AggregationError: If the aggregation strategy fails.\n            ConfigurationError: If configuration is invalid or required elements missing.\n            AiEnsembleSuiteError: For other generic ensemble errors.\n        \"\"\"\n        if not self._initialized:\n            logger.info(\"Ensemble not initialized, attempting auto-initialization for 'ask'...\")\n            await self.initialize()  # Auto-initialize if needed (will raise if it fails)\n            if not self._initialized:  # Double check after attempt\n                raise AiEnsembleSuiteError(\"Ensemble initialization failed. Cannot process query.\")\n\n        # Start timing the overall request processing\n        start_time = time.time()\n\n        # Determine if tracing is requested for this call\n        include_trace = kwargs.get(\"trace\", False)\n\n        # Instantiate TraceCollector ONLY if tracing is enabled, passing config and logger\n        trace_collector: Optional[TraceCollector] = None  # Initialize as None first\n        if include_trace:\n            # Pass the configuration and the imported logger instance\n            trace_collector = TraceCollector(\n                logger_instance=logger  # Pass the imported logger\n            )\n            # Now start session if successfully created\n            trace_collector.start_session()\n            logger.debug(\"Tracing enabled for this request.\")\n\n        try:\n            logger.info(f\"Processing query (first 100 chars): '{query[:100]}{'...' if len(query) &gt; 100 else ''}'\")\n\n            # --- Set up Initial Context ---\n            # Start with the query, allow merging external context if provided\n            initial_context = {\"query\": query}\n            provided_context = kwargs.get(\"initial_context\")\n            if isinstance(provided_context, dict):\n                initial_context.update(provided_context)\n                logger.debug(f\"Merging provided initial context keys: {list(provided_context.keys())}\")\n\n            # --- Execute Collaboration Phases ---\n            # Pass query, initial context, potential overrides (**kwargs), and tracer\n            phase_outputs, final_context = await self._execute_collaboration_phases(\n                query, initial_context, trace_collector, **kwargs\n            )\n\n            # --- Aggregate Results ---\n            # Pass phase outputs, the final context from phases, tracer, and overrides\n            aggregation_result = await self._aggregate_results(\n                phase_outputs, final_context, trace_collector, **kwargs\n            )\n\n            # Extract final response text from aggregation result\n            response_text = aggregation_result.get(\"response\", \"\")\n            if not isinstance(response_text, str):\n                logger.warning(\n                    f\"Aggregation result 'response' is not a string (type: {type(response_text)}). Converting.\")\n                response_text = str(response_text)\n\n            if not response_text:\n                logger.warning(\"Aggregation resulted in an empty response.\")\n\n            # Calculate total execution time\n            execution_time = time.time() - start_time\n            logger.info(f\"Query processed successfully in {execution_time:.2f} seconds.\")\n\n            # Finalize trace session if tracing was enabled\n            if trace_collector:\n                # Create a snapshot of relevant configuration (avoiding secrets)\n                # Use the actual mode/strategy used, considering overrides\n                collab_mode_used = kwargs.get(\"collaboration_mode\", self.config_manager.get_collaboration_mode())\n                agg_strategy_used = kwargs.get(\"aggregation_strategy\", self.config_manager.get_aggregation_strategy())\n\n                config_snapshot = {\n                    \"collaboration_mode_used\": collab_mode_used,\n                    \"aggregation_strategy_used\": agg_strategy_used,\n                    \"model_ids_configured\": self.config_manager.get_model_ids(),\n                    \"phase_sequence_executed\": final_context.get(\"phase_sequence\", [])\n                    # Add other relevant high-level config keys if needed\n                }\n                trace_collector.add_session_trace(\n                    query=query,\n                    final_response=response_text,\n                    total_execution_time=execution_time,\n                    configuration=config_snapshot  # Use sanitized snapshot\n                )\n                trace_collector.end_session()\n                logger.debug(\"Trace session finalized.\")\n\n            # Return the final result (either text or dict with trace)\n            if include_trace and trace_collector:\n                return {\n                    \"response\": response_text,\n                    # Use get_trace_data() which includes stats\n                    \"trace\": trace_collector.get_trace_data(),\n                    \"execution_time\": execution_time,\n                    # Include top-level confidence from aggregation if available\n                    \"confidence\": aggregation_result.get(\"confidence\")\n                }\n            else:\n                # Default: return only the response string\n                return response_text\n\n        except (ModelError, CollaborationError, AggregationError, ConfigurationError, ValidationError) as e:\n            # Log specific known errors and re-raise them\n            logger.error(f\"Error processing query: {type(e).__name__}: {str(e)}\", exc_info=True)\n            if trace_collector: trace_collector.end_session()  # Ensure trace session ends on error\n            raise  # Re-raise the original error type\n        except Exception as e:\n            # Catch unexpected errors\n            logger.error(f\"Unexpected error processing query: {e}\", exc_info=True)\n            if trace_collector: trace_collector.end_session()\n            # Wrap in a generic ensemble error\n            raise AiEnsembleSuiteError(f\"Failed to process query due to unexpected error: {e}\") from e\n\n    async def _execute_collaboration_phases(\n            self,\n            query: str,\n            initial_context: Dict[str, Any],\n            trace_collector: Optional[TraceCollector] = None,\n            **kwargs: Any\n    ) -&gt; Tuple[Dict[str, Dict[str, Any]], Dict[str, Any]]:\n        \"\"\"Execute the sequence of collaboration phases defined in the configuration.\n\n        Args:\n            query: The user query.\n            initial_context: Starting context dictionary, including the query.\n            trace_collector: Optional trace collector for gathering execution details.\n            **kwargs: Optional configuration overrides (e.g., collaboration_mode).\n\n        Returns:\n            A tuple containing:\n            - Dictionary mapping phase names to their full output dictionaries.\n            - The final context dictionary after all phases have executed.\n\n        Raises:\n            CollaborationError: If any phase fails execution.\n            ConfigurationError: If phase configuration is invalid or missing.\n        \"\"\"\n        # Determine collaboration mode (allow override via kwargs)\n        collaboration_mode = kwargs.get(\n            \"collaboration_mode\",\n            self.config_manager.get_collaboration_mode()  # Get from config manager\n        )\n        logger.info(f\"Executing collaboration phases using mode: '{collaboration_mode}'\")\n\n        # Get phases configuration list\n        try:\n            # Get the full collaboration section config\n            collaboration_config = self.config_manager.get_collaboration_config()  # Gets entire section\n            phases_config_list = collaboration_config.get(\"phases\", [])\n\n            if not isinstance(phases_config_list, list):\n                raise ConfigurationError(f\"Config error: 'phases' must be a list in collaboration section.\")\n            if not phases_config_list:\n                logger.warning(\n                    f\"No phases defined in collaboration configuration for mode '{collaboration_mode}'. Collaboration stage will be skipped.\")\n                # Return empty outputs and the initial context if no phases\n                return {}, initial_context\n\n        except ConfigurationError as e:\n            logger.error(f\"Failed to retrieve collaboration phase configuration: {e}\")\n            raise  # Re-raise config error\n\n        # Check for circular dependencies between phases\n        dependency_graph = {}\n        for phase_config in phases_config_list:\n            if not isinstance(phase_config, dict):\n                continue\n\n            phase_name = phase_config.get(\"name\")\n            if not phase_name:\n                continue\n\n            input_from = phase_config.get(\"input_from\", [])\n            if isinstance(input_from, str):\n                input_from = [input_from]\n            elif not isinstance(input_from, list):\n                input_from = []\n\n            dependency_graph[phase_name] = input_from\n\n        # Detect circular dependencies\n        def detect_cycle(node: str, visited: Set[str], path: List[str]) -&gt; bool:\n            \"\"\"Recursive helper to detect cycles in the dependency graph.\"\"\"\n            if node in path:\n                cycle_path = path[path.index(node):] + [node]\n                raise CollaborationError(f\"Circular dependency detected in phases: {' -&gt; '.join(cycle_path)}\")\n\n            if node in visited:\n                return False\n\n            visited.add(node)\n            path.append(node)\n\n            for dep in dependency_graph.get(node, []):\n                if dep in dependency_graph:  # Only check dependencies that are actual phases\n                    detect_cycle(dep, visited, path)\n\n            path.pop()\n            return False\n\n        # Check each phase for cycles in its dependencies\n        visited: Set[str] = set()\n        for phase_name in dependency_graph:\n            if phase_name not in visited:\n                detect_cycle(phase_name, visited, [])\n\n        # Initialize execution variables\n        current_context: Dict[str, Any] = copy.deepcopy(initial_context)\n        phase_outputs: Dict[str, Dict[str, Any]] = {}  # Store results keyed by phase name\n        phase_sequence: List[str] = []  # Track execution order\n\n        # Execute each defined phase sequentially\n        for phase_config in phases_config_list:\n            if not isinstance(phase_config, dict):\n                logger.warning(f\"Skipping invalid phase config item (not a dict): {phase_config}\")\n                continue\n\n            # Validate essential phase config keys\n            phase_name = phase_config.get(\"name\")\n            phase_type = phase_config.get(\"type\")\n            if not phase_name or not isinstance(phase_name, str):\n                raise ConfigurationError(\n                    f\"Phase configuration item missing required string field: 'name'. Config: {phase_config}\")\n            if not phase_type or not isinstance(phase_type, str):\n                raise ConfigurationError(f\"Phase '{phase_name}' missing required string field: 'type'\")\n            if phase_name in phase_outputs:\n                raise ConfigurationError(f\"Duplicate phase name detected: '{phase_name}'. Phase names must be unique.\")\n\n            logger.info(f\"-- Executing Phase: '{phase_name}' (Type: '{phase_type}') --\")\n\n            # Get the appropriate phase class based on type and potentially subtype\n            try:\n                phase_class = self._get_phase_class(phase_type, phase_config)\n            except ConfigurationError as e:\n                logger.error(f\"Cannot execute phase '{phase_name}' due to configuration issue: {e}\")\n                # Wrap in CollaborationError to indicate phase execution failure point\n                raise CollaborationError(f\"Configuration error for phase '{phase_name}': {e}\") from e\n\n            # Instantiate and execute the phase\n            phase_instance: BaseCollaborationPhase\n            try:\n                # Phase constructor expects model_manager, config_manager, and phase_name\n                phase_instance = phase_class(\n                    model_manager=self.model_manager,\n                    config_manager=self.config_manager,\n                    phase_name=phase_name  # Pass name for config lookup within phase\n                )\n            except (ConfigurationError, ValidationError) as e:\n                logger.error(f\"Failed to initialize phase '{phase_name}' instance: {e}\", exc_info=True)\n                raise CollaborationError(f\"Initialization failed for phase '{phase_name}': {e}\") from e\n            except Exception as e:\n                logger.error(f\"Unexpected error initializing phase '{phase_name}' instance: {e}\", exc_info=True)\n                raise CollaborationError(f\"Unexpected initialization failure for phase '{phase_name}': {e}\") from e\n\n            try:\n                # Execute the phase, passing the current context\n                # Phase returns its result dictionary\n                phase_start_time = time.time()\n                phase_result = await phase_instance.execute(query, current_context, trace_collector)\n                phase_duration = time.time() - phase_start_time\n                logger.debug(f\"Phase '{phase_name}' executed in {phase_duration:.2f}s\")\n\n                # --- Update context and store results ---\n                if not isinstance(phase_result, dict):\n                    logger.warning(\n                        f\"Phase '{phase_name}' did not return a dictionary (returned {type(phase_result)}). Wrapping result.\")\n                    # Store something basic to avoid KeyError later\n                    phase_outputs[phase_name] = {\"output\": str(phase_result), \"confidence\": 0.0,\n                                                 \"error\": \"Invalid return type\"}\n                else:\n                    # Ensure core keys exist, provide defaults if missing\n                    phase_result.setdefault(\"output\", \"\")  # Expecting 'output' as primary text\n                    phase_result.setdefault(\"confidence\", 0.5)  # Default confidence\n                    phase_outputs[phase_name] = phase_result\n\n                # Update the main context with the *full results dict* of this phase, keyed by phase name\n                # This makes the output available to subsequent phases via _get_inputs_from_context\n                current_context[phase_name] = phase_outputs[phase_name]\n\n                # Add phase name to the execution sequence tracker\n                phase_sequence.append(phase_name)\n\n            except CollaborationError as e:\n                # Log phase-specific collaboration errors and re-raise\n                logger.error(f\"Execution failed for phase '{phase_name}': {e}\", exc_info=True)\n                # Re-raise to halt execution\n                raise CollaborationError(f\"Failed during execution of phase '{phase_name}': {e}\") from e\n            except Exception as e:\n                # Catch unexpected errors during phase execution\n                logger.error(f\"Unexpected error during phase '{phase_name}' execution: {e}\", exc_info=True)\n                raise CollaborationError(f\"Unexpected failure in phase '{phase_name}': {e}\") from e\n\n        # Add the final phase sequence to the context for reference (e.g., by aggregation)\n        current_context[\"phase_sequence\"] = phase_sequence\n\n        logger.info(\"Completed execution of all collaboration phases.\")\n        return phase_outputs, current_context\n\n    def _get_phase_class(\n            self,\n            phase_type: str,\n            phase_config: Dict[str, Any]  # Pass config for subtype lookup\n    ) -&gt; Type[BaseCollaborationPhase]:\n        \"\"\"Retrieve the collaboration phase class based on its type name.\n\n        Handles special cases like 'structured_debate' which uses a subtype.\n\n        Args:\n            phase_type: The type name of the phase (string).\n            phase_config: The configuration dictionary for the phase (used for subtype).\n\n        Returns:\n            The corresponding BaseCollaborationPhase subclass.\n\n        Raises:\n            ConfigurationError: If the phase type or subtype is unknown.\n        \"\"\"\n        phase_type_lower = phase_type.lower()\n\n        # Handle structured_debate with subtypes\n        if phase_type_lower == \"structured_debate\":\n            subtype = phase_config.get(\"subtype\")\n            if not subtype or not isinstance(subtype, str):\n                # If subtype is missing or invalid, default to critique\n                logger.warning(\n                    f\"Phase '{phase_config.get('name', 'unknown')}' is type 'structured_debate' but missing valid 'subtype'. Defaulting to 'critique'.\")\n                subtype = \"critique\"\n\n            subtype_lower = subtype.lower()\n            phase_class = self._DEBATE_SUBTYPES.get(subtype_lower)\n            if phase_class:\n                logger.debug(\n                    f\"Resolved 'structured_debate' with subtype '{subtype_lower}' to class {phase_class.__name__}\")\n                return phase_class\n            else:\n                known_subtypes = list(self._DEBATE_SUBTYPES.keys())\n                raise ConfigurationError(\n                    f\"Unknown structured_debate subtype: '{subtype}' for phase '{phase_config.get('name', 'unknown')}'. \"\n                    f\"Known subtypes: {known_subtypes}\"\n                )\n\n        # Handle direct mapping from type name for other phases\n        phase_class = self._COLLABORATION_TYPES.get(phase_type_lower)\n        if phase_class:\n            # If the type happens to be a base debate class but used directly, maybe guide user?\n            if phase_class == BaseDebate:\n                logger.warning(f\"Phase '{phase_config.get('name', 'unknown')}' uses 'structured_debate' directly. \"\n                               f\"Consider using a specific subtype (critique, synthesis, role_based) for clearer behavior.\")\n                # Default to critique if BaseDebate used directly?\n                return StructuredCritique\n\n            logger.debug(f\"Resolved phase type '{phase_type_lower}' to class {phase_class.__name__}\")\n            return phase_class\n        else:\n            # Type not found in any mapping\n            known_types = list(self._COLLABORATION_TYPES.keys())\n            raise ConfigurationError(\n                f\"Unknown collaboration phase type: '{phase_type}' for phase '{phase_config.get('name', 'unknown')}'. \"\n                f\"Known types: {known_types}\"\n            )\n\n    async def _aggregate_results(\n            self,\n            outputs: Dict[str, Dict[str, Any]],\n            context: Dict[str, Any],\n            trace_collector: Optional[TraceCollector] = None,\n            **kwargs: Any\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Aggregate the outputs from the collaboration phases using the configured strategy.\n\n        Args:\n            outputs: Dictionary mapping phase names to their full output dictionaries.\n            context: Dictionary containing context information from the ensemble run.\n            trace_collector: Optional trace collector for gathering execution details.\n            **kwargs: Optional overrides (e.g., aggregation_strategy).\n\n        Returns:\n            Dictionary containing the aggregated response and metadata.\n\n        Raises:\n            AggregationError: If the aggregation process fails.\n            ConfigurationError: If the aggregation strategy configuration is invalid.\n        \"\"\"\n        # Determine aggregation strategy (allow override via kwargs)\n        aggregation_strategy_name = kwargs.get(\n            \"aggregation_strategy\",\n            self.config_manager.get_aggregation_strategy()\n        ).lower()\n\n        logger.info(f\"Aggregating phase results using strategy: '{aggregation_strategy_name}'\")\n\n        # Get the aggregator class from the registry with improved error handling\n        aggregator_class = self._AGGREGATION_TYPES.get(aggregation_strategy_name)\n\n        if aggregator_class is None:\n            known_strategies = list(self._AGGREGATION_TYPES.keys())\n            error_msg = f\"Unknown aggregation strategy: '{aggregation_strategy_name}'. Known strategies: {known_strategies}\"\n            logger.error(error_msg)\n\n            # Check for fallback strategy in configuration\n            try:\n                aggregation_config = self.config_manager.get_aggregation_config()\n                fallback_config = aggregation_config.get(\"fallback\", {})\n                if isinstance(fallback_config, dict) and \"strategy\" in fallback_config:\n                    fallback_strategy = fallback_config[\"strategy\"].lower()\n                    fallback_class = self._AGGREGATION_TYPES.get(fallback_strategy)\n\n                    if fallback_class:\n                        logger.warning(\n                            f\"Using fallback strategy '{fallback_strategy}' instead of unknown '{aggregation_strategy_name}'\")\n                        aggregator_class = fallback_class\n                        aggregation_strategy_name = fallback_strategy\n            except Exception as fallback_e:\n                logger.error(f\"Error attempting to use fallback strategy: {fallback_e}\")\n\n            # Last resort fallback to sequential_refinement\n            if aggregator_class is None:\n                if \"sequential_refinement\" in self._AGGREGATION_TYPES:\n                    logger.warning(f\"Using 'sequential_refinement' as last-resort fallback\")\n                    aggregator_class = self._AGGREGATION_TYPES[\"sequential_refinement\"]\n                    aggregation_strategy_name = \"sequential_refinement\"\n                else:\n                    raise AggregationError(error_msg)\n\n        # Instantiate the aggregator\n        aggregator_instance: BaseAggregator\n        try:\n            # Pass ConfigManager, strategy name, and ModelManager\n            # ModelManager is needed by some strategies (e.g., evaluation, fusion, adaptive)\n            # strategy_config_override is handled internally by AdaptiveSelection if used\n            aggregator_instance = aggregator_class(\n                config_manager=self.config_manager,\n                strategy_name=aggregation_strategy_name,\n                model_manager=self.model_manager  # Pass manager instance\n            )\n        except (ConfigurationError, TypeError, ValidationError) as e:\n            logger.error(f\"Configuration or Type error instantiating aggregator '{aggregation_strategy_name}': {e}\",\n                         exc_info=True)\n            raise ConfigurationError(f\"Failed to create aggregator '{aggregation_strategy_name}': {e}\") from e\n        except Exception as e:\n            logger.error(f\"Unexpected error instantiating aggregator '{aggregation_strategy_name}': {e}\", exc_info=True)\n            raise AggregationError(\n                f\"Unexpected failure creating aggregator instance for '{aggregation_strategy_name}': {e}\") from e\n\n        # Execute the aggregation process\n        try:\n            # Pass phase outputs and the FINAL context from the collaboration stage\n            aggregation_result = await aggregator_instance.aggregate(\n                outputs, context, trace_collector\n            )\n\n            # Validate the result structure\n            if not isinstance(aggregation_result, dict) or \"response\" not in aggregation_result:\n                logger.error(\n                    f\"Aggregation strategy '{aggregation_strategy_name}' did not return a valid dictionary with a 'response' key. Result: {aggregation_result}\")\n                # Attempt a fallback based on the last phase output\n                fallback_response = \"Aggregation failed: No valid response generated.\"\n                fallback_confidence = 0.0\n                if context.get(\"phase_sequence\") and outputs:\n                    last_phase = context[\"phase_sequence\"][-1]\n                    if last_phase in outputs:\n                        response = outputs[last_phase].get(\"output\", fallback_response)\n                        fallback_response = str(response)  # Ensure string\n                        fallback_confidence = outputs[last_phase].get(\"confidence\", fallback_confidence)\n\n                return {\n                    \"response\": fallback_response,\n                    \"confidence\": fallback_confidence,\n                    \"error\": f\"Invalid result structure from aggregator '{aggregation_strategy_name}'\"\n                }\n\n            # Ensure confidence key exists, provide default if missing\n            aggregation_result.setdefault(\"confidence\", 0.5)  # Default confidence if strategy forgets\n\n            logger.info(f\"Aggregation completed using strategy '{aggregation_strategy_name}'.\")\n            return aggregation_result\n\n        except AggregationError as e:\n            # Log and re-raise known aggregation errors\n            logger.error(f\"Aggregation failed using strategy '{aggregation_strategy_name}': {e}\", exc_info=True)\n            raise  # Re-raise the original error\n        except Exception as e:\n            # Catch unexpected errors during aggregation execution\n            logger.error(f\"Unexpected error during aggregation with strategy '{aggregation_strategy_name}': {e}\",\n                         exc_info=True)\n            raise AggregationError(\n                f\"Aggregation strategy '{aggregation_strategy_name}' failed unexpectedly: {e}\") from e\n\n    def configure(\n            self,\n            config_dict: Dict[str, Any]\n    ) -&gt; None:\n        \"\"\"Update the ensemble's configuration dynamically.\n\n        Applies the provided dictionary on top of the existing configuration.\n        Re-validates the configuration after update. If validation fails,\n        the configuration change is reverted.\n\n        Note: This does NOT automatically reload models if model paths or parameters change.\n              A subsequent manual re-initialization (`await ensemble.shutdown(); await ensemble.initialize()`)\n              is typically required for model changes to take effect.\n\n        Args:\n            config_dict: Dictionary containing configuration values to update.\n\n        Raises:\n            ConfigurationError: If the update dict is not a dict or if the resulting\n                                configuration is invalid after merge.\n            ValidationError: If validation fails (subclass of ConfigurationError).\n        \"\"\"\n        if self._initialized:\n            logger.warning(\"Configuring Ensemble after initialization. This does NOT automatically reload models. \"\n                           \"Call shutdown() and initialize() again for model config changes to apply.\")\n\n        logger.info(\"Attempting to update ensemble configuration...\")\n        try:\n            # Use ConfigManager's update method which includes validation and rollback on failure\n            self.config_manager.update(config_dict)\n            # If successful, ModelManager implicitly uses the updated config on next access requiring config\n            logger.info(\"Ensemble configuration updated successfully.\")\n        except (ConfigurationError, ValidationError) as e:\n            # ConfigManager.update handles logging and rollback\n            # Re-raise the error\n            raise ConfigurationError(f\"Configuration update failed validation: {e}\") from e\n        except Exception as e:\n            logger.error(f\"Unexpected error during configuration update: {e}\", exc_info=True)\n            raise ConfigurationError(f\"Unexpected error applying configuration update: {e}\") from e\n\n    # --- Async Context Manager Support ---\n\n    async def __aenter__(self: T_Ensemble) -&gt; T_Ensemble:\n        \"\"\"Async context manager entry: Initialize the ensemble.\"\"\"\n        await self.initialize()\n        return self\n\n    async def __aexit__(\n            self,\n            exc_type: Optional[Type[BaseException]],\n            exc_val: Optional[BaseException],\n            exc_tb: Optional[TracebackType]\n    ) -&gt; None:\n        \"\"\"Async context manager exit: Shutdown the ensemble.\"\"\"\n        await self.shutdown()\n</code></pre>"},{"location":"api/ensemble/#ai_ensemble_suite.ensemble.Ensemble.__aenter__","title":"<code>__aenter__()</code>  <code>async</code>","text":"<p>Async context manager entry: Initialize the ensemble.</p> Source code in <code>src\\ai_ensemble_suite\\ensemble.py</code> <pre><code>async def __aenter__(self: T_Ensemble) -&gt; T_Ensemble:\n    \"\"\"Async context manager entry: Initialize the ensemble.\"\"\"\n    await self.initialize()\n    return self\n</code></pre>"},{"location":"api/ensemble/#ai_ensemble_suite.ensemble.Ensemble.__aexit__","title":"<code>__aexit__(exc_type, exc_val, exc_tb)</code>  <code>async</code>","text":"<p>Async context manager exit: Shutdown the ensemble.</p> Source code in <code>src\\ai_ensemble_suite\\ensemble.py</code> <pre><code>async def __aexit__(\n        self,\n        exc_type: Optional[Type[BaseException]],\n        exc_val: Optional[BaseException],\n        exc_tb: Optional[TracebackType]\n) -&gt; None:\n    \"\"\"Async context manager exit: Shutdown the ensemble.\"\"\"\n    await self.shutdown()\n</code></pre>"},{"location":"api/ensemble/#ai_ensemble_suite.ensemble.Ensemble.__init__","title":"<code>__init__(config_path=None, config_dict=None)</code>","text":"<p>Initialize the Ensemble orchestration layer.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>Optional[str]</code> <p>Optional path to a YAML configuration file.</p> <code>None</code> <code>config_dict</code> <code>Optional[Dict[str, Any]]</code> <p>Optional dictionary containing configuration values.          If both are None, default configuration is used.          If both are provided, raises ConfigurationError.</p> <code>None</code> <p>Raises:</p> Type Description <code>ConfigurationError</code> <p>If configuration loading or validation fails,                 or if both config_path and config_dict are given.</p> <code>AiEnsembleSuiteError</code> <p>For other unexpected initialization errors.</p> Source code in <code>src\\ai_ensemble_suite\\ensemble.py</code> <pre><code>def __init__(\n        self,\n        config_path: Optional[str] = None,\n        config_dict: Optional[Dict[str, Any]] = None\n) -&gt; None:\n    \"\"\"Initialize the Ensemble orchestration layer.\n\n    Args:\n        config_path: Optional path to a YAML configuration file.\n        config_dict: Optional dictionary containing configuration values.\n                     If both are None, default configuration is used.\n                     If both are provided, raises ConfigurationError.\n\n    Raises:\n        ConfigurationError: If configuration loading or validation fails,\n                            or if both config_path and config_dict are given.\n        AiEnsembleSuiteError: For other unexpected initialization errors.\n    \"\"\"\n    logger.info(\"Initializing Ensemble...\")\n    self.config_manager: ConfigManager\n    self.model_manager: ModelManager\n    self._initialized: bool = False\n    self._initialization_lock = asyncio.Lock()  # Lock for initialization process\n\n    try:\n        # Initialize the configuration manager\n        # ConfigManager handles defaults and loading priority internally\n        self.config_manager = ConfigManager(config_path, config_dict)\n        logger.info(\"ConfigManager initialized.\")\n\n        # Initialize the template manager\n        self.template_manager = TemplateManager(self.config_manager)\n        logger.info(\"TemplateManager initialized.\")\n\n        # Initialize the model manager, passing the config manager and self-reference\n        self.model_manager = ModelManager(self.config_manager, ensemble=self)\n        logger.info(\"ModelManager initialized.\")\n\n    except (ConfigurationError, ValidationError) as e:\n        logger.error(f\"Ensemble initialization failed due to configuration error: {e}\", exc_info=True)\n        raise ConfigurationError(f\"Configuration failed: {e}\") from e  # Re-raise specific config errors\n    except Exception as e:\n        logger.error(f\"Unexpected error during Ensemble setup: {e}\", exc_info=True)\n        # Wrap unexpected errors\n        raise AiEnsembleSuiteError(f\"Failed to set up Ensemble components: {e}\") from e\n\n    logger.info(\"Ensemble instance created. Call initialize() or use async context manager before 'ask'.\")\n</code></pre>"},{"location":"api/ensemble/#ai_ensemble_suite.ensemble.Ensemble.ask","title":"<code>ask(query, **kwargs)</code>  <code>async</code>","text":"<p>Process a query through the configured collaboration and aggregation pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The user query or task description string.</p> required <code>**kwargs</code> <code>Any</code> <p>Optional overrides and parameters for the execution: trace (bool): If True, return a detailed trace dictionary along               with the response. Defaults to False. collaboration_mode (str): Override the collaboration mode from config. aggregation_strategy (str): Override the aggregation strategy from config.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[str, Dict[str, Any]]</code> <ul> <li>If <code>trace=False</code> (default): The final aggregated response string.</li> </ul> <code>Union[str, Dict[str, Any]]</code> <ul> <li>If <code>trace=True</code>: A dictionary containing:</li> <li>'response': The final aggregated response string.</li> <li>'trace': A dictionary containing detailed execution trace data.</li> <li>'execution_time': Total time taken for the query processing.</li> <li>'confidence': Aggregated confidence score (if available).</li> </ul> <p>Raises:</p> Type Description <code>ModelError</code> <p>If model inference fails.</p> <code>CollaborationError</code> <p>If a collaboration phase encounters an error.</p> <code>AggregationError</code> <p>If the aggregation strategy fails.</p> <code>ConfigurationError</code> <p>If configuration is invalid or required elements missing.</p> <code>AiEnsembleSuiteError</code> <p>For other generic ensemble errors.</p> Source code in <code>src\\ai_ensemble_suite\\ensemble.py</code> <pre><code>async def ask(\n        self,\n        query: str,\n        **kwargs: Any\n) -&gt; Union[str, Dict[str, Any]]:\n    \"\"\"Process a query through the configured collaboration and aggregation pipeline.\n\n    Args:\n        query: The user query or task description string.\n        **kwargs: Optional overrides and parameters for the execution:\n            trace (bool): If True, return a detailed trace dictionary along\n                          with the response. Defaults to False.\n            collaboration_mode (str): Override the collaboration mode from config.\n            aggregation_strategy (str): Override the aggregation strategy from config.\n            # Context can also be passed via kwargs to provide initial state\n            initial_context (Dict[str, Any]): A dictionary to merge into the starting context.\n\n    Returns:\n        - If `trace=False` (default): The final aggregated response string.\n        - If `trace=True`: A dictionary containing:\n            - 'response': The final aggregated response string.\n            - 'trace': A dictionary containing detailed execution trace data.\n            - 'execution_time': Total time taken for the query processing.\n            - 'confidence': Aggregated confidence score (if available).\n\n    Raises:\n        ModelError: If model inference fails.\n        CollaborationError: If a collaboration phase encounters an error.\n        AggregationError: If the aggregation strategy fails.\n        ConfigurationError: If configuration is invalid or required elements missing.\n        AiEnsembleSuiteError: For other generic ensemble errors.\n    \"\"\"\n    if not self._initialized:\n        logger.info(\"Ensemble not initialized, attempting auto-initialization for 'ask'...\")\n        await self.initialize()  # Auto-initialize if needed (will raise if it fails)\n        if not self._initialized:  # Double check after attempt\n            raise AiEnsembleSuiteError(\"Ensemble initialization failed. Cannot process query.\")\n\n    # Start timing the overall request processing\n    start_time = time.time()\n\n    # Determine if tracing is requested for this call\n    include_trace = kwargs.get(\"trace\", False)\n\n    # Instantiate TraceCollector ONLY if tracing is enabled, passing config and logger\n    trace_collector: Optional[TraceCollector] = None  # Initialize as None first\n    if include_trace:\n        # Pass the configuration and the imported logger instance\n        trace_collector = TraceCollector(\n            logger_instance=logger  # Pass the imported logger\n        )\n        # Now start session if successfully created\n        trace_collector.start_session()\n        logger.debug(\"Tracing enabled for this request.\")\n\n    try:\n        logger.info(f\"Processing query (first 100 chars): '{query[:100]}{'...' if len(query) &gt; 100 else ''}'\")\n\n        # --- Set up Initial Context ---\n        # Start with the query, allow merging external context if provided\n        initial_context = {\"query\": query}\n        provided_context = kwargs.get(\"initial_context\")\n        if isinstance(provided_context, dict):\n            initial_context.update(provided_context)\n            logger.debug(f\"Merging provided initial context keys: {list(provided_context.keys())}\")\n\n        # --- Execute Collaboration Phases ---\n        # Pass query, initial context, potential overrides (**kwargs), and tracer\n        phase_outputs, final_context = await self._execute_collaboration_phases(\n            query, initial_context, trace_collector, **kwargs\n        )\n\n        # --- Aggregate Results ---\n        # Pass phase outputs, the final context from phases, tracer, and overrides\n        aggregation_result = await self._aggregate_results(\n            phase_outputs, final_context, trace_collector, **kwargs\n        )\n\n        # Extract final response text from aggregation result\n        response_text = aggregation_result.get(\"response\", \"\")\n        if not isinstance(response_text, str):\n            logger.warning(\n                f\"Aggregation result 'response' is not a string (type: {type(response_text)}). Converting.\")\n            response_text = str(response_text)\n\n        if not response_text:\n            logger.warning(\"Aggregation resulted in an empty response.\")\n\n        # Calculate total execution time\n        execution_time = time.time() - start_time\n        logger.info(f\"Query processed successfully in {execution_time:.2f} seconds.\")\n\n        # Finalize trace session if tracing was enabled\n        if trace_collector:\n            # Create a snapshot of relevant configuration (avoiding secrets)\n            # Use the actual mode/strategy used, considering overrides\n            collab_mode_used = kwargs.get(\"collaboration_mode\", self.config_manager.get_collaboration_mode())\n            agg_strategy_used = kwargs.get(\"aggregation_strategy\", self.config_manager.get_aggregation_strategy())\n\n            config_snapshot = {\n                \"collaboration_mode_used\": collab_mode_used,\n                \"aggregation_strategy_used\": agg_strategy_used,\n                \"model_ids_configured\": self.config_manager.get_model_ids(),\n                \"phase_sequence_executed\": final_context.get(\"phase_sequence\", [])\n                # Add other relevant high-level config keys if needed\n            }\n            trace_collector.add_session_trace(\n                query=query,\n                final_response=response_text,\n                total_execution_time=execution_time,\n                configuration=config_snapshot  # Use sanitized snapshot\n            )\n            trace_collector.end_session()\n            logger.debug(\"Trace session finalized.\")\n\n        # Return the final result (either text or dict with trace)\n        if include_trace and trace_collector:\n            return {\n                \"response\": response_text,\n                # Use get_trace_data() which includes stats\n                \"trace\": trace_collector.get_trace_data(),\n                \"execution_time\": execution_time,\n                # Include top-level confidence from aggregation if available\n                \"confidence\": aggregation_result.get(\"confidence\")\n            }\n        else:\n            # Default: return only the response string\n            return response_text\n\n    except (ModelError, CollaborationError, AggregationError, ConfigurationError, ValidationError) as e:\n        # Log specific known errors and re-raise them\n        logger.error(f\"Error processing query: {type(e).__name__}: {str(e)}\", exc_info=True)\n        if trace_collector: trace_collector.end_session()  # Ensure trace session ends on error\n        raise  # Re-raise the original error type\n    except Exception as e:\n        # Catch unexpected errors\n        logger.error(f\"Unexpected error processing query: {e}\", exc_info=True)\n        if trace_collector: trace_collector.end_session()\n        # Wrap in a generic ensemble error\n        raise AiEnsembleSuiteError(f\"Failed to process query due to unexpected error: {e}\") from e\n</code></pre>"},{"location":"api/ensemble/#ai_ensemble_suite.ensemble.Ensemble.ask--context-can-also-be-passed-via-kwargs-to-provide-initial-state","title":"Context can also be passed via kwargs to provide initial state","text":"<p>initial_context (Dict[str, Any]): A dictionary to merge into the starting context.</p>"},{"location":"api/ensemble/#ai_ensemble_suite.ensemble.Ensemble.configure","title":"<code>configure(config_dict)</code>","text":"<p>Update the ensemble's configuration dynamically.</p> <p>Applies the provided dictionary on top of the existing configuration. Re-validates the configuration after update. If validation fails, the configuration change is reverted.</p> This does NOT automatically reload models if model paths or parameters change. <p>A subsequent manual re-initialization (<code>await ensemble.shutdown(); await ensemble.initialize()</code>) is typically required for model changes to take effect.</p> <p>Parameters:</p> Name Type Description Default <code>config_dict</code> <code>Dict[str, Any]</code> <p>Dictionary containing configuration values to update.</p> required <p>Raises:</p> Type Description <code>ConfigurationError</code> <p>If the update dict is not a dict or if the resulting                 configuration is invalid after merge.</p> <code>ValidationError</code> <p>If validation fails (subclass of ConfigurationError).</p> Source code in <code>src\\ai_ensemble_suite\\ensemble.py</code> <pre><code>def configure(\n        self,\n        config_dict: Dict[str, Any]\n) -&gt; None:\n    \"\"\"Update the ensemble's configuration dynamically.\n\n    Applies the provided dictionary on top of the existing configuration.\n    Re-validates the configuration after update. If validation fails,\n    the configuration change is reverted.\n\n    Note: This does NOT automatically reload models if model paths or parameters change.\n          A subsequent manual re-initialization (`await ensemble.shutdown(); await ensemble.initialize()`)\n          is typically required for model changes to take effect.\n\n    Args:\n        config_dict: Dictionary containing configuration values to update.\n\n    Raises:\n        ConfigurationError: If the update dict is not a dict or if the resulting\n                            configuration is invalid after merge.\n        ValidationError: If validation fails (subclass of ConfigurationError).\n    \"\"\"\n    if self._initialized:\n        logger.warning(\"Configuring Ensemble after initialization. This does NOT automatically reload models. \"\n                       \"Call shutdown() and initialize() again for model config changes to apply.\")\n\n    logger.info(\"Attempting to update ensemble configuration...\")\n    try:\n        # Use ConfigManager's update method which includes validation and rollback on failure\n        self.config_manager.update(config_dict)\n        # If successful, ModelManager implicitly uses the updated config on next access requiring config\n        logger.info(\"Ensemble configuration updated successfully.\")\n    except (ConfigurationError, ValidationError) as e:\n        # ConfigManager.update handles logging and rollback\n        # Re-raise the error\n        raise ConfigurationError(f\"Configuration update failed validation: {e}\") from e\n    except Exception as e:\n        logger.error(f\"Unexpected error during configuration update: {e}\", exc_info=True)\n        raise ConfigurationError(f\"Unexpected error applying configuration update: {e}\") from e\n</code></pre>"},{"location":"api/ensemble/#ai_ensemble_suite.ensemble.Ensemble.initialize","title":"<code>initialize()</code>  <code>async</code>","text":"<p>Load models and prepare the ensemble for processing queries.</p> <p>This method is idempotent; it only performs initialization once. Must be called before <code>ask</code> if not using the async context manager (<code>async with</code>).</p> <p>Raises:</p> Type Description <code>ModelError</code> <p>If model loading fails during ModelManager initialization.</p> <code>ConfigurationError</code> <p>If configuration issues prevent model loading.</p> <code>AiEnsembleSuiteError</code> <p>For unexpected errors during initialization.</p> Source code in <code>src\\ai_ensemble_suite\\ensemble.py</code> <pre><code>async def initialize(self) -&gt; None:\n    \"\"\"Load models and prepare the ensemble for processing queries.\n\n    This method is idempotent; it only performs initialization once.\n    Must be called before `ask` if not using the async context manager (`async with`).\n\n    Raises:\n        ModelError: If model loading fails during ModelManager initialization.\n        ConfigurationError: If configuration issues prevent model loading.\n        AiEnsembleSuiteError: For unexpected errors during initialization.\n    \"\"\"\n    async with self._initialization_lock:\n        if self._initialized:\n            logger.debug(\"Ensemble already initialized. Skipping.\")\n            return\n\n        logger.info(\"Initializing ensemble resources (loading models)...\")\n        init_start_time = time.time()\n\n        try:\n            # Initialize the model manager (this loads the models)\n            await self.model_manager.initialize()\n\n            self._initialized = True\n            init_duration = time.time() - init_start_time\n            logger.info(f\"Ensemble initialization complete in {init_duration:.2f}s. Ready for queries.\")\n\n        except (ModelError, ConfigurationError) as e:\n            # ModelManager.initialize handles its own logging and potential shutdown on failure\n            logger.error(f\"Ensemble initialization failed: {e}\")\n            self._initialized = False  # Ensure state reflects failure\n            raise  # Re-raise caught known errors\n        except Exception as e:\n            logger.error(f\"Unexpected error during ensemble initialization: {e}\", exc_info=True)\n            # Attempt graceful shutdown if possible, though model_manager might have already tried\n            try:\n                if self.model_manager: await self.model_manager.shutdown()\n            except Exception as shutdown_e:\n                logger.error(f\"Error during shutdown attempt after initialization failure: {shutdown_e}\")\n            self._initialized = False\n            raise AiEnsembleSuiteError(f\"Unexpected error during ensemble initialization: {e}\") from e\n</code></pre>"},{"location":"api/ensemble/#ai_ensemble_suite.ensemble.Ensemble.shutdown","title":"<code>shutdown()</code>  <code>async</code>","text":"<p>Release resources used by the ensemble (unload models, shutdown threads).</p> <p>This method is idempotent. Should be called when done with the ensemble if not using the async context manager.</p> Source code in <code>src\\ai_ensemble_suite\\ensemble.py</code> <pre><code>async def shutdown(self) -&gt; None:\n    \"\"\"Release resources used by the ensemble (unload models, shutdown threads).\n\n    This method is idempotent. Should be called when done with the ensemble\n    if not using the async context manager.\n    \"\"\"\n    # Use the lock to prevent race conditions with initialization or concurrent shutdowns\n    async with self._initialization_lock:\n        # Check if already effectively shut down or never initialized properly\n        if not self._initialized and (not hasattr(self,\n                                                  'model_manager') or self.model_manager is None or not self.model_manager.initialized):\n            logger.info(\"Ensemble already shut down or was not successfully initialized.\")\n            return\n\n        logger.info(\"Shutting down ensemble resources...\")\n        shutdown_start_time = time.time()\n\n        try:\n            # Shutdown the model manager (this unloads models and stops executor)\n            if hasattr(self, 'model_manager') and self.model_manager:\n                await self.model_manager.shutdown()\n\n        except Exception as e:\n            # Log error but proceed to reset state\n            logger.error(f\"Error during ensemble shutdown: {e}\", exc_info=True)\n\n        finally:\n            # Reset state regardless of shutdown errors\n            self._initialized = False\n            # Setting managers to None might prevent reuse, decide based on desired behavior.\n            # Clearing might be safer if re-initialization with new config is possible.\n            # self.model_manager = None\n            # self.config_manager = None\n            shutdown_duration = time.time() - shutdown_start_time\n            logger.info(f\"Ensemble shutdown completed in {shutdown_duration:.2f}s.\")\n</code></pre>"},{"location":"api/models/","title":"Models API","text":"<p>API reference for model implementations. </p> <p>Model management for ai-ensemble-suite.</p>"},{"location":"api/models/#ai_ensemble_suite.models.GGUFModel","title":"<code>GGUFModel</code>","text":"<p>Wrapper for llama-cpp-python GGUF models.</p> Source code in <code>src\\ai_ensemble_suite\\models\\gguf_model.py</code> <pre><code>class GGUFModel:\n    \"\"\"Wrapper for llama-cpp-python GGUF models.\"\"\"\n\n    def __init__(\n            self,\n            model_id: str,\n            model_path: str,\n            model_config: Dict[str, Any],\n            config_manager: Optional[\"ConfigManager\"] = None,\n            executor: Optional[ThreadPoolExecutor] = None\n    ) -&gt; None:\n        if not LLAMA_CPP_AVAILABLE:\n            raise ResourceError(\n                \"llama-cpp-python is not installed. Please install it to use GGUF models.\"\n                \" See warning message above for installation instructions.\"\n            )\n        self._model_id = model_id\n        self._model_path = str(model_path)  # Ensure path is string\n        self._model_config_original = model_config\n        self._config_manager = config_manager\n        self._executor = executor\n        self._llm: Optional[Llama] = None\n        self._is_loaded = False\n        self._inference_lock = asyncio.Lock()\n        self._role = model_config.get(\"role\")\n\n        # --- Parameter Merging ---\n        model_params_specific = model_config.get(\"parameters\", {})\n        if not isinstance(model_params_specific, dict):\n            logger.warning(f\"Model '{model_id}': 'parameters' section is not a dictionary. Ignoring.\")\n            model_params_specific = {}\n\n        model_params_defaults = {}\n        if self._config_manager:\n            try:\n                if hasattr(self._config_manager, 'get_default_model_parameters'):\n                    model_params_defaults = self._config_manager.get_default_model_parameters()\n                elif hasattr(self._config_manager, 'get_model_defaults'):  # Fallback check\n                    logger.warning(\n                        f\"ConfigManager has 'get_model_defaults' not 'get_default_model_parameters'. Using it for defaults for {model_id}, but might indicate mismatch.\")\n                    model_params_defaults = self._config_manager.get_model_defaults()\n\n                if not isinstance(model_params_defaults, dict):\n                    logger.warning(\n                        f\"Default model parameters retrieved from ConfigManager for '{model_id}' is not a dict. Ignoring defaults.\")\n                    model_params_defaults = {}\n            except AttributeError:\n                logger.warning(\n                    f\"ConfigManager instance passed to GGUFModel '{model_id}' does not implement expected defaults method. Skipping defaults.\")\n            except Exception as e:\n                logger.warning(f\"Could not retrieve model defaults from ConfigManager for '{model_id}': {e}\")\n\n        self._parameters = {**model_params_defaults, **model_params_specific}\n\n        # --- Path Validation ---\n        try:\n            resolved_path_obj = Path(self._model_path).resolve()\n            if not resolved_path_obj.exists():\n                logger.error(f\"Model path does not exist: {resolved_path_obj}\")\n                raise ModelError(f\"Model path does not exist: {resolved_path_obj}\")\n            self._model_path = str(resolved_path_obj)  # Store the resolved absolute path\n        except Exception as path_e:\n            logger.error(f\"Error resolving or checking model path '{self._model_path}': {path_e}\")\n            raise ModelError(f\"Invalid model path '{self._model_path}': {path_e}\") from path_e\n\n        logger.info(f\"Initialized GGUFModel '{self._model_id}' (Path: {os.path.basename(self._model_path)})\",\n                    extra={\"role\": self._role})\n        logger.debug(f\"Effective parameters for '{self._model_id}': {self._parameters}\")\n\n    def _get_inference_lock(self) -&gt; asyncio.Lock:\n        # Provides access to the instance's lock for external coordinated use if needed\n        return self._inference_lock\n\n    def count_tokens(self, text: str) -&gt; int:\n        \"\"\"Count the number of tokens in the text using the model's tokenizer.\n\n        Args:\n            text: The text to tokenize and count\n\n        Returns:\n            Number of tokens in the text\n\n        Raises:\n            ModelError: If the model isn't loaded\n        \"\"\"\n        if not self.is_loaded() or self._llm is None:\n            raise ModelError(f\"Model {self._model_id} must be loaded to count tokens\")\n\n        try:\n            # Use the model's tokenizer to get an accurate count\n            tokens = self._llm.tokenize(text.encode('utf-8'))\n            return len(tokens)\n        except Exception as e:\n            logger.warning(f\"Error counting tokens with model tokenizer for {self._model_id}: {e}\")\n            # Fallback estimate (rough approximation for English text)\n            words = len(text.split())\n            est_tokens = int(words * 1.3)  # ~1.3 tokens per word is a rough estimate\n            logger.warning(\n                f\"Using fallback token estimation for {self._model_id}: ~{est_tokens} tokens (based on {words} words)\")\n            return est_tokens\n\n    def load(self) -&gt; bool:\n        \"\"\"Loads the GGUF model using llama-cpp-python. Blocking call.\"\"\"\n        if self._llm is not None:\n            logger.debug(f\"Model {self._model_id} is already loaded.\")\n            return True\n        if not os.path.exists(self._model_path):\n            logger.error(f\"Model path disappeared before loading: {self._model_path}\")\n            return False\n\n        load_start_time = time.time()\n        thread_name = threading.current_thread().name  # Get thread name for logging\n        logger.info(f\"[{thread_name}] Loading model {self._model_id} from {self._model_path}\")\n        llama_params = self._get_llama_constructor_params()\n        try:\n            logger.debug(\n                f\"[{thread_name}] Initializing llama_cpp.Llama for '{self._model_id}' with params: {llama_params}\")\n            self._llm = Llama(\n                model_path=str(self._model_path),\n                **llama_params\n            )\n            logger.debug(f\"[{thread_name}] llama_cpp.Llama constructor RETURNED SUCCESSFULLY for '{self._model_id}'\")\n\n            self._is_loaded = True\n            load_time = time.time() - load_start_time\n            logger.info(f\"[{thread_name}] Successfully loaded model {self._model_id} in {load_time:.2f}s\")\n            return True\n\n        except Exception as e:\n            logger.error(f\"[{thread_name}] EXCEPTION during llama_cpp.Llama constructor for '{self._model_id}'\")\n            self._llm = None\n            self._is_loaded = False\n            logger.error(f\"[{thread_name}] Failed to load model {self._model_id} from {self._model_path}: {str(e)}\",\n                         exc_info=True)\n            str_e = str(e).lower()\n            if \"ggml_assert\" in str_e: logger.error(\n                f\"[{thread_name}] Hint: GGML_ASSERT errors often indicate an issue with the model file itself (corruption) or incompatibility with the llama.cpp version.\")\n            if \"cublas\" in str_e or \"cuda\" in str_e: logger.error(\n                f\"[{thread_name}] Hint: CUDA/cuBLAS related errors often point to driver issues or problems with the llama-cpp-python GPU build.\")\n            if \"blas\" in str_e and \"cublas\" not in str_e: logger.error(\n                f\"[{thread_name}] Hint: BLAS errors might indicate issues with the CPU backend library (OpenBLAS, etc.).\")\n            if \"metal\" in str_e: logger.error(\n                f\"[{thread_name}] Hint: Metal errors usually occur on macOS and might relate to GPU compatibility or build issues.\")\n            if \"path\" in str_e: logger.error(\n                f\"[{thread_name}] Hint: Double-check the model path is correct and accessible: {self._model_path}\")\n            return False\n        # finally:\n        #    logger.debug(f\"[{thread_name}] Exiting load() method for {self._model_id}\")\n\n    def _get_llama_constructor_params(self) -&gt; Dict[str, Any]:\n        \"\"\"Prepare parameters specifically for the Llama constructor.\"\"\"\n        known_llama_params = {\n            # Model Params\n            \"n_ctx\": int, \"n_parts\": int, \"n_gpu_layers\": int, \"seed\": int,\n            \"f16_kv\": bool, \"logits_all\": bool, \"vocab_only\": bool, \"use_mmap\": bool, \"use_mlock\": bool,\n            # Loading Params\n            \"lora_base\": str, \"lora_path\": str,\n            # Context Params\n            \"embedding\": bool, \"n_threads\": int, \"n_threads_batch\": int, \"n_batch\": int,\n            \"last_n_tokens_size\": int,\n            # Misc\n            \"numa\": bool, \"verbose\": bool, \"chat_format\": str,\n            # Newer params examples\n            # \"rope_freq_base\": float, \"rope_freq_scale\": float,\n            # \"main_gpu\": int, \"tensor_split\": List[float],\n        }\n\n        constructor_params = {}\n        model_params = self._parameters\n\n        for param_key, param_type in known_llama_params.items():\n            if param_key in model_params:\n                value = model_params[param_key]\n                try:\n                    if value is None and param_key in [\"lora_base\", \"lora_path\", \"chat_format\"]:\n                        constructor_params[param_key] = None\n                        continue\n                    elif value is None:\n                        continue\n\n                    original_value_repr = repr(value)\n                    if param_type == bool and not isinstance(value, bool):\n                        if isinstance(value, str):\n                            value = value.lower() in ['true', '1', 't', 'y', 'yes', 'on']\n                        else:\n                            value = bool(value)\n                    elif param_type == int and not isinstance(value, int):\n                        value = int(float(value))\n                    elif param_type == float and not isinstance(value, float):\n                        value = float(value)\n                    elif param_type == str and not isinstance(value, str):\n                        value = str(value)\n                    # Add list handling if needed (e.g., tensor_split)\n\n                    constructor_params[param_key] = value\n                except (ValueError, TypeError) as e:\n                    logger.warning(\n                        f\"Could not convert param '{param_key}' (value: {original_value_repr}) for Llama constructor for '{self._model_id}': {e}. Skipping parameter.\")\n\n        # Sensible Defaults &amp; Overrides\n        if constructor_params.get(\"verbose\") is not True:\n            constructor_params[\"verbose\"] = False\n\n        constructor_params[\"logits_all\"] = True  # Required for logprobs\n\n        if \"n_ctx\" not in constructor_params:\n            constructor_params[\"n_ctx\"] = 4096\n            logger.debug(f\"Using default n_ctx={constructor_params['n_ctx']} for {self._model_id}\")\n\n        if \"n_gpu_layers\" not in constructor_params:\n            constructor_params[\"n_gpu_layers\"] = 0\n            logger.debug(f\"Using default n_gpu_layers={constructor_params['n_gpu_layers']} for {self._model_id}\")\n        else:\n            logger.debug(f\"Using configured n_gpu_layers={constructor_params['n_gpu_layers']} for {self._model_id}\")\n\n        # Keep n_batch config or default from ConfigManager if present\n        if \"n_batch\" in constructor_params:\n            logger.debug(f\"Using configured n_batch={constructor_params['n_batch']} for {self._model_id}\")\n        # NOTE: We removed the manual default setting for n_batch here based on previous findings.\n        # If it's needed, it should come from the model defaults in the config.\n        # else:\n        #    logger.debug(f\"n_batch not configured for {self._model_id}, llama-cpp will use its internal default.\")\n\n        return constructor_params\n\n    # Changed to sync function based on previous analysis\n    def unload(self) -&gt; None:\n        \"\"\"Release the Llama model instance resources.\"\"\"\n        if not self._is_loaded or self._llm is None:\n            logger.debug(f\"Model {self._model_id} is not loaded or already unloaded\")\n            return\n\n        logger.info(f\"Unloading model {self._model_id}...\")\n        unload_start_time = time.time()\n\n        llm_instance_ref = self._llm\n        self._llm = None\n        self._is_loaded = False\n\n        try:\n            del llm_instance_ref\n            import gc\n            gc.collect()\n\n            try:\n                import torch\n                if torch.cuda.is_available():\n                    torch.cuda.empty_cache()\n                    logger.debug(f\"Requested clearing CUDA cache during unload of {self._model_id}.\")\n            except ImportError:\n                pass\n            except Exception as cuda_e:\n                logger.warning(f\"Could not clear CUDA cache during unload of {self._model_id}: {cuda_e}\")\n\n            unload_duration = time.time() - unload_start_time\n            logger.info(\n                f\"Model {self._model_id} instance released in {unload_duration:.2f}s (Resource cleanup might take slightly longer).\")\n        except Exception as e:\n            logger.error(f\"Error occurred during model {self._model_id} unload/cleanup: {str(e)}\", exc_info=True)\n\n    def is_loaded(self) -&gt; bool:\n        \"\"\"Check if the model is currently loaded and the instance exists.\"\"\"\n        return self._is_loaded and self._llm is not None\n\n    def _prepare_generation_params(\n            self,\n            compute_confidence: bool = False,\n            **kwargs: Any\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Merges default, model-specific, and call-specific generation parameters.\"\"\"\n\n        gen_params = {\n            \"temperature\": 0.7, \"top_p\": 0.9, \"top_k\": 40, \"max_tokens\": 2048,\n            \"repeat_penalty\": 1.1, \"echo\": False, \"logprobs\": None\n        }\n\n        known_gen_keys = {\n            \"suffix\", \"max_tokens\", \"temperature\", \"top_p\", \"logprobs\", \"echo\",\n            \"stop\", \"frequency_penalty\", \"presence_penalty\", \"repeat_penalty\",\n            \"top_k\", \"stream\", \"seed\", \"tfs_z\", \"mirostat_mode\", \"mirostat_tau\",\n            \"mirostat_eta\", \"grammar\", \"logits_processor\"\n        }\n\n        model_gen_params = {k: v for k, v in self._parameters.items() if k in known_gen_keys}\n        gen_params.update(model_gen_params)\n\n        kwarg_overrides = {k: v for k, v in kwargs.items() if k in known_gen_keys}\n        gen_params.update(kwarg_overrides)\n\n        # --- Type checks and Post-processing ---\n\n        if gen_params.get(\"max_tokens\") is not None:\n            try:\n                max_t = int(gen_params[\"max_tokens\"])\n                gen_params[\"max_tokens\"] = max_t if max_t &gt; 0 else None\n            except (ValueError, TypeError):\n                logger.warning(\n                    f\"'max_tokens' value {gen_params['max_tokens']} is invalid for {self._model_id}. Setting to None (unlimited).\")\n                gen_params[\"max_tokens\"] = None\n\n        if \"grammar\" in gen_params and isinstance(gen_params[\"grammar\"], str):\n            grammar_str = gen_params[\"grammar\"]\n            if grammar_str:\n                try:\n                    gen_params[\"grammar\"] = LlamaGrammar.from_string(grammar_str)\n                    logger.debug(f\"Successfully parsed grammar string for {self._model_id}.\")\n                except Exception as e:\n                    logger.error(\n                        f\"Failed to parse grammar string for {self._model_id}: {e}. Removing grammar parameter.\")\n                    gen_params.pop(\"grammar\", None)\n            else:\n                gen_params.pop(\"grammar\", None)\n\n        # Automatically enable logprobs if confidence calculation is explicitly requested FOR THIS CALL\n        if compute_confidence and gen_params.get('logprobs') is None:\n            # Setting to 1 implies getting logprobs for the *selected* token.\n            # If top-k logprobs are needed, &gt;1 should be used, but that complicates parsing.\n            logprob_val = 1\n            logger.debug(f\"Confidence requested for {self._model_id}, setting 'logprobs={logprob_val}'.\")\n            gen_params['logprobs'] = logprob_val\n        elif gen_params.get('logprobs') is not None:\n            try:\n                gen_params['logprobs'] = int(gen_params['logprobs'])\n                if gen_params['logprobs'] &lt;= 0:  # Ensure it's positive if set\n                    logger.warning(f\"'logprobs' value must be positive for {self._model_id}. Disabling logprobs.\")\n                    gen_params['logprobs'] = None\n            except (ValueError, TypeError):\n                logger.warning(\n                    f\"Invalid 'logprobs' value {gen_params['logprobs']} for {self._model_id}. Disabling logprobs.\")\n                gen_params['logprobs'] = None\n\n        # 'stream' is handled differently; remove if present\n        if 'stream' in gen_params:\n            logger.debug(\n                f\"Removing 'stream' parameter for {self._model_id} as it's not supported by this async generate method's usage.\")\n            del gen_params['stream']\n\n        logger.debug(f\"Prepared final generation params for {self._model_id}: {gen_params}\")\n        return gen_params\n\n    async def generate(\n            self,\n            prompt: str,\n            **kwargs: Any  # Includes compute_confidence passed from ModelManager\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Generate text using the loaded model, executed in a thread pool.\"\"\"\n        if not self.is_loaded() or self._llm is None:\n            logger.error(f\"Attempted to generate with unloaded model: {self._model_id}\")\n            raise ModelError(f\"Model {self._model_id} is not loaded or instance is None\")\n\n        if self._executor is None:\n            logger.error(f\"Model {self._model_id} has no executor assigned for generation.\")\n            raise ModelError(f\"Executor not available for model {self._model_id}\")\n\n        # Explicitly read compute_confidence flag for this specific call\n        compute_confidence_flag = kwargs.get('compute_confidence', False)\n        parameters_override = kwargs.get('parameters_override', {})\n\n        kwargs_for_prepare = kwargs.copy()\n        kwargs_for_prepare.pop('compute_confidence', None)  # Don't pass meta-flag down\n        kwargs_for_prepare.pop('parameters_override', None)\n        if isinstance(parameters_override, dict):\n            kwargs_for_prepare.update(parameters_override)\n\n        generation_params = self._prepare_generation_params(\n            compute_confidence=compute_confidence_flag,  # Pass flag to enable logprobs if needed\n            **kwargs_for_prepare\n        )\n\n        formatted_prompt = self._format_prompt(prompt)\n\n        thread_name = threading.current_thread().name\n        # Include role in log message\n        model_id_with_role = f\"{self._model_id} ({self._role})\" if self._role else self._model_id\n\n        # Get context window size\n        context_window = self.get_context_window()\n\n        # Count tokens and log prompt metrics\n        token_count = 0\n        try:\n            if self._llm and hasattr(self._llm, 'tokenize'):\n                tokens = self._llm.tokenize(formatted_prompt.encode('utf-8'))\n                token_count = len(tokens)\n\n                # Calculate usage percentage\n                usage_pct = (token_count / context_window) * 100 if context_window &gt; 0 else 0\n\n                # Log comprehensive prompt metrics\n                logger.info(\n                    f\"[{thread_name}] Prompt metrics for {model_id_with_role}: \"\n                    f\"{len(formatted_prompt)} chars, {token_count}/{context_window} tokens \"\n                    f\"({usage_pct:.1f}% of context window)\"\n                )\n\n                # Warning if approaching context limit\n                if token_count &gt; context_window * 0.8 and token_count &lt;= context_window:\n                    logger.warning(\n                        f\"[{thread_name}] Prompt for {model_id_with_role} is using &gt;{usage_pct:.1f}% \"\n                        f\"of context window ({token_count}/{context_window} tokens)\"\n                    )\n                # Error if exceeding context limit\n                if token_count &gt; context_window:\n                    logger.error(\n                        f\"[{thread_name}] CONTEXT WINDOW EXCEEDED: Prompt token count ({token_count}) exceeds \"\n                        f\"model's context window ({context_window}) for {model_id_with_role}\"\n                    )\n                    raise ModelError(\n                        f\"Input exceeds model's context window ({token_count} &gt; {context_window}). \"\n                        f\"This can cause hanging or OOM errors.\"\n                    )\n            else:\n                logger.warning(\n                    f\"[{thread_name}] Cannot count tokens for {model_id_with_role} - tokenize method not available\")\n                # Still log basic character count\n                logger.debug(\n                    f\"[{thread_name}] Starting generation task for model {model_id_with_role}. \"\n                    f\"Prompt length: {len(formatted_prompt)} characters. Context window: {context_window} tokens.\"\n                )\n        except ModelError as me:\n            # Re-raise model errors (like context window exceeded)\n            raise\n        except Exception as token_e:\n            # Log but continue for other token counting errors\n            logger.warning(\n                f\"[{thread_name}] Error counting tokens for {model_id_with_role}: {token_e}. \"\n                f\"Prompt has {len(formatted_prompt)} characters. Context window: {context_window} tokens. \"\n                f\"Proceeding with generation anyway.\"\n            )\n\n        start_gen_time = time.time()\n\n        try:\n            raw_result = await run_in_threadpool(\n                self._llm,\n                _executor=self._executor,\n                prompt=formatted_prompt,\n                **generation_params\n            )\n\n            generation_time = time.time() - start_gen_time\n            logger.debug(\n                f\"[{thread_name}] Raw generation result received for {model_id_with_role} (took {generation_time:.3f}s).\")\n\n            # --- Process the result ---\n            generated_text = \"\"\n            completion_tokens = 0\n            logprobs_data = None  # Initialize\n\n            if isinstance(raw_result, dict) and \"choices\" in raw_result and isinstance(raw_result[\"choices\"], list) and \\\n                    raw_result[\"choices\"]:\n                first_choice = raw_result[\"choices\"][0]\n                if isinstance(first_choice, dict):\n                    generated_text = first_choice.get(\"text\", \"\").strip()\n                    # Extract logprobs if they exist (structure might vary!)\n                    logprobs_dict_or_list = first_choice.get(\"logprobs\")\n\n                    # --- ADD DETAILED LOGGING HERE ---\n                    logger.debug(\n                        f\"[{thread_name}] Raw logprobs structure for {model_id_with_role}: {str(logprobs_dict_or_list)[:500]}...\")  # Log snippet\n                    # --- END ADDED LOGGING ---\n\n                    # Attempt extraction - ADAPT THIS based on the logged structure!\n                    # ... [logprobs extraction code - unchanged] ...\n\n                else:\n                    logger.warning(\n                        f\"[{thread_name}] First choice in 'choices' list is not a dictionary for model {model_id_with_role}: {first_choice}\")\n            else:\n                logger.warning(\n                    f\"[{thread_name}] Unexpected raw_result structure or empty choices for model {model_id_with_role}. Cannot reliably extract text/logprobs. Result: {str(raw_result)[:200]}...\")\n\n            usage_stats = raw_result.get(\"usage\", {}) if isinstance(raw_result, dict) else {}\n            completion_tokens = usage_stats.get(\"completion_tokens\", 0) if isinstance(usage_stats, dict) else 0\n            prompt_tokens = usage_stats.get(\"prompt_tokens\", 0) if isinstance(usage_stats, dict) else 0\n\n            # Compare reported prompt tokens with our count if available\n            if token_count &gt; 0 and prompt_tokens &gt; 0 and abs(token_count - prompt_tokens) &gt; 5:\n                logger.debug(\n                    f\"[{thread_name}] Token count discrepancy for {model_id_with_role}: \"\n                    f\"pre-count={token_count}, reported={prompt_tokens}, diff={token_count - prompt_tokens}\"\n                )\n\n            # Calculate confidence score ONLY IF flag is set and data potentially available\n            confidence_score = None\n            if compute_confidence_flag:\n                # --- LOG BEFORE CALLING ---\n                logger.debug(\n                    f\"[{thread_name}] Data being passed to calculate_confidence_from_logprobs for {model_id_with_role}: {str(logprobs_data)[:500]}...\")  # Log snippet\n                # --- END LOG ---\n                confidence_score = calculate_confidence_from_logprobs(logprobs_data)\n                if confidence_score is not None:\n                    logger.debug(\n                        f\"[{thread_name}] Calculated confidence for {model_id_with_role}: {confidence_score:.4f}\")\n                else:\n                    # --- ADD LOG IF CALCULATION FAILED ---\n                    logger.warning(\n                        f\"[{thread_name}] calculate_confidence_from_logprobs returned None for {model_id_with_role}\")\n\n            response_data = {\n                \"text\": generated_text,\n                \"generation_time\": generation_time,\n                \"token_count\": completion_tokens,\n                \"prompt_token_count\": prompt_tokens if prompt_tokens &gt; 0 else token_count,\n                # Use our count if reported is missing\n                # Store calculated score (might be None)\n                \"confidence\": confidence_score,\n                # Keep raw output for potential use by external confidence logic\n                \"raw_output\": raw_result,\n            }\n\n            confidence_str = f\"{confidence_score:.4f}\" if confidence_score is not None else \"N/A (Not Calculated or Failed)\"\n            # MODIFIED HERE: Include role in log message\n            logger.info(\n                f\"[{thread_name}] Model {model_id_with_role} generated {completion_tokens} tokens in {generation_time:.2f}s. Confidence (Token Prob): {confidence_str}\"\n            )\n\n            return response_data\n\n        except Exception as e:\n            gen_duration = time.time() - start_gen_time\n            if isinstance(e, TypeError) and \"NoneType.__format__\" in str(e):\n                logger.error(\n                    f\"[{thread_name}] Generation failed for model {model_id_with_role} after {gen_duration:.2f}s due to likely logging format error with None confidence. \"\n                    f\"Original error: {type(e).__name__}: {str(e)}\",\n                    exc_info=False\n                )\n            else:\n                logger.error(\n                    f\"[{thread_name}] Generation failed for model {model_id_with_role} after {gen_duration:.2f}s: {type(e).__name__}: {str(e)}\",\n                    exc_info=True\n                )\n            raise ModelError(f\"Generation failed with model {model_id_with_role}: {str(e)}\") from e\n\n    def _format_prompt(self, prompt: str) -&gt; str:\n        \"\"\"Applies system prompt and/or chat formatting if configured.\"\"\"\n        system_prompt = self._parameters.get(\"system_prompt\")\n        chat_formatter_name = self._parameters.get(\"chat_format\")\n\n        if chat_formatter_name and self._llm and hasattr(self._llm, 'chat_handler') and self._llm.chat_handler:\n            try:\n                messages = []\n                if system_prompt:\n                    messages.append({\"role\": \"system\", \"content\": system_prompt})\n                messages.append({\"role\": \"user\", \"content\": prompt})\n\n                # llama-cpp-python &gt;=0.2.X uses create_chat_completion handlers\n                if hasattr(self._llm, 'create_chat_completion'):\n                    # This is complex as the handler isn't easily accessible just for prompt formatting\n                    # We might need to manually apply formatting based on chat_formatter_name if possible,\n                    # or rely on simpler system prompt prepending as fallback.\n                    logger.warning(\n                        f\"Automatic chat formatting via create_chat_completion handler not fully implemented for prompt preparation in GGUFModel._format_prompt for {self._model_id}. Falling back.\")\n                # Handle older chat_handler style if necessary (less likely now)\n                elif hasattr(self._llm, 'chat_handler'):\n                    chat_handler = self._llm.chat_handler(messages=messages)\n                    full_prompt = chat_handler.prompt()  # Deprecated?\n                    logger.debug(\n                        f\"Formatted prompt using legacy Llama chat_handler ({chat_formatter_name}) for {self._model_id}\")\n                    return full_prompt\n\n            except Exception as e:\n                logger.warning(\n                    f\"Failed to use Llama chat formatting for {self._model_id} (format: {chat_formatter_name}). Falling back. Error: {e}\")\n\n        # Fallback: Basic system prompt prepending\n        if system_prompt:\n            logger.debug(f\"Formatting prompt using basic system prompt prepend for {self._model_id}\")\n            return f\"{system_prompt.strip()}\\n\\n{prompt.strip()}\"\n        else:\n            logger.debug(f\"No special prompt formatting applied for {self._model_id}\")\n            return prompt.strip()\n\n    # --- Standard Getters ---\n    def get_id(self) -&gt; str:\n        return self._model_id\n\n    def get_path(self) -&gt; str:\n        return self._model_path\n\n    def get_role(self) -&gt; Optional[str]:\n        return self._role\n\n    def get_config(self) -&gt; Dict[str, Any]:\n        return copy.deepcopy(self._model_config_original)\n\n    def get_context_window(self) -&gt; int:\n        return self._parameters.get(\"n_ctx\", 4096)\n\n    def _set_executor(self, executor: Optional[ThreadPoolExecutor]) -&gt; None:\n        self._executor = executor\n\n    def get_effective_parameters(self) -&gt; Dict[str, Any]:\n        return copy.deepcopy(self._parameters)\n</code></pre>"},{"location":"api/models/#ai_ensemble_suite.models.GGUFModel.count_tokens","title":"<code>count_tokens(text)</code>","text":"<p>Count the number of tokens in the text using the model's tokenizer.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to tokenize and count</p> required <p>Returns:</p> Type Description <code>int</code> <p>Number of tokens in the text</p> <p>Raises:</p> Type Description <code>ModelError</code> <p>If the model isn't loaded</p> Source code in <code>src\\ai_ensemble_suite\\models\\gguf_model.py</code> <pre><code>def count_tokens(self, text: str) -&gt; int:\n    \"\"\"Count the number of tokens in the text using the model's tokenizer.\n\n    Args:\n        text: The text to tokenize and count\n\n    Returns:\n        Number of tokens in the text\n\n    Raises:\n        ModelError: If the model isn't loaded\n    \"\"\"\n    if not self.is_loaded() or self._llm is None:\n        raise ModelError(f\"Model {self._model_id} must be loaded to count tokens\")\n\n    try:\n        # Use the model's tokenizer to get an accurate count\n        tokens = self._llm.tokenize(text.encode('utf-8'))\n        return len(tokens)\n    except Exception as e:\n        logger.warning(f\"Error counting tokens with model tokenizer for {self._model_id}: {e}\")\n        # Fallback estimate (rough approximation for English text)\n        words = len(text.split())\n        est_tokens = int(words * 1.3)  # ~1.3 tokens per word is a rough estimate\n        logger.warning(\n            f\"Using fallback token estimation for {self._model_id}: ~{est_tokens} tokens (based on {words} words)\")\n        return est_tokens\n</code></pre>"},{"location":"api/models/#ai_ensemble_suite.models.GGUFModel.generate","title":"<code>generate(prompt, **kwargs)</code>  <code>async</code>","text":"<p>Generate text using the loaded model, executed in a thread pool.</p> Source code in <code>src\\ai_ensemble_suite\\models\\gguf_model.py</code> <pre><code>async def generate(\n        self,\n        prompt: str,\n        **kwargs: Any  # Includes compute_confidence passed from ModelManager\n) -&gt; Dict[str, Any]:\n    \"\"\"Generate text using the loaded model, executed in a thread pool.\"\"\"\n    if not self.is_loaded() or self._llm is None:\n        logger.error(f\"Attempted to generate with unloaded model: {self._model_id}\")\n        raise ModelError(f\"Model {self._model_id} is not loaded or instance is None\")\n\n    if self._executor is None:\n        logger.error(f\"Model {self._model_id} has no executor assigned for generation.\")\n        raise ModelError(f\"Executor not available for model {self._model_id}\")\n\n    # Explicitly read compute_confidence flag for this specific call\n    compute_confidence_flag = kwargs.get('compute_confidence', False)\n    parameters_override = kwargs.get('parameters_override', {})\n\n    kwargs_for_prepare = kwargs.copy()\n    kwargs_for_prepare.pop('compute_confidence', None)  # Don't pass meta-flag down\n    kwargs_for_prepare.pop('parameters_override', None)\n    if isinstance(parameters_override, dict):\n        kwargs_for_prepare.update(parameters_override)\n\n    generation_params = self._prepare_generation_params(\n        compute_confidence=compute_confidence_flag,  # Pass flag to enable logprobs if needed\n        **kwargs_for_prepare\n    )\n\n    formatted_prompt = self._format_prompt(prompt)\n\n    thread_name = threading.current_thread().name\n    # Include role in log message\n    model_id_with_role = f\"{self._model_id} ({self._role})\" if self._role else self._model_id\n\n    # Get context window size\n    context_window = self.get_context_window()\n\n    # Count tokens and log prompt metrics\n    token_count = 0\n    try:\n        if self._llm and hasattr(self._llm, 'tokenize'):\n            tokens = self._llm.tokenize(formatted_prompt.encode('utf-8'))\n            token_count = len(tokens)\n\n            # Calculate usage percentage\n            usage_pct = (token_count / context_window) * 100 if context_window &gt; 0 else 0\n\n            # Log comprehensive prompt metrics\n            logger.info(\n                f\"[{thread_name}] Prompt metrics for {model_id_with_role}: \"\n                f\"{len(formatted_prompt)} chars, {token_count}/{context_window} tokens \"\n                f\"({usage_pct:.1f}% of context window)\"\n            )\n\n            # Warning if approaching context limit\n            if token_count &gt; context_window * 0.8 and token_count &lt;= context_window:\n                logger.warning(\n                    f\"[{thread_name}] Prompt for {model_id_with_role} is using &gt;{usage_pct:.1f}% \"\n                    f\"of context window ({token_count}/{context_window} tokens)\"\n                )\n            # Error if exceeding context limit\n            if token_count &gt; context_window:\n                logger.error(\n                    f\"[{thread_name}] CONTEXT WINDOW EXCEEDED: Prompt token count ({token_count}) exceeds \"\n                    f\"model's context window ({context_window}) for {model_id_with_role}\"\n                )\n                raise ModelError(\n                    f\"Input exceeds model's context window ({token_count} &gt; {context_window}). \"\n                    f\"This can cause hanging or OOM errors.\"\n                )\n        else:\n            logger.warning(\n                f\"[{thread_name}] Cannot count tokens for {model_id_with_role} - tokenize method not available\")\n            # Still log basic character count\n            logger.debug(\n                f\"[{thread_name}] Starting generation task for model {model_id_with_role}. \"\n                f\"Prompt length: {len(formatted_prompt)} characters. Context window: {context_window} tokens.\"\n            )\n    except ModelError as me:\n        # Re-raise model errors (like context window exceeded)\n        raise\n    except Exception as token_e:\n        # Log but continue for other token counting errors\n        logger.warning(\n            f\"[{thread_name}] Error counting tokens for {model_id_with_role}: {token_e}. \"\n            f\"Prompt has {len(formatted_prompt)} characters. Context window: {context_window} tokens. \"\n            f\"Proceeding with generation anyway.\"\n        )\n\n    start_gen_time = time.time()\n\n    try:\n        raw_result = await run_in_threadpool(\n            self._llm,\n            _executor=self._executor,\n            prompt=formatted_prompt,\n            **generation_params\n        )\n\n        generation_time = time.time() - start_gen_time\n        logger.debug(\n            f\"[{thread_name}] Raw generation result received for {model_id_with_role} (took {generation_time:.3f}s).\")\n\n        # --- Process the result ---\n        generated_text = \"\"\n        completion_tokens = 0\n        logprobs_data = None  # Initialize\n\n        if isinstance(raw_result, dict) and \"choices\" in raw_result and isinstance(raw_result[\"choices\"], list) and \\\n                raw_result[\"choices\"]:\n            first_choice = raw_result[\"choices\"][0]\n            if isinstance(first_choice, dict):\n                generated_text = first_choice.get(\"text\", \"\").strip()\n                # Extract logprobs if they exist (structure might vary!)\n                logprobs_dict_or_list = first_choice.get(\"logprobs\")\n\n                # --- ADD DETAILED LOGGING HERE ---\n                logger.debug(\n                    f\"[{thread_name}] Raw logprobs structure for {model_id_with_role}: {str(logprobs_dict_or_list)[:500]}...\")  # Log snippet\n                # --- END ADDED LOGGING ---\n\n                # Attempt extraction - ADAPT THIS based on the logged structure!\n                # ... [logprobs extraction code - unchanged] ...\n\n            else:\n                logger.warning(\n                    f\"[{thread_name}] First choice in 'choices' list is not a dictionary for model {model_id_with_role}: {first_choice}\")\n        else:\n            logger.warning(\n                f\"[{thread_name}] Unexpected raw_result structure or empty choices for model {model_id_with_role}. Cannot reliably extract text/logprobs. Result: {str(raw_result)[:200]}...\")\n\n        usage_stats = raw_result.get(\"usage\", {}) if isinstance(raw_result, dict) else {}\n        completion_tokens = usage_stats.get(\"completion_tokens\", 0) if isinstance(usage_stats, dict) else 0\n        prompt_tokens = usage_stats.get(\"prompt_tokens\", 0) if isinstance(usage_stats, dict) else 0\n\n        # Compare reported prompt tokens with our count if available\n        if token_count &gt; 0 and prompt_tokens &gt; 0 and abs(token_count - prompt_tokens) &gt; 5:\n            logger.debug(\n                f\"[{thread_name}] Token count discrepancy for {model_id_with_role}: \"\n                f\"pre-count={token_count}, reported={prompt_tokens}, diff={token_count - prompt_tokens}\"\n            )\n\n        # Calculate confidence score ONLY IF flag is set and data potentially available\n        confidence_score = None\n        if compute_confidence_flag:\n            # --- LOG BEFORE CALLING ---\n            logger.debug(\n                f\"[{thread_name}] Data being passed to calculate_confidence_from_logprobs for {model_id_with_role}: {str(logprobs_data)[:500]}...\")  # Log snippet\n            # --- END LOG ---\n            confidence_score = calculate_confidence_from_logprobs(logprobs_data)\n            if confidence_score is not None:\n                logger.debug(\n                    f\"[{thread_name}] Calculated confidence for {model_id_with_role}: {confidence_score:.4f}\")\n            else:\n                # --- ADD LOG IF CALCULATION FAILED ---\n                logger.warning(\n                    f\"[{thread_name}] calculate_confidence_from_logprobs returned None for {model_id_with_role}\")\n\n        response_data = {\n            \"text\": generated_text,\n            \"generation_time\": generation_time,\n            \"token_count\": completion_tokens,\n            \"prompt_token_count\": prompt_tokens if prompt_tokens &gt; 0 else token_count,\n            # Use our count if reported is missing\n            # Store calculated score (might be None)\n            \"confidence\": confidence_score,\n            # Keep raw output for potential use by external confidence logic\n            \"raw_output\": raw_result,\n        }\n\n        confidence_str = f\"{confidence_score:.4f}\" if confidence_score is not None else \"N/A (Not Calculated or Failed)\"\n        # MODIFIED HERE: Include role in log message\n        logger.info(\n            f\"[{thread_name}] Model {model_id_with_role} generated {completion_tokens} tokens in {generation_time:.2f}s. Confidence (Token Prob): {confidence_str}\"\n        )\n\n        return response_data\n\n    except Exception as e:\n        gen_duration = time.time() - start_gen_time\n        if isinstance(e, TypeError) and \"NoneType.__format__\" in str(e):\n            logger.error(\n                f\"[{thread_name}] Generation failed for model {model_id_with_role} after {gen_duration:.2f}s due to likely logging format error with None confidence. \"\n                f\"Original error: {type(e).__name__}: {str(e)}\",\n                exc_info=False\n            )\n        else:\n            logger.error(\n                f\"[{thread_name}] Generation failed for model {model_id_with_role} after {gen_duration:.2f}s: {type(e).__name__}: {str(e)}\",\n                exc_info=True\n            )\n        raise ModelError(f\"Generation failed with model {model_id_with_role}: {str(e)}\") from e\n</code></pre>"},{"location":"api/models/#ai_ensemble_suite.models.GGUFModel.is_loaded","title":"<code>is_loaded()</code>","text":"<p>Check if the model is currently loaded and the instance exists.</p> Source code in <code>src\\ai_ensemble_suite\\models\\gguf_model.py</code> <pre><code>def is_loaded(self) -&gt; bool:\n    \"\"\"Check if the model is currently loaded and the instance exists.\"\"\"\n    return self._is_loaded and self._llm is not None\n</code></pre>"},{"location":"api/models/#ai_ensemble_suite.models.GGUFModel.load","title":"<code>load()</code>","text":"<p>Loads the GGUF model using llama-cpp-python. Blocking call.</p> Source code in <code>src\\ai_ensemble_suite\\models\\gguf_model.py</code> <pre><code>def load(self) -&gt; bool:\n    \"\"\"Loads the GGUF model using llama-cpp-python. Blocking call.\"\"\"\n    if self._llm is not None:\n        logger.debug(f\"Model {self._model_id} is already loaded.\")\n        return True\n    if not os.path.exists(self._model_path):\n        logger.error(f\"Model path disappeared before loading: {self._model_path}\")\n        return False\n\n    load_start_time = time.time()\n    thread_name = threading.current_thread().name  # Get thread name for logging\n    logger.info(f\"[{thread_name}] Loading model {self._model_id} from {self._model_path}\")\n    llama_params = self._get_llama_constructor_params()\n    try:\n        logger.debug(\n            f\"[{thread_name}] Initializing llama_cpp.Llama for '{self._model_id}' with params: {llama_params}\")\n        self._llm = Llama(\n            model_path=str(self._model_path),\n            **llama_params\n        )\n        logger.debug(f\"[{thread_name}] llama_cpp.Llama constructor RETURNED SUCCESSFULLY for '{self._model_id}'\")\n\n        self._is_loaded = True\n        load_time = time.time() - load_start_time\n        logger.info(f\"[{thread_name}] Successfully loaded model {self._model_id} in {load_time:.2f}s\")\n        return True\n\n    except Exception as e:\n        logger.error(f\"[{thread_name}] EXCEPTION during llama_cpp.Llama constructor for '{self._model_id}'\")\n        self._llm = None\n        self._is_loaded = False\n        logger.error(f\"[{thread_name}] Failed to load model {self._model_id} from {self._model_path}: {str(e)}\",\n                     exc_info=True)\n        str_e = str(e).lower()\n        if \"ggml_assert\" in str_e: logger.error(\n            f\"[{thread_name}] Hint: GGML_ASSERT errors often indicate an issue with the model file itself (corruption) or incompatibility with the llama.cpp version.\")\n        if \"cublas\" in str_e or \"cuda\" in str_e: logger.error(\n            f\"[{thread_name}] Hint: CUDA/cuBLAS related errors often point to driver issues or problems with the llama-cpp-python GPU build.\")\n        if \"blas\" in str_e and \"cublas\" not in str_e: logger.error(\n            f\"[{thread_name}] Hint: BLAS errors might indicate issues with the CPU backend library (OpenBLAS, etc.).\")\n        if \"metal\" in str_e: logger.error(\n            f\"[{thread_name}] Hint: Metal errors usually occur on macOS and might relate to GPU compatibility or build issues.\")\n        if \"path\" in str_e: logger.error(\n            f\"[{thread_name}] Hint: Double-check the model path is correct and accessible: {self._model_path}\")\n        return False\n</code></pre>"},{"location":"api/models/#ai_ensemble_suite.models.GGUFModel.unload","title":"<code>unload()</code>","text":"<p>Release the Llama model instance resources.</p> Source code in <code>src\\ai_ensemble_suite\\models\\gguf_model.py</code> <pre><code>def unload(self) -&gt; None:\n    \"\"\"Release the Llama model instance resources.\"\"\"\n    if not self._is_loaded or self._llm is None:\n        logger.debug(f\"Model {self._model_id} is not loaded or already unloaded\")\n        return\n\n    logger.info(f\"Unloading model {self._model_id}...\")\n    unload_start_time = time.time()\n\n    llm_instance_ref = self._llm\n    self._llm = None\n    self._is_loaded = False\n\n    try:\n        del llm_instance_ref\n        import gc\n        gc.collect()\n\n        try:\n            import torch\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n                logger.debug(f\"Requested clearing CUDA cache during unload of {self._model_id}.\")\n        except ImportError:\n            pass\n        except Exception as cuda_e:\n            logger.warning(f\"Could not clear CUDA cache during unload of {self._model_id}: {cuda_e}\")\n\n        unload_duration = time.time() - unload_start_time\n        logger.info(\n            f\"Model {self._model_id} instance released in {unload_duration:.2f}s (Resource cleanup might take slightly longer).\")\n    except Exception as e:\n        logger.error(f\"Error occurred during model {self._model_id} unload/cleanup: {str(e)}\", exc_info=True)\n</code></pre>"},{"location":"api/models/#ai_ensemble_suite.models.ModelManager","title":"<code>ModelManager</code>","text":"<p>Manages the loading, execution, and lifecycle of GGUF models.</p> Source code in <code>src\\ai_ensemble_suite\\models\\model_manager.py</code> <pre><code>class ModelManager:\n    \"\"\"Manages the loading, execution, and lifecycle of GGUF models.\"\"\"\n\n    def __init__(\n            self,\n            config_manager: ConfigProvider,\n            ensemble: Optional[\"Ensemble\"] = None,  # Add ensemble parameter\n            max_workers: Optional[int] = None\n    ) -&gt; None:\n        \"\"\"Initialize the ModelManager.\n\n        Args:\n            config_manager: Configuration manager instance that implements ConfigProvider.\n            ensemble: Optional reference to the parent Ensemble instance.\n            max_workers: Optional limit for thread pool size.\n\n        Raises:\n            TypeError: If config_manager doesn't implement required methods.\n        \"\"\"\n        # Validate the ConfigProvider implementation\n        required_methods = [\n            'get_all_models_config', 'get_template', 'get_confidence_config',\n            'get_default_model_parameters', 'get_model_config'\n        ]\n        if not all(hasattr(config_manager, method) for method in required_methods):\n            missing = [m for m in required_methods if not hasattr(config_manager, m)]\n            raise TypeError(f\"config_manager must implement the ConfigProvider protocol methods. Missing: {missing}\")\n\n        self.config_manager = config_manager\n        self.ensemble = ensemble  # Store reference to parent ensemble\n        self.models: Dict[str, GGUFModel] = {}  # Stores successfully loaded GGUFModel instances\n        self.initialized = False\n\n        # Determine max workers for the thread pool\n        if max_workers is None:\n             try:\n                 # Use physical cores as a baseline, clamp between reasonable limits\n                 cores = psutil.cpu_count(logical=False) or 1\n                 # Example: Limit to 8 workers max by default, even if more cores\n                 max_workers = min(cores, 8)\n                 logger.info(f\"Determined default max_workers based on physical cores: {max_workers}\")\n             except Exception as e:\n                 logger.warning(f\"Could not detect CPU cores: {e}. Using default max_workers=4\")\n                 max_workers = 4 # Fallback default\n\n        # Ensure at least 1 worker\n        max_workers = max(1, max_workers)\n\n        # Create the thread pool executor\n        self.executor = ThreadPoolExecutor(max_workers=max_workers, thread_name_prefix=\"GGUFWorker\")\n\n        # --- Concurrency Control ---\n        # Limit concurrent loads (often IO/CPU bound init) more strictly than inference\n        # Example: Allow half the workers for loading concurrently\n        self._max_concurrent_loads: int = max(1, max_workers // 2)\n        self._max_concurrent_loads = 1  # Force limit to 1\n        self._load_semaphore = asyncio.Semaphore(self._max_concurrent_loads)\n\n        # Limit concurrent inference calls (can be adjusted)\n        self._max_concurrent_inference: int = max_workers # Allow all workers for inference by default\n\n        logger.info(f\"Initialized ModelManager with up to {max_workers} worker threads.\")\n        logger.info(f\"Concurrency limits set: Loads={self._max_concurrent_loads}, Inference={self._max_concurrent_inference}\")\n\n    def _resolve_model_path(self, model_id: str, model_path_str: str) -&gt; str:\n        \"\"\"Resolves a potentially relative model path to an absolute path.\n\n        Tries to resolve relative to CWD as a fallback. More robust implementation\n        would use the config file's directory from ConfigManager if available.\n        \"\"\"\n        model_path = Path(model_path_str)\n        if model_path.is_absolute():\n            return str(model_path)\n\n        # --- Attempt resolution relative to CWD (fallback) ---\n        # TODO: Enhance this to use config file location from config_manager if possible\n        # config_dir = Path(self.config_manager.config_path).parent if hasattr(self.config_manager, 'config_path') and self.config_manager.config_path else Path.cwd()\n        config_dir = Path.cwd() # Current fallback\n        resolved_path = (config_dir / model_path).resolve()\n\n        logger.debug(f\"Resolved relative path '{model_path_str}' for model '{model_id}' relative to '{config_dir}' -&gt; '{resolved_path}'\")\n        return str(resolved_path)\n\n    async def _load_model_async(self, model_id: str) -&gt; Tuple[str, Optional[GGUFModel], Optional[Exception]]:\n        \"\"\"Loads a single model asynchronously using the thread pool executor and semaphore.\"\"\"\n        # Use async with pattern for proper semaphore handling\n        async with self._load_semaphore:\n            thread_id = threading.get_ident()\n            logger.debug(f\"[Thread-{thread_id}] Acquired load semaphore slot for {model_id}.\")\n\n            loop = asyncio.get_running_loop()\n            try:\n                # Get a fresh copy of the config for this specific model\n                model_config = self.config_manager.get_model_config(model_id)\n                if not model_config:\n                    raise ConfigurationError(f\"Could not retrieve config for model '{model_id}'\")\n\n                model_path_from_config = model_config.get(\"path\")\n                if not model_path_from_config:\n                    raise ConfigurationError(f\"Path missing in config for model '{model_id}'\")\n\n                # Resolve path before passing to thread\n                resolved_path = self._resolve_model_path(model_id, model_path_from_config)\n                # Add resolved path back to config dict for GGUFModel __init__\n                model_config_with_resolved_path = model_config.copy()\n                model_config_with_resolved_path['resolved_path'] = resolved_path\n\n                logger.debug(f\"[Thread-{thread_id}] Submitting loading task for {model_id} to executor.\")\n\n                # Use run_in_executor to run the synchronous loading part in the pool\n                future = loop.run_in_executor(\n                    self.executor,\n                    lambda: self._perform_load_sync(model_id, model_config_with_resolved_path)\n                )\n                # Await the result from the thread pool\n                result: Tuple[str, Optional[GGUFModel], Optional[Exception]] = await future\n                logger.debug(f\"[Thread-{thread_id}] Received result from executor for {model_id}.\")\n                return result\n\n            except Exception as e:\n                # Catch errors during task submission or config retrieval\n                logger.error(f\"[Thread-{thread_id}] Error in _load_model_async coordinating load for {model_id}: {e}\",\n                             exc_info=True)\n                return model_id, None, e\n                # No need to manually release semaphore - async with handles it\n\n    def _perform_load_sync(self, model_id: str, model_config: Dict[str, Any]) -&gt; Tuple[\n        str, Optional[GGUFModel], Optional[Exception]]:\n        \"\"\"Synchronous part of model loading executed in the thread pool.\"\"\"\n        thread_name = threading.current_thread().name\n        logger.info(f\"[{thread_name}] Attempting to load model {model_id}...\")\n        start_load_time = time.time()\n        model_instance: Optional[GGUFModel] = None\n        try:\n            # Path should already be resolved and in the config dict\n            resolved_path = model_config.get('resolved_path')\n            if not resolved_path:\n                raise ConfigurationError(f\"Resolved path missing in config passed to _perform_load_sync for {model_id}\")\n\n            # ---- Instantiate the Model Wrapper ----\n            logger.debug(f\"[{thread_name}] Instantiating GGUFModel wrapper for {model_id}...\")\n            model_instance = GGUFModel(\n                model_id=model_id,\n                model_path=resolved_path,\n                model_config=model_config,\n                config_manager=self.config_manager,\n                executor=self.executor\n            )\n            logger.debug(f\"[{thread_name}] GGUFModel wrapper instantiated for {model_id}.\")\n\n            # ---- Perform the actual blocking load ----\n            load_successful: bool = model_instance.load()\n            logger.debug(f\"[{thread_name}] GGUFModel.load() RETURNED {load_successful} for {model_id}\")\n\n            if not load_successful:\n                logger.error(f\"[{thread_name}] GGUFModel.load() returned False for model {model_id}. Load failed.\")\n                raise ModelError(f\"GGUFModel.load() returned False for model {model_id}\")\n\n            # --- Load successful ---\n            load_time = time.time() - start_load_time\n            logger.info(f\"[{thread_name}] Model {model_id} loaded successfully in {load_time:.2f}s.\")\n            return model_id, model_instance, None\n\n        except Exception as e:\n            logger.error(f\"[{thread_name}] Error during sync load process for model {model_id}: {e}\", exc_info=True)\n            # Attempt cleanup if instance exists but load failed\n            if model_instance and hasattr(model_instance, 'is_loaded') and model_instance.is_loaded():\n                logger.debug(f\"[{thread_name}] Attempting cleanup for failed model {model_id}\")\n                try:\n                    # Mark model as not loaded to prevent future issues\n                    model_instance._is_loaded = False\n                    # Release any resources that can be directly released\n                    if hasattr(model_instance, '_llm'):\n                        model_instance._llm = None\n                    logger.debug(f\"[{thread_name}] Marked model as unloaded and cleared references\")\n                except Exception as unload_e:\n                    logger.error(f\"[{thread_name}] Error during cleanup for failed model {model_id}: {unload_e}\")\n            return model_id, None, e\n\n    async def initialize(self) -&gt; None:\n        \"\"\"Initialize the ModelManager: Instantiates models and loads them asynchronously.\"\"\"\n        if self.initialized:\n            logger.warning(\"ModelManager already initialized. Skipping.\")\n            return\n\n        logger.info(\"Initializing ModelManager...\")\n        init_start_time = time.time()\n        model_configs: Dict[str, Dict[str, Any]] = {}\n\n        try:\n            # 1. Get configurations for all models\n            model_configs = self.config_manager.get_all_models_config()\n            if not model_configs:\n                raise ConfigurationError(\"No models found in configuration\")\n            logger.info(f\"Found {len(model_configs)} models in configuration.\")\n\n            # 2. Create asynchronous loading tasks for each model\n            load_tasks = []\n            task_to_model_id_map = {} # Map index to model_id for result processing\n            valid_model_ids_to_load = list(model_configs.keys()) # Keep track\n\n            for i, model_id in enumerate(valid_model_ids_to_load):\n                logger.debug(f\"Creating async load task {i} for model {model_id}\")\n                # Use the helper that handles semaphore and calls sync part in executor\n                task = self._load_model_async(model_id)\n                load_tasks.append(task)\n                task_to_model_id_map[i] = model_id # Store mapping BEFORE awaiting\n\n            if not load_tasks:\n                raise ModelError(\"No load tasks were created (possibly no valid models in config).\")\n\n            # 3. Run loading tasks concurrently using asyncio.gather\n            # The semaphore inside _load_model_async limits actual concurrency.\n            logger.info(f\"Loading {len(load_tasks)} models concurrently (Hardware limit via semaphore: {self._max_concurrent_loads})...\")\n            # We can use simple gather here as semaphore controls concurrency\n            load_results = await asyncio.gather(*load_tasks, return_exceptions=True)\n\n            # 4. Process the results\n            temp_models = {} # Dict to store successfully loaded model instances\n            failed_loads = [] # List to store IDs of models that failed to load\n\n            # &gt;&gt;&gt; NEW LOGGING LINE &lt;&lt;&lt;\n            logger.debug(f\"Processing {len(load_results)} results from asyncio.gather in initialize()...\")\n            # ^^^^^^^^^^^^^^^^^^^^^^^^\n            for i, result in enumerate(load_results):\n                # Get the model ID corresponding to this result index\n                model_id = task_to_model_id_map.get(i, f\"Unknown Task {i}\")\n                # &gt;&gt;&gt; NEW LOGGING LINE &lt;&lt;&lt;\n                logger.debug(f\"Processing result index {i} for model_id '{model_id}'...\")\n                # ^^^^^^^^^^^^^^^^^^^^^^^^\n\n                if isinstance(result, Exception):\n                    # Task itself raised exception (e.g., during coordination in _load_model_async)\n                    logger.error(f\"Load task for model '{model_id}' failed directly in gather (exception type: {type(result).__name__}): {result}\", exc_info=result)\n                    failed_loads.append(model_id)\n                elif isinstance(result, tuple) and len(result) == 3:\n                    # Expected result format: (model_id, model_instance | None, exception | None)\n                    res_id, model_instance, error = result\n                    if model_id != res_id: # Sanity check\n                         logger.warning(f\"Model ID mismatch in result processing: expected '{model_id}', got '{res_id}'. Using '{res_id}'.\")\n                         model_id = res_id # Trust the ID returned from the task\n\n                    if error:\n                         # _perform_load_sync caught an exception or load returned False\n                         logger.error(f\"Load task for model '{model_id}' reported failure (exception type: {type(error).__name__}): {error}\", exc_info=error if isinstance(error, Exception) else None)\n                         failed_loads.append(model_id)\n                    elif model_instance is not None and isinstance(model_instance, GGUFModel):\n                         # Successfully loaded\n                         # &gt;&gt;&gt; NEW LOGGING LINE &lt;&lt;&lt;\n                         logger.debug(f\"Successfully processed load result for model '{model_id}'. Adding to temp_models.\")\n                         # ^^^^^^^^^^^^^^^^^^^^^^^^\n                         temp_models[model_id] = model_instance\n                    else:\n                         # Should not happen: success tuple but no instance\n                         logger.error(f\"Load task for model '{model_id}' returned success tuple but no valid model instance.\")\n                         failed_loads.append(model_id)\n                else:\n                     # Unexpected format from gather (should be Exception or Tuple)\n                     logger.error(f\"Unexpected result format '{type(result)}' from load task for model '{model_id}'.\")\n                     failed_loads.append(model_id)\n\n            # &gt;&gt;&gt; NEW LOGGING LINE &lt;&lt;&lt;\n            logger.debug(\"Finished processing results from asyncio.gather.\")\n             # ^^^^^^^^^^^^^^^^^^^^^^^^\n\n            # 5. Finalize initialization state\n            self.models = temp_models # Assign successfully loaded models to the manager\n\n            if not self.models:\n                # All models failed to load\n                raise ModelError(\"Initialization failed: No models were loaded successfully.\")\n\n            self.initialized = True\n            init_duration = time.time() - init_start_time\n            logger.info(f\"ModelManager initialized successfully with {len(self.models)} loaded models in {init_duration:.2f}s.\")\n            if failed_loads:\n                 # Log warnings for models that failed\n                 logger.warning(f\"Models failed to load during initialization: {', '.join(failed_loads)}\")\n\n\n        except (ModelError, ConfigurationError, ResourceError) as e:\n            # Catch specific known errors during initialization phase\n            logger.error(f\"ModelManager initialization failed: {e}. Triggering shutdown.\", exc_info=True)\n            await self.shutdown() # Attempt graceful cleanup\n            raise # Re-raise the caught exception\n        except Exception as e:\n            # Catch any other unexpected errors\n            logger.error(f\"Unexpected error during ModelManager initialization: {e}. Triggering shutdown.\", exc_info=True)\n            await self.shutdown() # Attempt graceful cleanup\n            # Wrap in ModelError or re-raise depending on desired behavior\n            raise ModelError(f\"Unexpected initialization error: {e}\") from e\n\n    async def load_models(self, model_ids: Optional[List[str]] = None) -&gt; Tuple[int, int]:\n        \"\"\"Loads specified models or attempts to load all un-loaded configured models.\n\n        Uses the same async loading mechanism as initialize. Primarily for loading\n        models that failed during init or were manually unloaded.\n\n        Args:\n            model_ids: List of model IDs to load. If None, tries all models from\n                       config that are not currently loaded in the manager.\n\n        Returns:\n            Tuple (successful_loads, failed_loads).\n        \"\"\"\n        if model_ids is None:\n            # Load all models from config not currently present or loaded\n            all_config_ids = set(self.config_manager.get_all_models_config().keys())\n            loaded_ids = set(mid for mid, m in self.models.items() if m.is_loaded())\n            ids_to_consider = list(all_config_ids - loaded_ids)\n            logger.info(f\"Attempting to load all models defined in config that are not currently loaded ({len(ids_to_consider)} models).\")\n        else:\n            # Load specific requested models if they exist in config and aren't loaded\n            all_config_ids = set(self.config_manager.get_all_models_config().keys())\n            valid_ids_in_request = [mid for mid in model_ids if mid in all_config_ids]\n\n            if len(valid_ids_in_request) != len(model_ids):\n                 missing_in_config = set(model_ids) - set(valid_ids_in_request)\n                 logger.warning(f\"Requested models not defined in config and will be ignored: {list(missing_in_config)}\")\n\n            # Filter out models already loaded\n            ids_to_consider = [mid for mid in valid_ids_in_request if mid not in self.models or not self.models[mid].is_loaded()]\n            already_loaded_count = len(valid_ids_in_request) - len(ids_to_consider)\n            if already_loaded_count &gt; 0:\n                 logger.info(f\"{already_loaded_count} requested models are already loaded and will be skipped.\")\n\n        if not ids_to_consider:\n            logger.info(\"No models specified or require loading.\")\n            # Return reflects status of originally requested (if any) vs. needs loading\n            initial_request_count = len(model_ids) if model_ids is not None else 0\n            return initial_request_count, 0 # Report 0 failures for *this* operation\n\n        # --- Create and run load tasks ---\n        load_tasks = []\n        task_to_model_id = {} # Map index back to model_id\n        logger.info(f\"Preparing to load {len(ids_to_consider)} models...\")\n        for i, model_id in enumerate(ids_to_consider):\n            task = self._load_model_async(model_id) # Use the core async loader\n            load_tasks.append(task)\n            task_to_model_id[i] = model_id\n\n        if not load_tasks:\n             logger.error(\"Internal error: No load tasks created despite having models to consider.\")\n             return 0, len(ids_to_consider) # All considered models failed to start task\n\n        logger.info(f\"Loading {len(load_tasks)} models concurrently (Hardware limit: {self._max_concurrent_loads})...\")\n        start_time = time.time()\n\n        # Gather results (semaphore limits actual concurrency)\n        load_results = await asyncio.gather(*load_tasks, return_exceptions=True)\n\n        exec_time = time.time() - start_time\n        logger.info(f\"Finished loading models batch in {exec_time:.2f}s.\")\n\n        # --- Process results ---\n        success_count = 0\n        fail_count = 0\n        newly_loaded_models = {} # Store successfully loaded models from THIS batch\n\n        for i, result in enumerate(load_results):\n            model_id_loaded = task_to_model_id.get(i, f\"Unknown Task {i}\")\n\n            if isinstance(result, Exception):\n                logger.error(f\"Load task for model '{model_id_loaded}' failed directly in gather (exception type: {type(result).__name__}): {result}\", exc_info=result)\n                fail_count += 1\n            elif isinstance(result, tuple) and len(result) == 3:\n                 res_id, model_instance, error = result\n                 if model_id_loaded != res_id: logger.warning(f\"Model ID mismatch: expected '{model_id_loaded}', got '{res_id}'\")\n\n                 if error:\n                      logger.error(f\"Load task for model '{res_id}' reported failure: {error}\", exc_info=error if isinstance(error, Exception) else None)\n                      fail_count += 1\n                 elif model_instance is not None and isinstance(model_instance, GGUFModel):\n                      logger.debug(f\"Successfully loaded model '{res_id}' in this batch.\")\n                      newly_loaded_models[res_id] = model_instance\n                      success_count += 1\n                 else:\n                      logger.error(f\"Load task for model '{res_id}' returned success tuple but no valid model instance.\")\n                      fail_count += 1\n            else:\n                 logger.error(f\"Unexpected result format '{type(result)}' from load task for model '{model_id_loaded}'.\")\n                 fail_count += 1\n\n        # Update the manager's dictionary ONLY with newly loaded models from this batch\n        self.models.update(newly_loaded_models)\n\n        if fail_count &gt; 0:\n             logger.warning(f\"Model loading batch complete for requested IDs. Newly Loaded: {success_count}, Failures in batch: {fail_count}\")\n        else:\n             logger.info(f\"Model loading batch successful. Newly loaded: {success_count}\")\n\n        return success_count, fail_count # Return counts for THIS batch operation\n\n    async def shutdown(self) -&gt; None:\n        \"\"\"Shutdown the ModelManager: Unloads models and shuts down the executor.\"\"\"\n        if not self.models and self.executor is None and not self.initialized:\n             # Avoid logging shutdown if nothing was ever really started\n             logger.info(\"ModelManager already shut down or was not initialized.\")\n             return\n\n        logger.info(\"Shutting down ModelManager...\")\n        shutdown_start_time = time.time()\n\n        # --- Unload Models ---\n        unload_tasks = []\n        # Copy items to avoid modification during iteration if needed, though list() might suffice\n        active_models = list(self.models.values())\n        if active_models:\n             logger.debug(f\"Preparing to unload {len(active_models)} managed models...\")\n             for model in active_models:\n                  if model.is_loaded():\n                      # Run async unload (which likely calls sync unload in executor)\n                      # Make unload itself potentially async if it helps\n                      unload_tasks.append(run_in_threadpool(model.unload, _executor=self.executor)) # Run unload in pool\n                  else:\n                      # Clean up references if needed for unloaded models\n                      if hasattr(model, '_set_executor'): model._set_executor(None)\n\n        if unload_tasks:\n             logger.debug(f\"Executing {len(unload_tasks)} unload tasks concurrently...\")\n             unload_results = await asyncio.gather(*unload_tasks, return_exceptions=True)\n             # Log any errors during unload\n             for i, res in enumerate(unload_results):\n                 # Find corresponding model (order should match active_models used)\n                 # model_id_unloaded = active_models[i]._model_id if i &lt; len(active_models) and hasattr(active_models[i], '_model_id') else f\"Unknown Task {i}\" # Original attempt\n                 model_id_unloaded = active_models[i].get_id() if i &lt; len(\n                     active_models) else f\"Unknown Task {i}\"  # Use getter method\n                 if isinstance(res, Exception):\n                     logger.error(f\"Error during unload task for model '{model_id_unloaded}': {res}\", exc_info=res)\n                 elif res is False:  # If unload() can return False on failure\n                     logger.warning(f\"Unload method returned False for model '{model_id_unloaded}'.\")\n             logger.debug(\"Finished executing unload tasks.\")\n\n\n        # --- Shutdown Executor ---\n        # Should happen AFTER all tasks using it (including unload) are complete\n        if self.executor:\n            logger.debug(\"Shutting down thread pool executor...\")\n            try:\n                # Wait for all submitted tasks to complete before shutting down\n                self.executor.shutdown(wait=True, cancel_futures=False) # cancel_futures=False is safer usually\n                logger.debug(\"Executor shut down successfully.\")\n            except Exception as e:\n                 logger.error(f\"Error shutting down executor: {e}\")\n            finally:\n                 self.executor = None # Ensure it's marked as gone\n\n        # --- Clear State ---\n        self.models = {} # Clear the dictionary of model instances\n        self.initialized = False # Mark as not initialized\n\n        shutdown_duration = time.time() - shutdown_start_time\n        logger.info(f\"ModelManager shut down successfully in {shutdown_duration:.2f}s.\")\n\n    async def run_inference(\n            self,\n            model_id: str,\n            prompt: str,\n            compute_confidence: bool = True,\n            parameters_override: Optional[Dict[str, Any]] = None,\n            **kwargs: Any\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Run inference on a specific model using its generate method via thread pool.\"\"\"\n        # Combine explicit args with kwargs for passing down, giving precedence to kwargs\n        inference_kwargs = kwargs.copy()\n        if parameters_override:\n            inference_kwargs.update(parameters_override)  # Merge overrides\n        # Ensure compute_confidence flag is correctly passed or overridden\n        inference_kwargs['compute_confidence'] = compute_confidence\n\n        if not self.initialized:\n            # Check if we intend to allow inference before full init (e.g. loading on demand)\n            # For now, strict check:\n            raise ModelError(\"ModelManager not initialized. Call initialize() first.\")\n\n        # Retrieve the model instance\n        if model_id not in self.models:\n            raise ModelError(f\"Model not found: {model_id}\")\n\n        model = self.models[model_id]\n\n        # Ensure model is loaded\n        if not model.is_loaded():\n            # Option: Attempt to load it here? Or just fail? Currently fails.\n            # await self.load_models([model_id]) # Example load-on-demand (needs error handling)\n            # if not model.is_loaded(): raise ModelError(...)\n            raise ModelError(f\"Model '{model_id}' is not loaded.\")\n\n        # --- DIAGNOSTIC FLAG (Can be removed if confidence is stable) ---\n        internal_compute_confidence = False\n        role = model.get_role()\n        model_id_with_role = f\"{model_id} ({role})\" if role else model_id\n\n        if not internal_compute_confidence:\n            # Ensure the flag passed down to generate doesn't falsely trigger logprobs if we skip calc\n            inference_kwargs['compute_confidence'] = False\n            logger.debug(f\"Diagnostic: internal_compute_confidence override: False for model {model_id_with_role}\")\n        else:\n            logger.debug(f\"Diagnostic: internal_compute_confidence: True for model {model_id_with_role}\")\n        # --- END DIAGNOSTIC FLAG ---\n\n        total_start_time = time.time()\n        thread_name = threading.current_thread().name  # Coordinating thread\n\n        # Use the model's specific lock to prevent concurrent generation ON THE SAME instance\n        # Assumes GGUFModel has `_inference_lock = asyncio.Lock()`\n        if not hasattr(model, '_inference_lock') or not isinstance(getattr(model, '_inference_lock', None),\n                                                                   asyncio.Lock):\n            logger.warning(\n                f\"Model {model_id_with_role} lacks a valid '_inference_lock'. Inference will proceed without instance-level locking.\")\n            lock_context = asyncio.Lock()  # Create temp lock CANCELED - this locks ALL, use dummy\n\n            # Define a dummy async context manager that does nothing\n            class NoLock:\n                async def __aenter__(self): pass\n\n                async def __aexit__(self, exc_type, exc, tb): pass\n\n            lock_context = NoLock()\n        else:\n            lock_context = model._inference_lock  # Use the model's lock\n\n        async with lock_context:\n            logger.debug(f\"[{thread_name}] Acquired inference lock for model {model_id_with_role}\")\n            try:\n                # Call the GGUFModel's generate method (which uses run_in_threadpool)\n                logger.debug(f\"[{thread_name}] Calling model.generate for {model_id_with_role}\")\n                # Pass the prompt and all collected kwargs\n                generation_result = await model.generate(\n                    prompt=prompt,\n                    **inference_kwargs  # Includes compute_confidence, overrides, etc.\n                )\n                logger.debug(f\"[{thread_name}] model.generate completed for {model_id_with_role}\")\n\n                # --- Post-processing (if GGUFModel.generate doesn't do it all) ---\n                # GGUFModel.generate should ideally return the full desired dict including metadata\n                # If not, do minimal additions here:\n                result_with_meta = copy.deepcopy(generation_result)  # Assume generate returns a dict\n                result_with_meta[\"model_id\"] = model_id  # Ensure model ID is present\n                result_with_meta[\"role\"] = model.get_role()  # Add role info\n\n                # --- Confidence calculation (IF ENABLED INTERNALLY) ---\n                if internal_compute_confidence:\n                    logger.debug(f\"[{thread_name}] Attempting confidence calculation for {model_id_with_role}\")\n                    # This part might need adjustment based on what 'model.generate' returns\n                    # Assuming generate returns 'text' and 'raw_output' needed by get_confidence_score\n                    try:\n                        confidence_config = self.config_manager.get_confidence_config()\n                        conf_method = inference_kwargs.get(\"confidence_method\",\n                                                           confidence_config.get(\"default_method\", \"combined\"))\n                        # Extract relevant keys for confidence calculation from kwargs or config\n                        conf_kwargs_keys = {\n                            \"token_prob_weight\", \"self_eval_weight\", \"consistency_weight\",\n                            \"consistency_samples\", \"consistency_temperature\",\n                            \"consistency_max_tokens\", \"token_metric\", \"self_eval_template\"\n                        }\n                        conf_kwargs = {k: confidence_config.get(k) for k in conf_kwargs_keys if k in confidence_config}\n                        conf_kwargs.update({k: v for k, v in inference_kwargs.items() if k in conf_kwargs_keys})\n\n                        confidence_scores = await get_confidence_score(\n                            model=model,  # Pass the GGUFModel instance\n                            prompt=prompt,  # Original prompt\n                            response=generation_result.get(\"text\", \"\"),  # Generated text\n                            model_output=generation_result.get(\"raw_output\", {}),\n                            # Raw output potentially containing logprobs\n                            method=conf_method,\n                            **conf_kwargs\n                        )\n                        result_with_meta[\"confidence\"] = confidence_scores  # Add/update confidence\n                        logger.debug(f\"[{thread_name}] Confidence calculation successful for {model_id_with_role}\")\n                    except Exception as conf_e:\n                        logger.error(\n                            f\"[{thread_name}] Failed to compute confidence for model {model_id_with_role}: {conf_e}\",\n                            exc_info=True)\n                        result_with_meta[\"confidence\"] = {\"error\": str(conf_e), \"combined\": 0.5}  # Default error state\n                else:\n                    # Ensure key exists even if skipped (might be None or added by generate)\n                    if \"confidence\" not in result_with_meta:\n                        result_with_meta[\"confidence\"] = None\n                    logger.debug(\n                        f\"[{thread_name}] Confidence calculation skipped for {model_id_with_role} due to diagnostic flag.\")\n                # --- End Confidence Block ---\n\n                total_time = time.time() - total_start_time\n                result_with_meta[\"total_inference_time\"] = total_time  # More specific name?\n\n                logger.debug(\n                    f\"[{thread_name}] Completed run_inference coordinator for {model_id_with_role} in {total_time:.2f}s\")\n                return result_with_meta\n\n            except ModelError as e:\n                # Model-specific error already logged by generate, re-raise\n                logger.error(f\"[{thread_name}] ModelError during inference coordination for {model_id_with_role}: {e}\",\n                             exc_info=True)\n                raise\n            except Exception as e:\n                # Catch unexpected errors during the locked section/coordination\n                total_time = time.time() - total_start_time\n                logger.error(\n                    f\"[{thread_name}] Unexpected error during inference coordination for model {model_id_with_role} after {total_time:.2f}s: {str(e)}\",\n                    exc_info=True)\n                # Wrap in ModelError for consistency\n                raise ModelError(\n                    f\"Inference coordination failed unexpectedly for model {model_id_with_role}: {str(e)}\") from e\n            finally:\n                logger.debug(f\"[{thread_name}] Releasing inference lock for model {model_id_with_role}\")\n                # Lock released automatically by 'async with'\n\n    async def run_all_models(self,\n                            prompt: str,\n                            model_ids: Optional[List[str]] = None,\n                            compute_confidence: bool = True,\n                            parameters_override: Optional[Dict[str, Any]] = None,\n                            **kwargs) -&gt; Dict[str, Dict[str, Any]]:\n        \"\"\"Runs inference concurrently on specified or all loaded models.\"\"\"\n        if not self.initialized:\n             logger.warning(\"ModelManager not initialized, returning empty results for run_all_models.\")\n             return {}\n\n        start_time = time.time()\n\n        # Determine which models to run\n        if model_ids is None:\n            # Run all currently loaded models\n            ids_to_run = [mid for mid, m in self.models.items() if m.is_loaded()]\n            logger.info(f\"Running inference on all {len(ids_to_run)} loaded models.\")\n        else:\n            # Run only specified models that are loaded\n            ids_to_run = []\n            ignored_ids = []\n            for mid in model_ids:\n                if mid in self.models and self.models[mid].is_loaded():\n                    ids_to_run.append(mid)\n                else:\n                    ignored_ids.append(mid)\n            if ignored_ids:\n                 logger.warning(f\"Ignoring requested models for run_all_models as they are not loaded or managed: {ignored_ids}\")\n            logger.info(f\"Running inference on specified loaded models: {ids_to_run}\")\n\n\n        if not ids_to_run:\n            logger.warning(\"No loaded models specified or available to run inference in run_all_models.\")\n            return {}\n\n        # --- Create and run inference tasks ---\n        concurrency_limit = self._max_concurrent_inference\n        logger.info(f\"Running inference tasks concurrently (Hardware limit: {concurrency_limit}).\")\n\n        results: Dict[str, Dict[str, Any]] = {} # Store results keyed by model_id\n        tasks = []\n        task_to_model_id = {} # Map task index to model ID for better error association\n\n        for i, model_id in enumerate(ids_to_run):\n            # Create coroutine for run_inference\n            task_coro = self.run_inference(\n                 model_id=model_id,\n                 prompt=prompt,\n                 compute_confidence=compute_confidence,\n                 parameters_override=parameters_override,\n                 **kwargs # Pass any other relevant kwargs\n            )\n            tasks.append(task_coro)\n            task_to_model_id[i] = model_id # Store mapping\n\n        logger.debug(f\"Launching {len(tasks)} inference tasks...\")\n        # Use gather_with_concurrency to limit simultaneous executions\n        inference_results_list = await gather_with_concurrency(\n            concurrency_limit,\n            *tasks,\n            return_exceptions=True # Capture exceptions from run_inference tasks\n        )\n        logger.debug(\"All inference tasks completed or failed.\")\n\n        # --- Process results ---\n        success_count = 0\n        fail_count = 0\n        for i, task_result in enumerate(inference_results_list):\n            current_model_id = task_to_model_id.get(i, f\"Unknown Task {i}\") # Get model ID for this result\n\n            if isinstance(task_result, Exception):\n                 logger.error(f\"Inference task for model '{current_model_id}' failed with exception (type: {type(task_result).__name__}): {task_result}\", exc_info=task_result)\n                 fail_count += 1\n                 # Store error under the correct model ID\n                 results[current_model_id] = {\n                     \"error\": f\"Task Exception: {type(task_result).__name__}: {str(task_result)}\",\n                     \"model_id\": current_model_id, # Include model_id in error dict\n                     \"role\": self.models.get(current_model_id, None).get_role() if current_model_id in self.models else None # Add role if possible\n                }\n\n            elif isinstance(task_result, dict) and \"model_id\" in task_result:\n                 # Result is a dictionary, likely success or partial failure from run_inference\n                 model_id_res = task_result[\"model_id\"]\n                 if model_id_res != current_model_id:\n                     logger.warning(f\"Task index model ID '{current_model_id}' differs from result dict model ID '{model_id_res}'. Using result dict ID.\")\n\n                 if \"error\" in task_result: # Check if run_inference itself returned an error dict\n                      logger.warning(f\"Model '{model_id_res}' reported error during inference: {task_result['error']}\")\n                      fail_count += 1\n                 else:\n                      # Assume success if dict has model_id and no 'error' key\n                      logger.debug(f\"Model '{model_id_res}' inference successful.\")\n                      success_count += 1\n                 results[model_id_res] = task_result # Store the dictionary result\n            else:\n                 # Unexpected result type from gather\n                 logger.error(f\"Unexpected result format '{type(task_result)}' from inference task for model '{current_model_id}'\")\n                 fail_count += 1\n                 results[current_model_id] = {\n                    \"error\": f\"Unexpected task result format: {type(task_result)}\",\n                    \"model_id\": current_model_id,\n                    \"role\": self.models.get(current_model_id, None).get_role() if current_model_id in self.models else None\n                }\n\n        total_exec_time = time.time() - start_time\n        logger.info(f\"Finished run_all_models in {total_exec_time:.2f}s. Successes: {success_count}, Failures: {fail_count}.\")\n\n        return results\n\n    # --- Getter and Helper Methods ---\n\n    def get_model(self, model_id: str) -&gt; GGUFModel:\n        \"\"\"Retrieve a specific managed model instance by ID.\"\"\"\n        if model_id not in self.models:\n             # Distinguish between not initialized and model simply not found/loaded\n             if not self.initialized and not self.models:\n                  raise ModelError(\"ModelManager not initialized\")\n             raise ModelError(f\"Model not found or not loaded: {model_id}\")\n        return self.models[model_id]\n\n    def get_models_by_role(self, role: str) -&gt; List[GGUFModel]:\n        \"\"\"Get a list of loaded models matching a specific role.\"\"\"\n        if not self.initialized: return [] # Or raise error? Returning empty list is safer.\n        if not role or not isinstance(role, str):\n             logger.warning(f\"Invalid role requested: {role}\")\n             return []\n        # Filter models currently in the manager\n        matching_models = [\n             m for m in self.models.values()\n             if m.is_loaded() and m.get_role() is not None and m.get_role().lower() == role.lower()\n        ]\n        logger.debug(f\"Found {len(matching_models)} loaded models with role '{role}'.\")\n        return matching_models\n\n    def get_model_ids(self) -&gt; Set[str]:\n        \"\"\"Get the set of model IDs currently managed (loaded or not).\"\"\"\n        return set(self.models.keys())\n\n    def get_loaded_model_ids(self) -&gt; Set[str]:\n        \"\"\"Get the set of model IDs that are currently successfully loaded.\"\"\"\n        return {mid for mid, m in self.models.items() if m.is_loaded()}\n\n    def get_roles(self) -&gt; Set[str]:\n        \"\"\"Get the set of unique roles assigned to the managed models.\"\"\"\n        if not self.initialized and not self.models: return set()\n        # Use walrus operator (Python 3.8+) for conciseness\n        roles = {role for m in self.models.values() if (role := m.get_role())}\n        return roles\n\n    def get_random_model(self, loaded_only: bool = True) -&gt; Optional[GGUFModel]:\n        \"\"\"Get a random model instance, optionally restricted to loaded models.\"\"\"\n        if not self.models: return None # No models managed\n\n        eligible_model_ids = []\n        if loaded_only:\n            eligible_model_ids = [mid for mid, m in self.models.items() if m.is_loaded()]\n            if not eligible_model_ids:\n                 logger.warning(\"No models are currently loaded to select a random one from.\")\n                 return None\n        else:\n            eligible_model_ids = list(self.models.keys())\n            if not eligible_model_ids: return None # Should not happen if self.models is not empty\n\n        try:\n             random_model_id = random.choice(eligible_model_ids)\n             return self.models[random_model_id]\n        except IndexError: # Should not happen if eligible_model_ids is checked\n             logger.error(\"Error selecting random model (IndexError despite checks).\")\n             return None\n        except KeyError: # If ID somehow not in self.models after selection (concurrency?)\n             logger.error(f\"Error retrieving random model '{random_model_id}' after selection (KeyError).\")\n             return None\n\n    def get_model_count(self, loaded_only: bool = False) -&gt; int:\n        \"\"\"Get the count of managed models, optionally only loaded ones.\"\"\"\n        if loaded_only:\n             # Count loaded models even if manager isn't fully \"initialized\" yet\n             return sum(1 for m in self.models.values() if m.is_loaded())\n        else:\n             # Return count of all model wrappers held (reflects config)\n             return len(self.models)\n\n    def is_model_available(self, model_id: str) -&gt; bool:\n         \"\"\"Checks if a model ID is managed AND currently loaded.\"\"\"\n         return model_id in self.models and self.models[model_id].is_loaded()\n</code></pre>"},{"location":"api/models/#ai_ensemble_suite.models.ModelManager.__init__","title":"<code>__init__(config_manager, ensemble=None, max_workers=None)</code>","text":"<p>Initialize the ModelManager.</p> <p>Parameters:</p> Name Type Description Default <code>config_manager</code> <code>ConfigProvider</code> <p>Configuration manager instance that implements ConfigProvider.</p> required <code>ensemble</code> <code>Optional[Ensemble]</code> <p>Optional reference to the parent Ensemble instance.</p> <code>None</code> <code>max_workers</code> <code>Optional[int]</code> <p>Optional limit for thread pool size.</p> <code>None</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If config_manager doesn't implement required methods.</p> Source code in <code>src\\ai_ensemble_suite\\models\\model_manager.py</code> <pre><code>def __init__(\n        self,\n        config_manager: ConfigProvider,\n        ensemble: Optional[\"Ensemble\"] = None,  # Add ensemble parameter\n        max_workers: Optional[int] = None\n) -&gt; None:\n    \"\"\"Initialize the ModelManager.\n\n    Args:\n        config_manager: Configuration manager instance that implements ConfigProvider.\n        ensemble: Optional reference to the parent Ensemble instance.\n        max_workers: Optional limit for thread pool size.\n\n    Raises:\n        TypeError: If config_manager doesn't implement required methods.\n    \"\"\"\n    # Validate the ConfigProvider implementation\n    required_methods = [\n        'get_all_models_config', 'get_template', 'get_confidence_config',\n        'get_default_model_parameters', 'get_model_config'\n    ]\n    if not all(hasattr(config_manager, method) for method in required_methods):\n        missing = [m for m in required_methods if not hasattr(config_manager, m)]\n        raise TypeError(f\"config_manager must implement the ConfigProvider protocol methods. Missing: {missing}\")\n\n    self.config_manager = config_manager\n    self.ensemble = ensemble  # Store reference to parent ensemble\n    self.models: Dict[str, GGUFModel] = {}  # Stores successfully loaded GGUFModel instances\n    self.initialized = False\n\n    # Determine max workers for the thread pool\n    if max_workers is None:\n         try:\n             # Use physical cores as a baseline, clamp between reasonable limits\n             cores = psutil.cpu_count(logical=False) or 1\n             # Example: Limit to 8 workers max by default, even if more cores\n             max_workers = min(cores, 8)\n             logger.info(f\"Determined default max_workers based on physical cores: {max_workers}\")\n         except Exception as e:\n             logger.warning(f\"Could not detect CPU cores: {e}. Using default max_workers=4\")\n             max_workers = 4 # Fallback default\n\n    # Ensure at least 1 worker\n    max_workers = max(1, max_workers)\n\n    # Create the thread pool executor\n    self.executor = ThreadPoolExecutor(max_workers=max_workers, thread_name_prefix=\"GGUFWorker\")\n\n    # --- Concurrency Control ---\n    # Limit concurrent loads (often IO/CPU bound init) more strictly than inference\n    # Example: Allow half the workers for loading concurrently\n    self._max_concurrent_loads: int = max(1, max_workers // 2)\n    self._max_concurrent_loads = 1  # Force limit to 1\n    self._load_semaphore = asyncio.Semaphore(self._max_concurrent_loads)\n\n    # Limit concurrent inference calls (can be adjusted)\n    self._max_concurrent_inference: int = max_workers # Allow all workers for inference by default\n\n    logger.info(f\"Initialized ModelManager with up to {max_workers} worker threads.\")\n    logger.info(f\"Concurrency limits set: Loads={self._max_concurrent_loads}, Inference={self._max_concurrent_inference}\")\n</code></pre>"},{"location":"api/models/#ai_ensemble_suite.models.ModelManager.get_loaded_model_ids","title":"<code>get_loaded_model_ids()</code>","text":"<p>Get the set of model IDs that are currently successfully loaded.</p> Source code in <code>src\\ai_ensemble_suite\\models\\model_manager.py</code> <pre><code>def get_loaded_model_ids(self) -&gt; Set[str]:\n    \"\"\"Get the set of model IDs that are currently successfully loaded.\"\"\"\n    return {mid for mid, m in self.models.items() if m.is_loaded()}\n</code></pre>"},{"location":"api/models/#ai_ensemble_suite.models.ModelManager.get_model","title":"<code>get_model(model_id)</code>","text":"<p>Retrieve a specific managed model instance by ID.</p> Source code in <code>src\\ai_ensemble_suite\\models\\model_manager.py</code> <pre><code>def get_model(self, model_id: str) -&gt; GGUFModel:\n    \"\"\"Retrieve a specific managed model instance by ID.\"\"\"\n    if model_id not in self.models:\n         # Distinguish between not initialized and model simply not found/loaded\n         if not self.initialized and not self.models:\n              raise ModelError(\"ModelManager not initialized\")\n         raise ModelError(f\"Model not found or not loaded: {model_id}\")\n    return self.models[model_id]\n</code></pre>"},{"location":"api/models/#ai_ensemble_suite.models.ModelManager.get_model_count","title":"<code>get_model_count(loaded_only=False)</code>","text":"<p>Get the count of managed models, optionally only loaded ones.</p> Source code in <code>src\\ai_ensemble_suite\\models\\model_manager.py</code> <pre><code>def get_model_count(self, loaded_only: bool = False) -&gt; int:\n    \"\"\"Get the count of managed models, optionally only loaded ones.\"\"\"\n    if loaded_only:\n         # Count loaded models even if manager isn't fully \"initialized\" yet\n         return sum(1 for m in self.models.values() if m.is_loaded())\n    else:\n         # Return count of all model wrappers held (reflects config)\n         return len(self.models)\n</code></pre>"},{"location":"api/models/#ai_ensemble_suite.models.ModelManager.get_model_ids","title":"<code>get_model_ids()</code>","text":"<p>Get the set of model IDs currently managed (loaded or not).</p> Source code in <code>src\\ai_ensemble_suite\\models\\model_manager.py</code> <pre><code>def get_model_ids(self) -&gt; Set[str]:\n    \"\"\"Get the set of model IDs currently managed (loaded or not).\"\"\"\n    return set(self.models.keys())\n</code></pre>"},{"location":"api/models/#ai_ensemble_suite.models.ModelManager.get_models_by_role","title":"<code>get_models_by_role(role)</code>","text":"<p>Get a list of loaded models matching a specific role.</p> Source code in <code>src\\ai_ensemble_suite\\models\\model_manager.py</code> <pre><code>def get_models_by_role(self, role: str) -&gt; List[GGUFModel]:\n    \"\"\"Get a list of loaded models matching a specific role.\"\"\"\n    if not self.initialized: return [] # Or raise error? Returning empty list is safer.\n    if not role or not isinstance(role, str):\n         logger.warning(f\"Invalid role requested: {role}\")\n         return []\n    # Filter models currently in the manager\n    matching_models = [\n         m for m in self.models.values()\n         if m.is_loaded() and m.get_role() is not None and m.get_role().lower() == role.lower()\n    ]\n    logger.debug(f\"Found {len(matching_models)} loaded models with role '{role}'.\")\n    return matching_models\n</code></pre>"},{"location":"api/models/#ai_ensemble_suite.models.ModelManager.get_random_model","title":"<code>get_random_model(loaded_only=True)</code>","text":"<p>Get a random model instance, optionally restricted to loaded models.</p> Source code in <code>src\\ai_ensemble_suite\\models\\model_manager.py</code> <pre><code>def get_random_model(self, loaded_only: bool = True) -&gt; Optional[GGUFModel]:\n    \"\"\"Get a random model instance, optionally restricted to loaded models.\"\"\"\n    if not self.models: return None # No models managed\n\n    eligible_model_ids = []\n    if loaded_only:\n        eligible_model_ids = [mid for mid, m in self.models.items() if m.is_loaded()]\n        if not eligible_model_ids:\n             logger.warning(\"No models are currently loaded to select a random one from.\")\n             return None\n    else:\n        eligible_model_ids = list(self.models.keys())\n        if not eligible_model_ids: return None # Should not happen if self.models is not empty\n\n    try:\n         random_model_id = random.choice(eligible_model_ids)\n         return self.models[random_model_id]\n    except IndexError: # Should not happen if eligible_model_ids is checked\n         logger.error(\"Error selecting random model (IndexError despite checks).\")\n         return None\n    except KeyError: # If ID somehow not in self.models after selection (concurrency?)\n         logger.error(f\"Error retrieving random model '{random_model_id}' after selection (KeyError).\")\n         return None\n</code></pre>"},{"location":"api/models/#ai_ensemble_suite.models.ModelManager.get_roles","title":"<code>get_roles()</code>","text":"<p>Get the set of unique roles assigned to the managed models.</p> Source code in <code>src\\ai_ensemble_suite\\models\\model_manager.py</code> <pre><code>def get_roles(self) -&gt; Set[str]:\n    \"\"\"Get the set of unique roles assigned to the managed models.\"\"\"\n    if not self.initialized and not self.models: return set()\n    # Use walrus operator (Python 3.8+) for conciseness\n    roles = {role for m in self.models.values() if (role := m.get_role())}\n    return roles\n</code></pre>"},{"location":"api/models/#ai_ensemble_suite.models.ModelManager.initialize","title":"<code>initialize()</code>  <code>async</code>","text":"<p>Initialize the ModelManager: Instantiates models and loads them asynchronously.</p> Source code in <code>src\\ai_ensemble_suite\\models\\model_manager.py</code> <pre><code>async def initialize(self) -&gt; None:\n    \"\"\"Initialize the ModelManager: Instantiates models and loads them asynchronously.\"\"\"\n    if self.initialized:\n        logger.warning(\"ModelManager already initialized. Skipping.\")\n        return\n\n    logger.info(\"Initializing ModelManager...\")\n    init_start_time = time.time()\n    model_configs: Dict[str, Dict[str, Any]] = {}\n\n    try:\n        # 1. Get configurations for all models\n        model_configs = self.config_manager.get_all_models_config()\n        if not model_configs:\n            raise ConfigurationError(\"No models found in configuration\")\n        logger.info(f\"Found {len(model_configs)} models in configuration.\")\n\n        # 2. Create asynchronous loading tasks for each model\n        load_tasks = []\n        task_to_model_id_map = {} # Map index to model_id for result processing\n        valid_model_ids_to_load = list(model_configs.keys()) # Keep track\n\n        for i, model_id in enumerate(valid_model_ids_to_load):\n            logger.debug(f\"Creating async load task {i} for model {model_id}\")\n            # Use the helper that handles semaphore and calls sync part in executor\n            task = self._load_model_async(model_id)\n            load_tasks.append(task)\n            task_to_model_id_map[i] = model_id # Store mapping BEFORE awaiting\n\n        if not load_tasks:\n            raise ModelError(\"No load tasks were created (possibly no valid models in config).\")\n\n        # 3. Run loading tasks concurrently using asyncio.gather\n        # The semaphore inside _load_model_async limits actual concurrency.\n        logger.info(f\"Loading {len(load_tasks)} models concurrently (Hardware limit via semaphore: {self._max_concurrent_loads})...\")\n        # We can use simple gather here as semaphore controls concurrency\n        load_results = await asyncio.gather(*load_tasks, return_exceptions=True)\n\n        # 4. Process the results\n        temp_models = {} # Dict to store successfully loaded model instances\n        failed_loads = [] # List to store IDs of models that failed to load\n\n        # &gt;&gt;&gt; NEW LOGGING LINE &lt;&lt;&lt;\n        logger.debug(f\"Processing {len(load_results)} results from asyncio.gather in initialize()...\")\n        # ^^^^^^^^^^^^^^^^^^^^^^^^\n        for i, result in enumerate(load_results):\n            # Get the model ID corresponding to this result index\n            model_id = task_to_model_id_map.get(i, f\"Unknown Task {i}\")\n            # &gt;&gt;&gt; NEW LOGGING LINE &lt;&lt;&lt;\n            logger.debug(f\"Processing result index {i} for model_id '{model_id}'...\")\n            # ^^^^^^^^^^^^^^^^^^^^^^^^\n\n            if isinstance(result, Exception):\n                # Task itself raised exception (e.g., during coordination in _load_model_async)\n                logger.error(f\"Load task for model '{model_id}' failed directly in gather (exception type: {type(result).__name__}): {result}\", exc_info=result)\n                failed_loads.append(model_id)\n            elif isinstance(result, tuple) and len(result) == 3:\n                # Expected result format: (model_id, model_instance | None, exception | None)\n                res_id, model_instance, error = result\n                if model_id != res_id: # Sanity check\n                     logger.warning(f\"Model ID mismatch in result processing: expected '{model_id}', got '{res_id}'. Using '{res_id}'.\")\n                     model_id = res_id # Trust the ID returned from the task\n\n                if error:\n                     # _perform_load_sync caught an exception or load returned False\n                     logger.error(f\"Load task for model '{model_id}' reported failure (exception type: {type(error).__name__}): {error}\", exc_info=error if isinstance(error, Exception) else None)\n                     failed_loads.append(model_id)\n                elif model_instance is not None and isinstance(model_instance, GGUFModel):\n                     # Successfully loaded\n                     # &gt;&gt;&gt; NEW LOGGING LINE &lt;&lt;&lt;\n                     logger.debug(f\"Successfully processed load result for model '{model_id}'. Adding to temp_models.\")\n                     # ^^^^^^^^^^^^^^^^^^^^^^^^\n                     temp_models[model_id] = model_instance\n                else:\n                     # Should not happen: success tuple but no instance\n                     logger.error(f\"Load task for model '{model_id}' returned success tuple but no valid model instance.\")\n                     failed_loads.append(model_id)\n            else:\n                 # Unexpected format from gather (should be Exception or Tuple)\n                 logger.error(f\"Unexpected result format '{type(result)}' from load task for model '{model_id}'.\")\n                 failed_loads.append(model_id)\n\n        # &gt;&gt;&gt; NEW LOGGING LINE &lt;&lt;&lt;\n        logger.debug(\"Finished processing results from asyncio.gather.\")\n         # ^^^^^^^^^^^^^^^^^^^^^^^^\n\n        # 5. Finalize initialization state\n        self.models = temp_models # Assign successfully loaded models to the manager\n\n        if not self.models:\n            # All models failed to load\n            raise ModelError(\"Initialization failed: No models were loaded successfully.\")\n\n        self.initialized = True\n        init_duration = time.time() - init_start_time\n        logger.info(f\"ModelManager initialized successfully with {len(self.models)} loaded models in {init_duration:.2f}s.\")\n        if failed_loads:\n             # Log warnings for models that failed\n             logger.warning(f\"Models failed to load during initialization: {', '.join(failed_loads)}\")\n\n\n    except (ModelError, ConfigurationError, ResourceError) as e:\n        # Catch specific known errors during initialization phase\n        logger.error(f\"ModelManager initialization failed: {e}. Triggering shutdown.\", exc_info=True)\n        await self.shutdown() # Attempt graceful cleanup\n        raise # Re-raise the caught exception\n    except Exception as e:\n        # Catch any other unexpected errors\n        logger.error(f\"Unexpected error during ModelManager initialization: {e}. Triggering shutdown.\", exc_info=True)\n        await self.shutdown() # Attempt graceful cleanup\n        # Wrap in ModelError or re-raise depending on desired behavior\n        raise ModelError(f\"Unexpected initialization error: {e}\") from e\n</code></pre>"},{"location":"api/models/#ai_ensemble_suite.models.ModelManager.is_model_available","title":"<code>is_model_available(model_id)</code>","text":"<p>Checks if a model ID is managed AND currently loaded.</p> Source code in <code>src\\ai_ensemble_suite\\models\\model_manager.py</code> <pre><code>def is_model_available(self, model_id: str) -&gt; bool:\n     \"\"\"Checks if a model ID is managed AND currently loaded.\"\"\"\n     return model_id in self.models and self.models[model_id].is_loaded()\n</code></pre>"},{"location":"api/models/#ai_ensemble_suite.models.ModelManager.load_models","title":"<code>load_models(model_ids=None)</code>  <code>async</code>","text":"<p>Loads specified models or attempts to load all un-loaded configured models.</p> <p>Uses the same async loading mechanism as initialize. Primarily for loading models that failed during init or were manually unloaded.</p> <p>Parameters:</p> Name Type Description Default <code>model_ids</code> <code>Optional[List[str]]</code> <p>List of model IDs to load. If None, tries all models from        config that are not currently loaded in the manager.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[int, int]</code> <p>Tuple (successful_loads, failed_loads).</p> Source code in <code>src\\ai_ensemble_suite\\models\\model_manager.py</code> <pre><code>async def load_models(self, model_ids: Optional[List[str]] = None) -&gt; Tuple[int, int]:\n    \"\"\"Loads specified models or attempts to load all un-loaded configured models.\n\n    Uses the same async loading mechanism as initialize. Primarily for loading\n    models that failed during init or were manually unloaded.\n\n    Args:\n        model_ids: List of model IDs to load. If None, tries all models from\n                   config that are not currently loaded in the manager.\n\n    Returns:\n        Tuple (successful_loads, failed_loads).\n    \"\"\"\n    if model_ids is None:\n        # Load all models from config not currently present or loaded\n        all_config_ids = set(self.config_manager.get_all_models_config().keys())\n        loaded_ids = set(mid for mid, m in self.models.items() if m.is_loaded())\n        ids_to_consider = list(all_config_ids - loaded_ids)\n        logger.info(f\"Attempting to load all models defined in config that are not currently loaded ({len(ids_to_consider)} models).\")\n    else:\n        # Load specific requested models if they exist in config and aren't loaded\n        all_config_ids = set(self.config_manager.get_all_models_config().keys())\n        valid_ids_in_request = [mid for mid in model_ids if mid in all_config_ids]\n\n        if len(valid_ids_in_request) != len(model_ids):\n             missing_in_config = set(model_ids) - set(valid_ids_in_request)\n             logger.warning(f\"Requested models not defined in config and will be ignored: {list(missing_in_config)}\")\n\n        # Filter out models already loaded\n        ids_to_consider = [mid for mid in valid_ids_in_request if mid not in self.models or not self.models[mid].is_loaded()]\n        already_loaded_count = len(valid_ids_in_request) - len(ids_to_consider)\n        if already_loaded_count &gt; 0:\n             logger.info(f\"{already_loaded_count} requested models are already loaded and will be skipped.\")\n\n    if not ids_to_consider:\n        logger.info(\"No models specified or require loading.\")\n        # Return reflects status of originally requested (if any) vs. needs loading\n        initial_request_count = len(model_ids) if model_ids is not None else 0\n        return initial_request_count, 0 # Report 0 failures for *this* operation\n\n    # --- Create and run load tasks ---\n    load_tasks = []\n    task_to_model_id = {} # Map index back to model_id\n    logger.info(f\"Preparing to load {len(ids_to_consider)} models...\")\n    for i, model_id in enumerate(ids_to_consider):\n        task = self._load_model_async(model_id) # Use the core async loader\n        load_tasks.append(task)\n        task_to_model_id[i] = model_id\n\n    if not load_tasks:\n         logger.error(\"Internal error: No load tasks created despite having models to consider.\")\n         return 0, len(ids_to_consider) # All considered models failed to start task\n\n    logger.info(f\"Loading {len(load_tasks)} models concurrently (Hardware limit: {self._max_concurrent_loads})...\")\n    start_time = time.time()\n\n    # Gather results (semaphore limits actual concurrency)\n    load_results = await asyncio.gather(*load_tasks, return_exceptions=True)\n\n    exec_time = time.time() - start_time\n    logger.info(f\"Finished loading models batch in {exec_time:.2f}s.\")\n\n    # --- Process results ---\n    success_count = 0\n    fail_count = 0\n    newly_loaded_models = {} # Store successfully loaded models from THIS batch\n\n    for i, result in enumerate(load_results):\n        model_id_loaded = task_to_model_id.get(i, f\"Unknown Task {i}\")\n\n        if isinstance(result, Exception):\n            logger.error(f\"Load task for model '{model_id_loaded}' failed directly in gather (exception type: {type(result).__name__}): {result}\", exc_info=result)\n            fail_count += 1\n        elif isinstance(result, tuple) and len(result) == 3:\n             res_id, model_instance, error = result\n             if model_id_loaded != res_id: logger.warning(f\"Model ID mismatch: expected '{model_id_loaded}', got '{res_id}'\")\n\n             if error:\n                  logger.error(f\"Load task for model '{res_id}' reported failure: {error}\", exc_info=error if isinstance(error, Exception) else None)\n                  fail_count += 1\n             elif model_instance is not None and isinstance(model_instance, GGUFModel):\n                  logger.debug(f\"Successfully loaded model '{res_id}' in this batch.\")\n                  newly_loaded_models[res_id] = model_instance\n                  success_count += 1\n             else:\n                  logger.error(f\"Load task for model '{res_id}' returned success tuple but no valid model instance.\")\n                  fail_count += 1\n        else:\n             logger.error(f\"Unexpected result format '{type(result)}' from load task for model '{model_id_loaded}'.\")\n             fail_count += 1\n\n    # Update the manager's dictionary ONLY with newly loaded models from this batch\n    self.models.update(newly_loaded_models)\n\n    if fail_count &gt; 0:\n         logger.warning(f\"Model loading batch complete for requested IDs. Newly Loaded: {success_count}, Failures in batch: {fail_count}\")\n    else:\n         logger.info(f\"Model loading batch successful. Newly loaded: {success_count}\")\n\n    return success_count, fail_count # Return counts for THIS batch operation\n</code></pre>"},{"location":"api/models/#ai_ensemble_suite.models.ModelManager.run_all_models","title":"<code>run_all_models(prompt, model_ids=None, compute_confidence=True, parameters_override=None, **kwargs)</code>  <code>async</code>","text":"<p>Runs inference concurrently on specified or all loaded models.</p> Source code in <code>src\\ai_ensemble_suite\\models\\model_manager.py</code> <pre><code>async def run_all_models(self,\n                        prompt: str,\n                        model_ids: Optional[List[str]] = None,\n                        compute_confidence: bool = True,\n                        parameters_override: Optional[Dict[str, Any]] = None,\n                        **kwargs) -&gt; Dict[str, Dict[str, Any]]:\n    \"\"\"Runs inference concurrently on specified or all loaded models.\"\"\"\n    if not self.initialized:\n         logger.warning(\"ModelManager not initialized, returning empty results for run_all_models.\")\n         return {}\n\n    start_time = time.time()\n\n    # Determine which models to run\n    if model_ids is None:\n        # Run all currently loaded models\n        ids_to_run = [mid for mid, m in self.models.items() if m.is_loaded()]\n        logger.info(f\"Running inference on all {len(ids_to_run)} loaded models.\")\n    else:\n        # Run only specified models that are loaded\n        ids_to_run = []\n        ignored_ids = []\n        for mid in model_ids:\n            if mid in self.models and self.models[mid].is_loaded():\n                ids_to_run.append(mid)\n            else:\n                ignored_ids.append(mid)\n        if ignored_ids:\n             logger.warning(f\"Ignoring requested models for run_all_models as they are not loaded or managed: {ignored_ids}\")\n        logger.info(f\"Running inference on specified loaded models: {ids_to_run}\")\n\n\n    if not ids_to_run:\n        logger.warning(\"No loaded models specified or available to run inference in run_all_models.\")\n        return {}\n\n    # --- Create and run inference tasks ---\n    concurrency_limit = self._max_concurrent_inference\n    logger.info(f\"Running inference tasks concurrently (Hardware limit: {concurrency_limit}).\")\n\n    results: Dict[str, Dict[str, Any]] = {} # Store results keyed by model_id\n    tasks = []\n    task_to_model_id = {} # Map task index to model ID for better error association\n\n    for i, model_id in enumerate(ids_to_run):\n        # Create coroutine for run_inference\n        task_coro = self.run_inference(\n             model_id=model_id,\n             prompt=prompt,\n             compute_confidence=compute_confidence,\n             parameters_override=parameters_override,\n             **kwargs # Pass any other relevant kwargs\n        )\n        tasks.append(task_coro)\n        task_to_model_id[i] = model_id # Store mapping\n\n    logger.debug(f\"Launching {len(tasks)} inference tasks...\")\n    # Use gather_with_concurrency to limit simultaneous executions\n    inference_results_list = await gather_with_concurrency(\n        concurrency_limit,\n        *tasks,\n        return_exceptions=True # Capture exceptions from run_inference tasks\n    )\n    logger.debug(\"All inference tasks completed or failed.\")\n\n    # --- Process results ---\n    success_count = 0\n    fail_count = 0\n    for i, task_result in enumerate(inference_results_list):\n        current_model_id = task_to_model_id.get(i, f\"Unknown Task {i}\") # Get model ID for this result\n\n        if isinstance(task_result, Exception):\n             logger.error(f\"Inference task for model '{current_model_id}' failed with exception (type: {type(task_result).__name__}): {task_result}\", exc_info=task_result)\n             fail_count += 1\n             # Store error under the correct model ID\n             results[current_model_id] = {\n                 \"error\": f\"Task Exception: {type(task_result).__name__}: {str(task_result)}\",\n                 \"model_id\": current_model_id, # Include model_id in error dict\n                 \"role\": self.models.get(current_model_id, None).get_role() if current_model_id in self.models else None # Add role if possible\n            }\n\n        elif isinstance(task_result, dict) and \"model_id\" in task_result:\n             # Result is a dictionary, likely success or partial failure from run_inference\n             model_id_res = task_result[\"model_id\"]\n             if model_id_res != current_model_id:\n                 logger.warning(f\"Task index model ID '{current_model_id}' differs from result dict model ID '{model_id_res}'. Using result dict ID.\")\n\n             if \"error\" in task_result: # Check if run_inference itself returned an error dict\n                  logger.warning(f\"Model '{model_id_res}' reported error during inference: {task_result['error']}\")\n                  fail_count += 1\n             else:\n                  # Assume success if dict has model_id and no 'error' key\n                  logger.debug(f\"Model '{model_id_res}' inference successful.\")\n                  success_count += 1\n             results[model_id_res] = task_result # Store the dictionary result\n        else:\n             # Unexpected result type from gather\n             logger.error(f\"Unexpected result format '{type(task_result)}' from inference task for model '{current_model_id}'\")\n             fail_count += 1\n             results[current_model_id] = {\n                \"error\": f\"Unexpected task result format: {type(task_result)}\",\n                \"model_id\": current_model_id,\n                \"role\": self.models.get(current_model_id, None).get_role() if current_model_id in self.models else None\n            }\n\n    total_exec_time = time.time() - start_time\n    logger.info(f\"Finished run_all_models in {total_exec_time:.2f}s. Successes: {success_count}, Failures: {fail_count}.\")\n\n    return results\n</code></pre>"},{"location":"api/models/#ai_ensemble_suite.models.ModelManager.run_inference","title":"<code>run_inference(model_id, prompt, compute_confidence=True, parameters_override=None, **kwargs)</code>  <code>async</code>","text":"<p>Run inference on a specific model using its generate method via thread pool.</p> Source code in <code>src\\ai_ensemble_suite\\models\\model_manager.py</code> <pre><code>async def run_inference(\n        self,\n        model_id: str,\n        prompt: str,\n        compute_confidence: bool = True,\n        parameters_override: Optional[Dict[str, Any]] = None,\n        **kwargs: Any\n) -&gt; Dict[str, Any]:\n    \"\"\"Run inference on a specific model using its generate method via thread pool.\"\"\"\n    # Combine explicit args with kwargs for passing down, giving precedence to kwargs\n    inference_kwargs = kwargs.copy()\n    if parameters_override:\n        inference_kwargs.update(parameters_override)  # Merge overrides\n    # Ensure compute_confidence flag is correctly passed or overridden\n    inference_kwargs['compute_confidence'] = compute_confidence\n\n    if not self.initialized:\n        # Check if we intend to allow inference before full init (e.g. loading on demand)\n        # For now, strict check:\n        raise ModelError(\"ModelManager not initialized. Call initialize() first.\")\n\n    # Retrieve the model instance\n    if model_id not in self.models:\n        raise ModelError(f\"Model not found: {model_id}\")\n\n    model = self.models[model_id]\n\n    # Ensure model is loaded\n    if not model.is_loaded():\n        # Option: Attempt to load it here? Or just fail? Currently fails.\n        # await self.load_models([model_id]) # Example load-on-demand (needs error handling)\n        # if not model.is_loaded(): raise ModelError(...)\n        raise ModelError(f\"Model '{model_id}' is not loaded.\")\n\n    # --- DIAGNOSTIC FLAG (Can be removed if confidence is stable) ---\n    internal_compute_confidence = False\n    role = model.get_role()\n    model_id_with_role = f\"{model_id} ({role})\" if role else model_id\n\n    if not internal_compute_confidence:\n        # Ensure the flag passed down to generate doesn't falsely trigger logprobs if we skip calc\n        inference_kwargs['compute_confidence'] = False\n        logger.debug(f\"Diagnostic: internal_compute_confidence override: False for model {model_id_with_role}\")\n    else:\n        logger.debug(f\"Diagnostic: internal_compute_confidence: True for model {model_id_with_role}\")\n    # --- END DIAGNOSTIC FLAG ---\n\n    total_start_time = time.time()\n    thread_name = threading.current_thread().name  # Coordinating thread\n\n    # Use the model's specific lock to prevent concurrent generation ON THE SAME instance\n    # Assumes GGUFModel has `_inference_lock = asyncio.Lock()`\n    if not hasattr(model, '_inference_lock') or not isinstance(getattr(model, '_inference_lock', None),\n                                                               asyncio.Lock):\n        logger.warning(\n            f\"Model {model_id_with_role} lacks a valid '_inference_lock'. Inference will proceed without instance-level locking.\")\n        lock_context = asyncio.Lock()  # Create temp lock CANCELED - this locks ALL, use dummy\n\n        # Define a dummy async context manager that does nothing\n        class NoLock:\n            async def __aenter__(self): pass\n\n            async def __aexit__(self, exc_type, exc, tb): pass\n\n        lock_context = NoLock()\n    else:\n        lock_context = model._inference_lock  # Use the model's lock\n\n    async with lock_context:\n        logger.debug(f\"[{thread_name}] Acquired inference lock for model {model_id_with_role}\")\n        try:\n            # Call the GGUFModel's generate method (which uses run_in_threadpool)\n            logger.debug(f\"[{thread_name}] Calling model.generate for {model_id_with_role}\")\n            # Pass the prompt and all collected kwargs\n            generation_result = await model.generate(\n                prompt=prompt,\n                **inference_kwargs  # Includes compute_confidence, overrides, etc.\n            )\n            logger.debug(f\"[{thread_name}] model.generate completed for {model_id_with_role}\")\n\n            # --- Post-processing (if GGUFModel.generate doesn't do it all) ---\n            # GGUFModel.generate should ideally return the full desired dict including metadata\n            # If not, do minimal additions here:\n            result_with_meta = copy.deepcopy(generation_result)  # Assume generate returns a dict\n            result_with_meta[\"model_id\"] = model_id  # Ensure model ID is present\n            result_with_meta[\"role\"] = model.get_role()  # Add role info\n\n            # --- Confidence calculation (IF ENABLED INTERNALLY) ---\n            if internal_compute_confidence:\n                logger.debug(f\"[{thread_name}] Attempting confidence calculation for {model_id_with_role}\")\n                # This part might need adjustment based on what 'model.generate' returns\n                # Assuming generate returns 'text' and 'raw_output' needed by get_confidence_score\n                try:\n                    confidence_config = self.config_manager.get_confidence_config()\n                    conf_method = inference_kwargs.get(\"confidence_method\",\n                                                       confidence_config.get(\"default_method\", \"combined\"))\n                    # Extract relevant keys for confidence calculation from kwargs or config\n                    conf_kwargs_keys = {\n                        \"token_prob_weight\", \"self_eval_weight\", \"consistency_weight\",\n                        \"consistency_samples\", \"consistency_temperature\",\n                        \"consistency_max_tokens\", \"token_metric\", \"self_eval_template\"\n                    }\n                    conf_kwargs = {k: confidence_config.get(k) for k in conf_kwargs_keys if k in confidence_config}\n                    conf_kwargs.update({k: v for k, v in inference_kwargs.items() if k in conf_kwargs_keys})\n\n                    confidence_scores = await get_confidence_score(\n                        model=model,  # Pass the GGUFModel instance\n                        prompt=prompt,  # Original prompt\n                        response=generation_result.get(\"text\", \"\"),  # Generated text\n                        model_output=generation_result.get(\"raw_output\", {}),\n                        # Raw output potentially containing logprobs\n                        method=conf_method,\n                        **conf_kwargs\n                    )\n                    result_with_meta[\"confidence\"] = confidence_scores  # Add/update confidence\n                    logger.debug(f\"[{thread_name}] Confidence calculation successful for {model_id_with_role}\")\n                except Exception as conf_e:\n                    logger.error(\n                        f\"[{thread_name}] Failed to compute confidence for model {model_id_with_role}: {conf_e}\",\n                        exc_info=True)\n                    result_with_meta[\"confidence\"] = {\"error\": str(conf_e), \"combined\": 0.5}  # Default error state\n            else:\n                # Ensure key exists even if skipped (might be None or added by generate)\n                if \"confidence\" not in result_with_meta:\n                    result_with_meta[\"confidence\"] = None\n                logger.debug(\n                    f\"[{thread_name}] Confidence calculation skipped for {model_id_with_role} due to diagnostic flag.\")\n            # --- End Confidence Block ---\n\n            total_time = time.time() - total_start_time\n            result_with_meta[\"total_inference_time\"] = total_time  # More specific name?\n\n            logger.debug(\n                f\"[{thread_name}] Completed run_inference coordinator for {model_id_with_role} in {total_time:.2f}s\")\n            return result_with_meta\n\n        except ModelError as e:\n            # Model-specific error already logged by generate, re-raise\n            logger.error(f\"[{thread_name}] ModelError during inference coordination for {model_id_with_role}: {e}\",\n                         exc_info=True)\n            raise\n        except Exception as e:\n            # Catch unexpected errors during the locked section/coordination\n            total_time = time.time() - total_start_time\n            logger.error(\n                f\"[{thread_name}] Unexpected error during inference coordination for model {model_id_with_role} after {total_time:.2f}s: {str(e)}\",\n                exc_info=True)\n            # Wrap in ModelError for consistency\n            raise ModelError(\n                f\"Inference coordination failed unexpectedly for model {model_id_with_role}: {str(e)}\") from e\n        finally:\n            logger.debug(f\"[{thread_name}] Releasing inference lock for model {model_id_with_role}\")\n</code></pre>"},{"location":"api/models/#ai_ensemble_suite.models.ModelManager.shutdown","title":"<code>shutdown()</code>  <code>async</code>","text":"<p>Shutdown the ModelManager: Unloads models and shuts down the executor.</p> Source code in <code>src\\ai_ensemble_suite\\models\\model_manager.py</code> <pre><code>async def shutdown(self) -&gt; None:\n    \"\"\"Shutdown the ModelManager: Unloads models and shuts down the executor.\"\"\"\n    if not self.models and self.executor is None and not self.initialized:\n         # Avoid logging shutdown if nothing was ever really started\n         logger.info(\"ModelManager already shut down or was not initialized.\")\n         return\n\n    logger.info(\"Shutting down ModelManager...\")\n    shutdown_start_time = time.time()\n\n    # --- Unload Models ---\n    unload_tasks = []\n    # Copy items to avoid modification during iteration if needed, though list() might suffice\n    active_models = list(self.models.values())\n    if active_models:\n         logger.debug(f\"Preparing to unload {len(active_models)} managed models...\")\n         for model in active_models:\n              if model.is_loaded():\n                  # Run async unload (which likely calls sync unload in executor)\n                  # Make unload itself potentially async if it helps\n                  unload_tasks.append(run_in_threadpool(model.unload, _executor=self.executor)) # Run unload in pool\n              else:\n                  # Clean up references if needed for unloaded models\n                  if hasattr(model, '_set_executor'): model._set_executor(None)\n\n    if unload_tasks:\n         logger.debug(f\"Executing {len(unload_tasks)} unload tasks concurrently...\")\n         unload_results = await asyncio.gather(*unload_tasks, return_exceptions=True)\n         # Log any errors during unload\n         for i, res in enumerate(unload_results):\n             # Find corresponding model (order should match active_models used)\n             # model_id_unloaded = active_models[i]._model_id if i &lt; len(active_models) and hasattr(active_models[i], '_model_id') else f\"Unknown Task {i}\" # Original attempt\n             model_id_unloaded = active_models[i].get_id() if i &lt; len(\n                 active_models) else f\"Unknown Task {i}\"  # Use getter method\n             if isinstance(res, Exception):\n                 logger.error(f\"Error during unload task for model '{model_id_unloaded}': {res}\", exc_info=res)\n             elif res is False:  # If unload() can return False on failure\n                 logger.warning(f\"Unload method returned False for model '{model_id_unloaded}'.\")\n         logger.debug(\"Finished executing unload tasks.\")\n\n\n    # --- Shutdown Executor ---\n    # Should happen AFTER all tasks using it (including unload) are complete\n    if self.executor:\n        logger.debug(\"Shutting down thread pool executor...\")\n        try:\n            # Wait for all submitted tasks to complete before shutting down\n            self.executor.shutdown(wait=True, cancel_futures=False) # cancel_futures=False is safer usually\n            logger.debug(\"Executor shut down successfully.\")\n        except Exception as e:\n             logger.error(f\"Error shutting down executor: {e}\")\n        finally:\n             self.executor = None # Ensure it's marked as gone\n\n    # --- Clear State ---\n    self.models = {} # Clear the dictionary of model instances\n    self.initialized = False # Mark as not initialized\n\n    shutdown_duration = time.time() - shutdown_start_time\n    logger.info(f\"ModelManager shut down successfully in {shutdown_duration:.2f}s.\")\n</code></pre>"},{"location":"api/models/#ai_ensemble_suite.models.calculate_similarity","title":"<code>calculate_similarity(text1, text2)</code>","text":"<p>Calculate semantic similarity between two texts.</p> <p>Uses a combined n-gram and word-level approach suitable for GGUF models without requiring external embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>text1</code> <code>str</code> <p>First text.</p> required <code>text2</code> <code>str</code> <p>Second text.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Similarity score between 0.0 and 1.0.</p> Source code in <code>src\\ai_ensemble_suite\\models\\confidence.py</code> <pre><code>def calculate_similarity(\n    text1: str,\n    text2: str\n) -&gt; float:\n    \"\"\"Calculate semantic similarity between two texts.\n\n    Uses a combined n-gram and word-level approach suitable for GGUF models\n    without requiring external embeddings.\n\n    Args:\n        text1: First text.\n        text2: Second text.\n\n    Returns:\n        Similarity score between 0.0 and 1.0.\n    \"\"\"\n    # Quick exact match check\n    if text1 == text2:\n        return 1.0\n\n    # Handle empty or very short texts\n    if not text1 or not text2 or len(text1) &lt; 5 or len(text2) &lt; 5:\n        # For very short or empty texts, simple comparison\n        return 1.0 if text1 == text2 else 0.0\n\n    # Check length difference - if too different, decrease similarity\n    len_ratio = min(len(text1), len(text2)) / max(len(text1), len(text2))\n    # Consider a less harsh penalty than direct return\n    # if len_ratio &lt; 0.3:\n    #     return 0.3 * len_ratio\n\n    # Basic normalization\n    text1_norm = text1.lower().strip()\n    text2_norm = text2.lower().strip()\n\n    # Simple word-level overlap calculation\n    try:\n        words1 = set(re.findall(r'\\b\\w+\\b', text1_norm))\n        words2 = set(re.findall(r'\\b\\w+\\b', text2_norm))\n    except Exception as e:\n        logger.warning(f\"Regex failed in calculate_similarity: {e}\")\n        return 0.5 # Fallback\n\n    # Handle empty sets after regex\n    if not words1 or not words2:\n        return 0.0 if words1 != words2 else 1.0\n\n    # For very small word sets, use direct set ratio\n    # Increase threshold slightly\n    if len(words1) &lt; 5 or len(words2) &lt; 5:\n        intersection_len = len(words1.intersection(words2))\n        max_len = max(len(words1), len(words2))\n        return intersection_len / max_len if max_len &gt; 0 else 0.0\n\n    # Jaccard similarity (Word level)\n    intersection = len(words1.intersection(words2))\n    union = len(words1.union(words2))\n    jaccard = intersection / union if union &gt; 0 else 0.0\n\n    # N-gram similarity (adjust n based on text length?)\n    def get_ngrams(text: str, n: int = 3) -&gt; Set[str]:\n        try:\n            tokens = re.findall(r'\\b\\w+\\b', text) # Use normalized text\n            if len(tokens) &lt; n:\n                return set() # Not enough tokens for n-gram\n            ngrams = [' '.join(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n            return set(ngrams)\n        except Exception as e:\n            logger.warning(f\"Regex/Ngram failed in get_ngrams: {e}\")\n            return set()\n\n    # Calculate trigram similarity\n    ngrams1 = get_ngrams(text1_norm, n=3)\n    ngrams2 = get_ngrams(text2_norm, n=3)\n    ngram_jaccard = 0.0\n    if ngrams1 and ngrams2:\n        intersection_ngram = len(ngrams1.intersection(ngrams2))\n        union_ngram = len(ngrams1.union(ngrams2))\n        ngram_jaccard = intersection_ngram / union_ngram if union_ngram &gt; 0 else 0.0\n    elif ngrams1 == ngrams2: # Both empty, means short text handled earlier or identical\n         ngram_jaccard = 1.0\n\n    # Compute a weighted similarity score\n    # Increased weight for ngram similarity, slightly reduced length ratio impact\n    # Weights: 35% word Jaccard, 45% n-gram Jaccard, 20% length ratio\n    weighted_score = 0.35 * jaccard + 0.45 * ngram_jaccard + 0.20 * len_ratio\n    return min(max(weighted_score, 0.0), 1.0) # Ensure score is within [0, 1]\n</code></pre>"},{"location":"api/models/#ai_ensemble_suite.models.calculate_token_confidence","title":"<code>calculate_token_confidence(model_output)</code>  <code>async</code>","text":"<p>Calculate confidence based on token probabilities.</p> <p>Parameters:</p> Name Type Description Default <code>model_output</code> <code>Dict[str, Any]</code> <p>The raw output from the model containing token information (e.g., from llama_cpp.Llama.create_completion).</p> required <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dictionary with different confidence metrics:</p> <code>Dict[str, float]</code> <ul> <li>mean: Average token probability</li> </ul> <code>Dict[str, float]</code> <ul> <li>min: Minimum token probability</li> </ul> <code>Dict[str, float]</code> <ul> <li>geometric_mean: Geometric mean of token probabilities</li> </ul> <code>Dict[str, float]</code> <ul> <li>median: Median token probability</li> </ul> Source code in <code>src\\ai_ensemble_suite\\models\\confidence.py</code> <pre><code>async def calculate_token_confidence(\n    model_output: Dict[str, Any]\n) -&gt; Dict[str, float]:\n    \"\"\"Calculate confidence based on token probabilities.\n\n    Args:\n        model_output: The raw output from the model containing token information\n            (e.g., from llama_cpp.Llama.create_completion).\n\n    Returns:\n        Dictionary with different confidence metrics:\n        - mean: Average token probability\n        - min: Minimum token probability\n        - geometric_mean: Geometric mean of token probabilities\n        - median: Median token probability\n    \"\"\"\n    # Extract token probabilities if available\n    # Llama-cpp returns logprobs in the 'logprobs' key (if requested) which contains 'top_logprobs' per token\n    # Or sometimes simple 'tokens' might have probabilities (less common for generation)\n    # We need to handle the structure returned by llama-cpp\n    logprobs_data = model_output.get(\"logprobs\")\n    probabilities = []\n\n    if logprobs_data and \"token_logprobs\" in logprobs_data:\n        # Prefer token_logprobs if available\n        token_logprobs = logprobs_data[\"token_logprobs\"]\n        # Filter out None values (e.g., for the first token) and negative infinity\n        valid_logprobs = [lp for lp in token_logprobs if lp is not None and lp &gt; -float('inf')]\n        if valid_logprobs:\n            probabilities = [math.exp(lp) for lp in valid_logprobs]\n\n    # Fallback or alternative: Check 'tokens' list if it contains probabilities\n    elif \"tokens\" in model_output:\n        tokens = model_output.get(\"tokens\", [])\n        for token_info in tokens:\n            # Check various possible keys for probability or log probability\n            if \"probability\" in token_info and isinstance(token_info[\"probability\"], (int, float)) and token_info[\"probability\"] &gt; 0:\n                probabilities.append(float(token_info[\"probability\"]))\n            elif \"prob\" in token_info and isinstance(token_info[\"prob\"], (int, float)) and token_info[\"prob\"] &gt; 0:\n                probabilities.append(float(token_info[\"prob\"]))\n            elif \"logprob\" in token_info and isinstance(token_info[\"logprob\"], (int, float)):\n                try:\n                    prob = math.exp(float(token_info[\"logprob\"]))\n                    if prob &gt; 0:\n                        probabilities.append(prob)\n                except OverflowError:\n                    pass # Ignore if logprob is too small\n\n    # If no probabilities available, return default confidence\n    if not probabilities:\n        logger.warning(\"No valid token probabilities found in model output for confidence calculation\")\n        return {\n            \"mean\": 0.7,  # Default medium-high confidence\n            \"min\": 0.7,\n            \"geometric_mean\": 0.7,\n            \"median\": 0.7\n        }\n\n    # Filter out any zero or negative probabilities that might sneak in\n    probabilities = [p for p in probabilities if p &gt; 0]\n    if not probabilities:\n        logger.warning(\"All extracted probabilities were non-positive.\")\n        return { \"mean\": 0.5, \"min\": 0.5, \"geometric_mean\": 0.5, \"median\": 0.5 }\n\n    # Calculate different confidence metrics\n    mean_prob = statistics.mean(probabilities)\n    min_prob = min(probabilities)\n\n    # Geometric mean calculation\n    try:\n        # Ensure all values are positive for geometric mean (already done, but double check)\n        positive_probs = [max(p, 1e-10) for p in probabilities] # Use a small epsilon\n        geometric_mean = statistics.geometric_mean(positive_probs) if positive_probs else 0.0\n    except (ValueError, statistics.StatisticsError) as e:\n        logger.warning(f\"Could not calculate geometric mean: {e}\")\n        geometric_mean = 0.0 # Or consider using mean as fallback\n\n    # Median calculation\n    median_prob = statistics.median(probabilities)\n\n    return {\n        \"mean\": mean_prob,\n        \"min\": min_prob,\n        \"geometric_mean\": geometric_mean,\n        \"median\": median_prob\n    }\n</code></pre>"},{"location":"api/models/#ai_ensemble_suite.models.get_combined_confidence","title":"<code>get_combined_confidence(model, prompt, response, model_output, token_prob_weight=0.5, self_eval_weight=0.3, consistency_weight=0.2, **kwargs)</code>  <code>async</code>","text":"<p>Calculate a weighted confidence score using the 'combined' method.</p> <p>This function is deprecated. Use get_confidence_score with method='combined' instead.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>GGUFModel</code> <p>The model that generated the response.</p> required <code>prompt</code> <code>str</code> <p>The prompt sent to the model.</p> required <code>response</code> <code>str</code> <p>The generated response.</p> required <code>model_output</code> <code>Dict[str, Any]</code> <p>The raw model output with token information.</p> required <code>token_prob_weight</code> <code>float</code> <p>Weight for token probability (default: 0.5).</p> <code>0.5</code> <code>self_eval_weight</code> <code>float</code> <p>Weight for self-evaluation (default: 0.3).</p> <code>0.3</code> <code>consistency_weight</code> <code>float</code> <p>Weight for consistency (default: 0.2).</p> <code>0.2</code> <code>**kwargs</code> <code>Any</code> <p>Additional args for underlying confidence methods.</p> <code>{}</code> <p>Returns:</p> Type Description <code>float</code> <p>Combined confidence score between 0.0 and 1.0.</p> Source code in <code>src\\ai_ensemble_suite\\models\\confidence.py</code> <pre><code>async def get_combined_confidence(\n    model: 'GGUFModel',\n    prompt: str,\n    response: str,\n    model_output: Dict[str, Any],\n    token_prob_weight: float = 0.5,\n    self_eval_weight: float = 0.3,\n    consistency_weight: float = 0.2,\n    **kwargs: Any # Pass other kwargs through\n) -&gt; float:\n    \"\"\"Calculate a weighted confidence score using the 'combined' method.\n\n    This function is deprecated. Use get_confidence_score with method='combined' instead.\n\n    Args:\n        model: The model that generated the response.\n        prompt: The prompt sent to the model.\n        response: The generated response.\n        model_output: The raw model output with token information.\n        token_prob_weight: Weight for token probability (default: 0.5).\n        self_eval_weight: Weight for self-evaluation (default: 0.3).\n        consistency_weight: Weight for consistency (default: 0.2).\n        **kwargs: Additional args for underlying confidence methods.\n\n    Returns:\n        Combined confidence score between 0.0 and 1.0.\n    \"\"\"\n    logger.warning(\"'get_combined_confidence' is deprecated. \"\n                   \"Consider using 'get_confidence_score' with method='combined'.\")\n\n    # Pass weights and other kwargs to get_confidence_score\n    confidence_scores = await get_confidence_score(\n        model,\n        prompt,\n        response,\n        model_output,\n        method=\"combined\",\n        token_prob_weight=token_prob_weight,\n        self_eval_weight=self_eval_weight,\n        consistency_weight=consistency_weight,\n        **kwargs # Pass through other relevant args like consistency_samples etc.\n    )\n\n    return confidence_scores.get(\"combined\", 0.5)\n</code></pre>"},{"location":"api/models/#ai_ensemble_suite.models.get_confidence_score","title":"<code>get_confidence_score(model, prompt, response, model_output, method='combined', **kwargs)</code>  <code>async</code>","text":"<p>Get confidence score(s) using the specified method(s).</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>GGUFModel</code> <p>The model instance used for generation/evaluation.</p> required <code>prompt</code> <code>str</code> <p>The prompt sent to the model for the original response.</p> required <code>response</code> <code>str</code> <p>The response text generated by the model.</p> required <code>model_output</code> <code>Dict[str, Any]</code> <p>Raw model output data (containing logprobs etc.).</p> required <code>method</code> <code>str</code> <p>The confidence estimation method(s) to use: - \"token_prob\": Token probability aggregation. - \"self_eval\": Self-evaluation prompting. - \"consistency\": Response consistency check. - \"combined\": Calculate all available methods and combine them. You can also provide a list/tuple, e.g., [\"token_prob\", \"self_eval\"].</p> <code>'combined'</code> <code>**kwargs</code> <code>Any</code> <p>Additional parameters for specific methods: - token_prob_weight: Weight for token probability (default: 0.5). - self_eval_weight: Weight for self-evaluation (default: 0.3). - consistency_weight: Weight for consistency (default: 0.2). - consistency_samples: Number of samples for consistency check (default: 3). - consistency_temperature: Temperature for consistency samples (default: 0.7). - consistency_max_tokens: Max tokens for consistency samples (default: 150). - token_metric: Which token metric to use ('geometric_mean', 'mean', 'median', 'min') (default: 'geometric_mean').</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dictionary with confidence scores for each calculated method</p> <code>Dict[str, float]</code> <p>and potentially a \"combined\" score.</p> Source code in <code>src\\ai_ensemble_suite\\models\\confidence.py</code> <pre><code>async def get_confidence_score(\n    model: 'GGUFModel',\n    prompt: str,\n    response: str,\n    model_output: Dict[str, Any],\n    method: str = \"combined\",\n    **kwargs: Any\n) -&gt; Dict[str, float]:\n    \"\"\"Get confidence score(s) using the specified method(s).\n\n    Args:\n        model: The model instance used for generation/evaluation.\n        prompt: The prompt sent to the model for the original response.\n        response: The response text generated by the model.\n        model_output: Raw model output data (containing logprobs etc.).\n        method: The confidence estimation method(s) to use:\n            - \"token_prob\": Token probability aggregation.\n            - \"self_eval\": Self-evaluation prompting.\n            - \"consistency\": Response consistency check.\n            - \"combined\": Calculate all available methods and combine them.\n            You can also provide a list/tuple, e.g., [\"token_prob\", \"self_eval\"].\n        **kwargs: Additional parameters for specific methods:\n            - token_prob_weight: Weight for token probability (default: 0.5).\n            - self_eval_weight: Weight for self-evaluation (default: 0.3).\n            - consistency_weight: Weight for consistency (default: 0.2).\n            - consistency_samples: Number of samples for consistency check (default: 3).\n            - consistency_temperature: Temperature for consistency samples (default: 0.7).\n            - consistency_max_tokens: Max tokens for consistency samples (default: 150).\n            - token_metric: Which token metric to use ('geometric_mean', 'mean', 'median', 'min') (default: 'geometric_mean').\n\n    Returns:\n        Dictionary with confidence scores for each calculated method\n        and potentially a \"combined\" score.\n    \"\"\"\n    result: Dict[str, float] = {}\n    methods_to_run: Set[str] = set()\n\n    if isinstance(method, str):\n        if method == \"combined\":\n            methods_to_run = {\"token_prob\", \"self_eval\", \"consistency\"}\n        elif method in {\"token_prob\", \"self_eval\", \"consistency\"}:\n            methods_to_run = {method}\n        else:\n            logger.warning(f\"Unknown single confidence method: {method}, defaulting to combined.\")\n            methods_to_run = {\"token_prob\", \"self_eval\", \"consistency\"}\n    elif isinstance(method, (list, tuple, set)):\n        valid_methods = {m for m in method if m in {\"token_prob\", \"self_eval\", \"consistency\"}}\n        if not valid_methods:\n             logger.warning(f\"No valid methods provided in list: {method}, defaulting to combined.\")\n             methods_to_run = {\"token_prob\", \"self_eval\", \"consistency\"}\n        else:\n            methods_to_run = valid_methods\n    else:\n        logger.warning(f\"Invalid type for confidence method: {type(method)}, defaulting to combined.\")\n        methods_to_run = {\"token_prob\", \"self_eval\", \"consistency\"}\n\n\n    # Get and validate weights - use defaults matching the _validate function\n    token_prob_weight, self_eval_weight, consistency_weight = _validate_confidence_weights(\n        kwargs.get(\"token_prob_weight\", 0.5),\n        kwargs.get(\"self_eval_weight\", 0.3),\n        kwargs.get(\"consistency_weight\", 0.2)\n    )\n\n    # --- Calculate individual scores based on methods_to_run ---\n\n    if \"token_prob\" in methods_to_run:\n        token_metrics = await calculate_token_confidence(model_output)\n        # Use the specified metric or default to geometric mean\n        token_metric = kwargs.get(\"token_metric\", \"geometric_mean\")\n        if token_metric not in token_metrics:\n             logger.warning(f\"Specified token_metric '{token_metric}' not found, using 'geometric_mean'.\")\n             token_metric = \"geometric_mean\"\n        result[\"token_prob\"] = token_metrics[token_metric]\n        # Optionally include all token metrics\n        # result.update({f\"token_{k}\": v for k, v in token_metrics.items()})\n\n    if \"self_eval\" in methods_to_run:\n        # Check if model has generate method needed for self-evaluation\n        if hasattr(model, 'generate') and callable(model.generate):\n             self_eval_score = await get_model_self_evaluation(model, response)\n             result[\"self_eval\"] = self_eval_score\n        else:\n            logger.warning(f\"Model {model.get_id()} missing 'generate' method required for self-evaluation.\")\n\n    if \"consistency\" in methods_to_run:\n         # Check if model has generate method needed for consistency\n        if hasattr(model, 'generate') and callable(model.generate):\n            consistency_samples = kwargs.get(\"consistency_samples\", 3)\n            consistency_temperature = kwargs.get(\"consistency_temperature\", 0.7)\n            consistency_max_tokens = kwargs.get(\"consistency_max_tokens\", 150)\n            consistency_score = await measure_consistency_confidence(\n                model,\n                prompt,\n                num_samples=consistency_samples,\n                temperature=consistency_temperature,\n                max_tokens=consistency_max_tokens\n            )\n            result[\"consistency\"] = consistency_score\n        else:\n            logger.warning(f\"Model {model.get_id()} missing 'generate' method required for consistency check.\")\n\n    # --- Calculate combined score if requested or only one method was run ---\n    if method == \"combined\" or len(methods_to_run) &gt; 1:\n         result[\"combined\"] = _combine_confidence_scores(\n            result,\n            token_prob_weight,\n            self_eval_weight,\n            consistency_weight\n        )\n    elif len(methods_to_run) == 1:\n        # If only one method ran, its score is the \"combined\" score\n        single_method = list(methods_to_run)[0]\n        result[\"combined\"] = result.get(single_method, 0.5) # Use 0.5 if score somehow missing\n\n    return result\n</code></pre>"},{"location":"api/models/#ai_ensemble_suite.models.get_model_self_evaluation","title":"<code>get_model_self_evaluation(model, response, prompt_template='self_evaluation')</code>  <code>async</code>","text":"<p>Have model evaluate its own confidence.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>GGUFModel</code> <p>The model to evaluate confidence.</p> required <code>response</code> <code>str</code> <p>The response to evaluate.</p> required <code>prompt_template</code> <code>str</code> <p>The template name for self-evaluation prompting.</p> <code>'self_evaluation'</code> <p>Returns:</p> Type Description <code>float</code> <p>Confidence score between 0.0 and 1.0.</p> Source code in <code>src\\ai_ensemble_suite\\models\\confidence.py</code> <pre><code>async def get_model_self_evaluation(\n    model: 'GGUFModel',\n    response: str,\n    prompt_template: str = \"self_evaluation\"\n) -&gt; float:\n    \"\"\"Have model evaluate its own confidence.\n\n    Args:\n        model: The model to evaluate confidence.\n        response: The response to evaluate.\n        prompt_template: The template name for self-evaluation prompting.\n\n    Returns:\n        Confidence score between 0.0 and 1.0.\n    \"\"\"\n    try:\n        # Get template and format prompt\n        template = await _get_self_evaluation_template(model, prompt_template)\n        # Ensure response is passed correctly to formatting\n        prompt = format_prompt(template, response=response)\n\n        # Set parameters for confidence evaluation\n        # Short max_tokens, low temperature needed\n        params = {\n            \"temperature\": 0.1,  # Low temperature for more deterministic rating\n            \"max_tokens\": 10,     # Slightly more tokens in case it adds context\n            \"logprobs\": None,    # We don't need logprobs for the rating itself\n            \"echo\": False\n        }\n\n        # Generate self-evaluation response\n        # Use the model's own generate method, assuming it handles locking\n        eval_result = await model.generate(prompt, **params)\n\n        eval_text = eval_result.get(\"text\", \"\").strip()\n\n        # Parse the response\n        return await _parse_self_evaluation_response(eval_text)\n\n    except Exception as e:\n        logger.error(f\"Error during self-evaluation for model {model.get_id()}: {str(e)}\")\n        return 0.5  # Default medium confidence\n</code></pre>"},{"location":"api/models/#ai_ensemble_suite.models.measure_consistency_confidence","title":"<code>measure_consistency_confidence(model, prompt, num_samples=3, temperature=0.7, max_tokens=150)</code>  <code>async</code>","text":"<p>Generate multiple responses and measure consistency.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>GGUFModel</code> <p>The model to generate responses.</p> required <code>prompt</code> <code>str</code> <p>The prompt to send to the model.</p> required <code>num_samples</code> <code>int</code> <p>Number of samples to generate.</p> <code>3</code> <code>temperature</code> <code>float</code> <p>Temperature to use for generation (should allow some variance).</p> <code>0.7</code> <code>max_tokens</code> <code>int</code> <p>Max tokens for sampled responses.</p> <code>150</code> <p>Returns:</p> Type Description <code>float</code> <p>Consistency score between 0.0 and 1.0.</p> Source code in <code>src\\ai_ensemble_suite\\models\\confidence.py</code> <pre><code>async def measure_consistency_confidence(\n    model: 'GGUFModel',\n    prompt: str,\n    num_samples: int = 3,\n    temperature: float = 0.7,\n    max_tokens: int = 150 # Added max_tokens for consistency\n) -&gt; float:\n    \"\"\"Generate multiple responses and measure consistency.\n\n    Args:\n        model: The model to generate responses.\n        prompt: The prompt to send to the model.\n        num_samples: Number of samples to generate.\n        temperature: Temperature to use for generation (should allow some variance).\n        max_tokens: Max tokens for sampled responses.\n\n    Returns:\n        Consistency score between 0.0 and 1.0.\n    \"\"\"\n    if num_samples &lt; 2:\n        logger.warning(\"At least 2 samples are required for consistency measurement\")\n        return 0.5  # Default medium confidence\n\n    responses = []\n\n    # Generate multiple responses\n    generation_params = {\n        \"temperature\": temperature,\n        \"max_tokens\": max_tokens, # Use passed or default max_tokens\n        \"logprobs\": None,        # Don't need logprobs for consistency text\n        \"echo\": False\n    }\n\n    try:\n        # Generate responses concurrently\n        async def generate_response():\n            try:\n                # Use the model's generate method\n                result = await model.generate(prompt, **generation_params)\n                return result.get(\"text\", \"\").strip()\n            except Exception as e:\n                logger.warning(f\"Error in consistency sample generation for model {model.get_id()}: {e}\")\n                return None\n\n        # Execute tasks concurrently\n        tasks = [generate_response() for _ in range(num_samples)]\n        results = await asyncio.gather(*tasks)\n\n        # Filter out None values (failed generations)\n        responses = [r for r in results if r is not None and r] # Also ensure not empty\n\n        if len(responses) &lt; 2:\n            logger.warning(f\"Not enough valid responses ({len(responses)}/{num_samples}) for consistency calculation\")\n            return 0.5 if len(responses) == 0 else 0.7 # Slightly higher if 1 response\n\n        # Calculate similarity scores between all pairs of responses\n        similarities = []\n        for i in range(len(responses)):\n            for j in range(i + 1, len(responses)):\n                similarity = calculate_similarity(responses[i], responses[j])\n                similarities.append(similarity)\n\n        # Average similarity as consistency score\n        consistency_score = statistics.mean(similarities) if similarities else 0.5\n        return consistency_score\n\n    except ValidationError as ve:\n        logger.warning(f\"Validation error during consistency check: {ve}\")\n        return 0.5\n    except Exception as e:\n        logger.error(f\"Error measuring consistency for model {model.get_id()}: {str(e)}\")\n        return 0.5  # Default medium confidence\n</code></pre>"},{"location":"api/utilities/","title":"Utilities API","text":"<p>API reference for utility functions. </p> <p>Utility functions for the ai-ensemble-suite package.</p>"},{"location":"api/utilities/#ai_ensemble_suite.utils.EnsembleLogger","title":"<code>EnsembleLogger</code>","text":"<p>Logging utility for ai-ensemble-suite.</p> <p>Provides structured logging with context information and configurable verbosity.</p> Source code in <code>src\\ai_ensemble_suite\\utils\\logging.py</code> <pre><code>class EnsembleLogger:\n    \"\"\"Logging utility for ai-ensemble-suite.\n\n    Provides structured logging with context information and configurable verbosity.\n    \"\"\"\n\n    def __init__(\n        self,\n        name: str = \"ai_ensemble_suite\",\n        level: Union[int, str] = logging.DEBUG,\n        format_string: Optional[str] = None,\n        handlers: Optional[List[logging.Handler]] = None,\n    ) -&gt; None:\n        \"\"\"Initialize the EnsembleLogger.\n\n        Args:\n            name: The logger name.\n            level: The logging level.\n            format_string: Custom format string for log messages.\n            handlers: Custom log handlers.\n        \"\"\"\n        self.name = name\n        self.logger = logging.getLogger(name)\n\n        # Set default format if not provided\n        if format_string is None:\n            format_string = (\n                \"%(asctime)s [%(levelname)s] %(name)s: \"\n                \"%(message)s (%(filename)s:%(lineno)d)\"\n            )\n\n        # Create formatter\n        formatter = logging.Formatter(format_string)\n\n        # Use provided handlers or create default handler\n        if handlers is None:\n            handler = logging.StreamHandler(sys.stdout)\n            handler.setFormatter(formatter)\n            self.logger.addHandler(handler)\n        else:\n            for handler in handlers:\n                handler.setFormatter(formatter)\n                self.logger.addHandler(handler)\n\n        # Set level\n        self.set_level(level)\n\n    def get_logger(self) -&gt; logging.Logger:\n        \"\"\"Get the underlying logger.\n\n        Returns:\n            The configured logger instance.\n        \"\"\"\n        return self.logger\n\n    def set_level(self, level: Union[int, str]) -&gt; None:\n        \"\"\"Set the logging level.\n\n        Args:\n            level: The new logging level.\n        \"\"\"\n        if isinstance(level, str):\n            level = getattr(logging, level.upper(), logging.INFO)\n        self.logger.setLevel(level)\n        self.logger.setLevel(logging.DEBUG)\n\n    def _sanitize(self, msg: str, sanitize: bool = False) -&gt; str:\n        \"\"\"Sanitize sensitive information from log messages.\n\n        Args:\n            msg: The message to sanitize.\n            sanitize: Whether to sanitize the message.\n\n        Returns:\n            Sanitized message.\n        \"\"\"\n        if not sanitize:\n            return msg\n\n        # Simple sanitization for now - could be expanded with regex patterns\n        # for more sophisticated redaction\n        if len(msg) &gt; 100:\n            return f\"{msg[:50]}...{msg[-50:]}\"\n        return msg\n\n    def _format_extra(self, extra: Dict[str, Any]) -&gt; str:\n        \"\"\"Format extra context information.\n\n        Args:\n            extra: Extra context information.\n\n        Returns:\n            Formatted extra context.\n        \"\"\"\n        if not extra:\n            return \"\"\n        return \" | \" + \" | \".join(f\"{k}={v}\" for k, v in extra.items())\n\n    def debug(self, msg: str, *args: Any, **kwargs: Any) -&gt; None:\n        \"\"\"Log a debug message.\n\n        Args:\n            msg: The message to log.\n            *args: Additional positional arguments.\n            **kwargs: Additional keyword arguments. Special keys:\n                extra: Dict[str, Any] - Extra context information\n                sanitize: bool - Whether to sanitize sensitive information\n        \"\"\"\n        extra = kwargs.pop(\"extra\", {})\n        sanitize = kwargs.pop(\"sanitize\", False)\n\n        formatted_msg = self._sanitize(msg, sanitize)\n        if extra:\n            formatted_msg += self._format_extra(extra)\n\n        self.logger.debug(formatted_msg, *args, **kwargs)\n\n    def info(self, msg: str, *args: Any, **kwargs: Any) -&gt; None:\n        \"\"\"Log an info message.\n\n        Args:\n            msg: The message to log.\n            *args: Additional positional arguments.\n            **kwargs: Additional keyword arguments. Special keys:\n                extra: Dict[str, Any] - Extra context information\n                sanitize: bool - Whether to sanitize sensitive information\n        \"\"\"\n        extra = kwargs.pop(\"extra\", {})\n        sanitize = kwargs.pop(\"sanitize\", False)\n\n        formatted_msg = self._sanitize(msg, sanitize)\n        if extra:\n            formatted_msg += self._format_extra(extra)\n\n        self.logger.info(formatted_msg, *args, **kwargs)\n\n    def warning(self, msg: str, *args: Any, **kwargs: Any) -&gt; None:\n        \"\"\"Log a warning message.\n\n        Args:\n            msg: The message to log.\n            *args: Additional positional arguments.\n            **kwargs: Additional keyword arguments. Special keys:\n                extra: Dict[str, Any] - Extra context information\n                sanitize: bool - Whether to sanitize sensitive information\n        \"\"\"\n        extra = kwargs.pop(\"extra\", {})\n        sanitize = kwargs.pop(\"sanitize\", False)\n\n        formatted_msg = self._sanitize(msg, sanitize)\n        if extra:\n            formatted_msg += self._format_extra(extra)\n\n        self.logger.warning(formatted_msg, *args, **kwargs)\n\n    def error(self, msg: str, *args: Any, **kwargs: Any) -&gt; None:\n        \"\"\"Log an error message.\n\n        Args:\n            msg: The message to log.\n            *args: Additional positional arguments.\n            **kwargs: Additional keyword arguments. Special keys:\n                extra: Dict[str, Any] - Extra context information\n                sanitize: bool - Whether to sanitize sensitive information\n        \"\"\"\n        extra = kwargs.pop(\"extra\", {})\n        sanitize = kwargs.pop(\"sanitize\", False)\n\n        formatted_msg = self._sanitize(msg, sanitize)\n        if extra:\n            formatted_msg += self._format_extra(extra)\n\n        self.logger.error(formatted_msg, *args, **kwargs)\n\n    def critical(self, msg: str, *args: Any, **kwargs: Any) -&gt; None:\n        \"\"\"Log a critical message.\n\n        Args:\n            msg: The message to log.\n            *args: Additional positional arguments.\n            **kwargs: Additional keyword arguments. Special keys:\n                extra: Dict[str, Any] - Extra context information\n                sanitize: bool - Whether to sanitize sensitive information\n        \"\"\"\n        extra = kwargs.pop(\"extra\", {})\n        sanitize = kwargs.pop(\"sanitize\", False)\n\n        formatted_msg = self._sanitize(msg, sanitize)\n        if extra:\n            formatted_msg += self._format_extra(extra)\n\n        self.logger.critical(formatted_msg, *args, **kwargs)\n</code></pre>"},{"location":"api/utilities/#ai_ensemble_suite.utils.EnsembleLogger.__init__","title":"<code>__init__(name='ai_ensemble_suite', level=logging.DEBUG, format_string=None, handlers=None)</code>","text":"<p>Initialize the EnsembleLogger.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The logger name.</p> <code>'ai_ensemble_suite'</code> <code>level</code> <code>Union[int, str]</code> <p>The logging level.</p> <code>DEBUG</code> <code>format_string</code> <code>Optional[str]</code> <p>Custom format string for log messages.</p> <code>None</code> <code>handlers</code> <code>Optional[List[Handler]]</code> <p>Custom log handlers.</p> <code>None</code> Source code in <code>src\\ai_ensemble_suite\\utils\\logging.py</code> <pre><code>def __init__(\n    self,\n    name: str = \"ai_ensemble_suite\",\n    level: Union[int, str] = logging.DEBUG,\n    format_string: Optional[str] = None,\n    handlers: Optional[List[logging.Handler]] = None,\n) -&gt; None:\n    \"\"\"Initialize the EnsembleLogger.\n\n    Args:\n        name: The logger name.\n        level: The logging level.\n        format_string: Custom format string for log messages.\n        handlers: Custom log handlers.\n    \"\"\"\n    self.name = name\n    self.logger = logging.getLogger(name)\n\n    # Set default format if not provided\n    if format_string is None:\n        format_string = (\n            \"%(asctime)s [%(levelname)s] %(name)s: \"\n            \"%(message)s (%(filename)s:%(lineno)d)\"\n        )\n\n    # Create formatter\n    formatter = logging.Formatter(format_string)\n\n    # Use provided handlers or create default handler\n    if handlers is None:\n        handler = logging.StreamHandler(sys.stdout)\n        handler.setFormatter(formatter)\n        self.logger.addHandler(handler)\n    else:\n        for handler in handlers:\n            handler.setFormatter(formatter)\n            self.logger.addHandler(handler)\n\n    # Set level\n    self.set_level(level)\n</code></pre>"},{"location":"api/utilities/#ai_ensemble_suite.utils.EnsembleLogger.critical","title":"<code>critical(msg, *args, **kwargs)</code>","text":"<p>Log a critical message.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str</code> <p>The message to log.</p> required <code>*args</code> <code>Any</code> <p>Additional positional arguments.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments. Special keys: extra: Dict[str, Any] - Extra context information sanitize: bool - Whether to sanitize sensitive information</p> <code>{}</code> Source code in <code>src\\ai_ensemble_suite\\utils\\logging.py</code> <pre><code>def critical(self, msg: str, *args: Any, **kwargs: Any) -&gt; None:\n    \"\"\"Log a critical message.\n\n    Args:\n        msg: The message to log.\n        *args: Additional positional arguments.\n        **kwargs: Additional keyword arguments. Special keys:\n            extra: Dict[str, Any] - Extra context information\n            sanitize: bool - Whether to sanitize sensitive information\n    \"\"\"\n    extra = kwargs.pop(\"extra\", {})\n    sanitize = kwargs.pop(\"sanitize\", False)\n\n    formatted_msg = self._sanitize(msg, sanitize)\n    if extra:\n        formatted_msg += self._format_extra(extra)\n\n    self.logger.critical(formatted_msg, *args, **kwargs)\n</code></pre>"},{"location":"api/utilities/#ai_ensemble_suite.utils.EnsembleLogger.debug","title":"<code>debug(msg, *args, **kwargs)</code>","text":"<p>Log a debug message.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str</code> <p>The message to log.</p> required <code>*args</code> <code>Any</code> <p>Additional positional arguments.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments. Special keys: extra: Dict[str, Any] - Extra context information sanitize: bool - Whether to sanitize sensitive information</p> <code>{}</code> Source code in <code>src\\ai_ensemble_suite\\utils\\logging.py</code> <pre><code>def debug(self, msg: str, *args: Any, **kwargs: Any) -&gt; None:\n    \"\"\"Log a debug message.\n\n    Args:\n        msg: The message to log.\n        *args: Additional positional arguments.\n        **kwargs: Additional keyword arguments. Special keys:\n            extra: Dict[str, Any] - Extra context information\n            sanitize: bool - Whether to sanitize sensitive information\n    \"\"\"\n    extra = kwargs.pop(\"extra\", {})\n    sanitize = kwargs.pop(\"sanitize\", False)\n\n    formatted_msg = self._sanitize(msg, sanitize)\n    if extra:\n        formatted_msg += self._format_extra(extra)\n\n    self.logger.debug(formatted_msg, *args, **kwargs)\n</code></pre>"},{"location":"api/utilities/#ai_ensemble_suite.utils.EnsembleLogger.error","title":"<code>error(msg, *args, **kwargs)</code>","text":"<p>Log an error message.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str</code> <p>The message to log.</p> required <code>*args</code> <code>Any</code> <p>Additional positional arguments.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments. Special keys: extra: Dict[str, Any] - Extra context information sanitize: bool - Whether to sanitize sensitive information</p> <code>{}</code> Source code in <code>src\\ai_ensemble_suite\\utils\\logging.py</code> <pre><code>def error(self, msg: str, *args: Any, **kwargs: Any) -&gt; None:\n    \"\"\"Log an error message.\n\n    Args:\n        msg: The message to log.\n        *args: Additional positional arguments.\n        **kwargs: Additional keyword arguments. Special keys:\n            extra: Dict[str, Any] - Extra context information\n            sanitize: bool - Whether to sanitize sensitive information\n    \"\"\"\n    extra = kwargs.pop(\"extra\", {})\n    sanitize = kwargs.pop(\"sanitize\", False)\n\n    formatted_msg = self._sanitize(msg, sanitize)\n    if extra:\n        formatted_msg += self._format_extra(extra)\n\n    self.logger.error(formatted_msg, *args, **kwargs)\n</code></pre>"},{"location":"api/utilities/#ai_ensemble_suite.utils.EnsembleLogger.get_logger","title":"<code>get_logger()</code>","text":"<p>Get the underlying logger.</p> <p>Returns:</p> Type Description <code>Logger</code> <p>The configured logger instance.</p> Source code in <code>src\\ai_ensemble_suite\\utils\\logging.py</code> <pre><code>def get_logger(self) -&gt; logging.Logger:\n    \"\"\"Get the underlying logger.\n\n    Returns:\n        The configured logger instance.\n    \"\"\"\n    return self.logger\n</code></pre>"},{"location":"api/utilities/#ai_ensemble_suite.utils.EnsembleLogger.info","title":"<code>info(msg, *args, **kwargs)</code>","text":"<p>Log an info message.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str</code> <p>The message to log.</p> required <code>*args</code> <code>Any</code> <p>Additional positional arguments.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments. Special keys: extra: Dict[str, Any] - Extra context information sanitize: bool - Whether to sanitize sensitive information</p> <code>{}</code> Source code in <code>src\\ai_ensemble_suite\\utils\\logging.py</code> <pre><code>def info(self, msg: str, *args: Any, **kwargs: Any) -&gt; None:\n    \"\"\"Log an info message.\n\n    Args:\n        msg: The message to log.\n        *args: Additional positional arguments.\n        **kwargs: Additional keyword arguments. Special keys:\n            extra: Dict[str, Any] - Extra context information\n            sanitize: bool - Whether to sanitize sensitive information\n    \"\"\"\n    extra = kwargs.pop(\"extra\", {})\n    sanitize = kwargs.pop(\"sanitize\", False)\n\n    formatted_msg = self._sanitize(msg, sanitize)\n    if extra:\n        formatted_msg += self._format_extra(extra)\n\n    self.logger.info(formatted_msg, *args, **kwargs)\n</code></pre>"},{"location":"api/utilities/#ai_ensemble_suite.utils.EnsembleLogger.set_level","title":"<code>set_level(level)</code>","text":"<p>Set the logging level.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>Union[int, str]</code> <p>The new logging level.</p> required Source code in <code>src\\ai_ensemble_suite\\utils\\logging.py</code> <pre><code>def set_level(self, level: Union[int, str]) -&gt; None:\n    \"\"\"Set the logging level.\n\n    Args:\n        level: The new logging level.\n    \"\"\"\n    if isinstance(level, str):\n        level = getattr(logging, level.upper(), logging.INFO)\n    self.logger.setLevel(level)\n    self.logger.setLevel(logging.DEBUG)\n</code></pre>"},{"location":"api/utilities/#ai_ensemble_suite.utils.EnsembleLogger.warning","title":"<code>warning(msg, *args, **kwargs)</code>","text":"<p>Log a warning message.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str</code> <p>The message to log.</p> required <code>*args</code> <code>Any</code> <p>Additional positional arguments.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments. Special keys: extra: Dict[str, Any] - Extra context information sanitize: bool - Whether to sanitize sensitive information</p> <code>{}</code> Source code in <code>src\\ai_ensemble_suite\\utils\\logging.py</code> <pre><code>def warning(self, msg: str, *args: Any, **kwargs: Any) -&gt; None:\n    \"\"\"Log a warning message.\n\n    Args:\n        msg: The message to log.\n        *args: Additional positional arguments.\n        **kwargs: Additional keyword arguments. Special keys:\n            extra: Dict[str, Any] - Extra context information\n            sanitize: bool - Whether to sanitize sensitive information\n    \"\"\"\n    extra = kwargs.pop(\"extra\", {})\n    sanitize = kwargs.pop(\"sanitize\", False)\n\n    formatted_msg = self._sanitize(msg, sanitize)\n    if extra:\n        formatted_msg += self._format_extra(extra)\n\n    self.logger.warning(formatted_msg, *args, **kwargs)\n</code></pre>"},{"location":"api/utilities/#ai_ensemble_suite.utils.TraceCollector","title":"<code>TraceCollector</code>","text":"<p>Collects and organizes trace information during execution.</p> Source code in <code>src\\ai_ensemble_suite\\utils\\tracing.py</code> <pre><code>class TraceCollector:\n    \"\"\"Collects and organizes trace information during execution.\"\"\"\n\n    def __init__(self, enabled: bool = True, logger_instance=None) -&gt; None: # Add logger_instance parameter\n        \"\"\"Initialize the TraceCollector.\"\"\"\n        self.enabled = enabled\n        if logger_instance:\n            self.logger = logger_instance\n        else:\n            # Fallback to standard logger if none provided\n            self.logger = logging.getLogger(__name__)\n            # Optionally add a handler if no root config exists\n            if not logging.getLogger().hasHandlers():\n                 handler = logging.StreamHandler()\n                 formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n                 handler.setFormatter(formatter)\n                 self.logger.addHandler(handler)\n                 self.logger.setLevel(logging.DEBUG) # Set a default level\n                 self.logger.warning(\n                    \"TraceCollector: No logger instance provided and root logger had no handlers. \"\n                    \"Added default StreamHandler for '%s'.\", __name__\n                 )\n            else:\n                 self.logger.warning(\n                    \"TraceCollector initialized without a specific logger instance. \"\n                    \"Using standard logger '%s'. Ensure it's configured correctly.\", __name__\n                 )\n\n\n        self.model_traces: Dict[str, List[ModelTrace]] = {}\n        self.phase_traces: Dict[str, PhaseTrace] = {}\n        self.aggregation_trace: Optional[AggregationTrace] = None\n        self.session_trace: Optional[SessionTrace] = None\n        self.start_time: Optional[float] = None\n        self.end_time: Optional[float] = None\n        self.session_id: str = f\"session_{int(time.time())}_{os.getpid()}\" # Unique session ID\n\n    def start_session(self) -&gt; None:\n        \"\"\"Start a new tracing session, clearing previous data.\"\"\"\n        if not self.enabled: return\n        self.clear() # Clear previous state\n        self.session_id = f\"session_{int(time.time())}_{os.getpid()}\" # Generate new ID\n        self.start_time = time.time()\n        self.logger.debug(f\"Trace session started. ID: {self.session_id}\")\n\n    def end_session(self, status: str = \"success\", final_error: Optional[str] = None) -&gt; None:\n        \"\"\"End the current tracing session.\"\"\"\n        if not self.enabled or self.start_time is None: return\n        if self.end_time is None: # Ensure end_time is only set once\n             self.end_time = time.time()\n             duration = self.end_time - self.start_time\n             # Update session trace status if it exists\n             if self.session_trace:\n                  self.session_trace.status = status\n                  self.session_trace.final_error = final_error\n             self.logger.debug(f\"Trace session ended. ID: {self.session_id}. Duration: {duration:.3f}s. Status: {status}.\")\n\n    # --- Modified add_* methods to include status/error ---\n    def add_model_trace(self, model_id: str, input_prompt: str, output: Dict[str, Any], execution_time: float, parameters: Dict[str, Any], status: str = \"success\", error_message: Optional[str] = None) -&gt; None:\n        \"\"\"Add a trace record for a model inference call.\"\"\"\n        if not self.enabled: return\n        trace = ModelTrace(model_id=model_id, input_prompt=input_prompt, output=output, execution_time=execution_time, parameters=parameters, status=status, error_message=error_message)\n        if model_id not in self.model_traces: self.model_traces[model_id] = []\n        self.model_traces[model_id].append(trace);\n        log_message = f\"Trace added: Model '{model_id}' (Status: {status})\"\n        if error_message: log_message += f\" Error: {error_message}\"\n        if status == \"failure\":\n            self.logger.error(log_message)\n        else:\n            self.logger.debug(log_message)\n\n    def add_phase_trace(self, phase_name: str, input_data: Dict[str, Any], output_data: Dict[str, Any], execution_time: float, phase_parameters: Dict[str, Any], phase_type: Optional[str] = None, status: str = \"success\", error_message: Optional[str] = None) -&gt; None:\n        \"\"\"Add a trace record for a collaboration phase execution.\"\"\"\n        if not self.enabled: return\n        trace = PhaseTrace(phase_name=phase_name, phase_type=phase_type, input_data=input_data, output_data=output_data, execution_time=execution_time, phase_parameters=phase_parameters, status=status, error_message=error_message)\n        self.phase_traces[phase_name] = trace;\n        log_message = f\"Trace added: Phase '{phase_name}' (Status: {status})\"\n        if error_message: log_message += f\" Error: {error_message}\"\n        if status == \"failure\":\n            self.logger.error(log_message)\n        else:\n            self.logger.debug(log_message)\n\n    def add_aggregation_trace(self, strategy_name: str, inputs: Dict[str, Any], output: Dict[str, Any], execution_time: float, parameters: Dict[str, Any], status: str = \"success\", error_message: Optional[str] = None) -&gt; None:\n        \"\"\"Add a trace record for the aggregation step.\"\"\"\n        if not self.enabled: return\n        trace = AggregationTrace(strategy_name=strategy_name, inputs=inputs, output=output, execution_time=execution_time, parameters=parameters, status=status, error_message=error_message)\n        self.aggregation_trace = trace;\n        log_message = f\"Trace added: Aggregation '{strategy_name}' (Status: {status})\"\n        if error_message: log_message += f\" Error: {error_message}\"\n        if status == \"failure\":\n            self.logger.error(log_message)\n        else:\n            self.logger.debug(log_message)\n\n    def add_session_trace(self, query: str, final_response: str, total_execution_time: float, configuration: Dict[str, Any]) -&gt; None:\n        \"\"\"Add the overall session summary trace record. Overwrites previous if called again.\"\"\"\n        if not self.enabled: return\n        # Status/error updated via end_session\n        trace = SessionTrace(query=query, final_response=final_response, total_execution_time=total_execution_time, configuration=configuration)\n        self.session_trace = trace;\n        self.logger.debug(\"Session trace data updated.\")\n\n    def get_trace_data(self) -&gt; Dict[str, Any]:\n        \"\"\"Retrieve the collected trace data as a dictionary.\"\"\"\n        if not self.enabled: return {\"error\": \"Tracing disabled\"}\n        # Ensure session ends if not explicitly called, using current status\n        if self.start_time is not None and self.end_time is None:\n            self.logger.warning(\"get_trace_data called before end_session. Ending session implicitly.\")\n            self.end_session() # End with default status\n\n        result: Dict[str, Any] = {\n            \"session_id\": self.session_id,\n            \"session\": self.session_trace.to_dict() if self.session_trace else None,\n            \"phases\": {name: trace.to_dict() for name, trace in self.phase_traces.items()},\n            \"aggregation\": self.aggregation_trace.to_dict() if self.aggregation_trace else None,\n            \"models\": {model_id: [trace.to_dict() for trace in traces] for model_id, traces in self.model_traces.items()},\n            \"statistics\": self.calculate_statistics(),\n            \"trace_metadata\": {\n                \"start_time_unix\": self.start_time,\n                \"end_time_unix\": self.end_time,\n                \"collection_duration_ms\": round((self.end_time - self.start_time) * 1000) if self.start_time and self.end_time else None,\n            }\n        }\n        return result\n\n    def calculate_statistics(self) -&gt; Dict[str, Any]:\n        \"\"\"Calculate summary statistics from the collected trace data.\"\"\"\n        stats: Dict[str, Any] = {\n            \"model_count_configured\": 0, # Need config access for this realistically\n            \"phase_count_executed\": len(self.phase_traces),\n            \"total_model_calls\": sum(len(traces) for traces in self.model_traces.values()),\n            \"successful_model_calls\": sum(1 for traces in self.model_traces.values() for trace in traces if trace.status == \"success\"),\n            \"failed_model_calls\": sum(1 for traces in self.model_traces.values() for trace in traces if trace.status != \"success\"),\n            \"total_model_execution_time_ms\": 0,\n            \"average_model_execution_time_ms\": 0,\n            \"total_phase_execution_time_ms\": 0,\n            \"average_phase_execution_time_ms\": 0,\n            \"aggregation_execution_time_ms\": 0,\n            \"model_execution_details\": {},\n            \"phase_execution_times_ms\": {}\n        }\n\n        all_model_times_ms = []\n        for model_id, traces in self.model_traces.items():\n            times_ms = [trace.execution_time * 1000 for trace in traces if trace.execution_time is not None and trace.status == \"success\"]\n            total_ms = sum(times_ms) if times_ms else 0\n            if times_ms:\n                stats[\"model_execution_details\"][model_id] = {\n                    \"total_ms\": round(total_ms),\n                    \"average_ms\": round(total_ms / len(times_ms)),\n                    \"min_ms\": round(min(times_ms)),\n                    \"max_ms\": round(max(times_ms)),\n                    \"success_calls\": len(times_ms),\n                    \"total_calls\": len(traces) # Include failures in total call count for the model\n                }\n                all_model_times_ms.extend(times_ms)\n            elif traces: # If traces exist but none were successful/timed\n                 stats[\"model_execution_details\"][model_id] = {\"total_ms\": 0, \"average_ms\": 0, \"success_calls\": 0, \"total_calls\": len(traces)}\n\n        if all_model_times_ms:\n            stats[\"total_model_execution_time_ms\"] = round(sum(all_model_times_ms))\n            stats[\"average_model_execution_time_ms\"] = round(sum(all_model_times_ms) / len(all_model_times_ms))\n\n        all_phase_times_ms = []\n        for phase_name, trace in self.phase_traces.items():\n            if trace.execution_time is not None and trace.status == \"success\":\n                time_ms = trace.execution_time * 1000\n                stats[\"phase_execution_times_ms\"][phase_name] = round(time_ms)\n                all_phase_times_ms.append(time_ms)\n            elif trace.status != \"success\": # Record failed/skipped phases as 0 time for stats\n                 stats[\"phase_execution_times_ms\"][phase_name] = 0\n\n        if all_phase_times_ms:\n            stats[\"total_phase_execution_time_ms\"] = round(sum(all_phase_times_ms))\n            stats[\"average_phase_execution_time_ms\"] = round(sum(all_phase_times_ms) / len(all_phase_times_ms))\n\n        if self.aggregation_trace and self.aggregation_trace.execution_time is not None and self.aggregation_trace.status == \"success\":\n            stats[\"aggregation_execution_time_ms\"] = round(self.aggregation_trace.execution_time * 1000)\n\n        # Calculate perceived total time (sum of phase + aggregation times) vs actual session time\n        perceived_total_ms = stats[\"total_phase_execution_time_ms\"] + stats[\"aggregation_execution_time_ms\"]\n        stats[\"_perceived_total_execution_time_ms\"] = perceived_total_ms\n\n        # Add overall session time from session trace if available\n        if self.session_trace and self.session_trace.total_execution_time is not None:\n             stats[\"session_total_execution_time_ms\"] = round(self.session_trace.total_execution_time * 1000)\n        elif self.start_time and self.end_time: # Use collector's measured time if session trace missing total\n             stats[\"session_total_execution_time_ms\"] = round((self.end_time - self.start_time) * 1000)\n\n        return stats\n\n    def clear(self) -&gt; None:\n        \"\"\"Clear all collected trace data and reset timers.\"\"\"\n        self.model_traces = {}\n        self.phase_traces = {}\n        self.aggregation_trace = None\n        self.session_trace = None\n        self.start_time = None\n        self.end_time = None\n        # Keep session_id or generate new? Let's keep existing if any, start_session generates new one.\n        self.logger.debug(\"Trace data cleared.\")\n\n    def save_to_file(self, filepath: Union[str, Path]) -&gt; bool:\n        \"\"\"Save the collected trace data to a JSON file.\n\n        Args:\n            filepath: The path to the file where the trace data should be saved.\n\n        Returns:\n            True if saving was successful, False otherwise.\n        \"\"\"\n        if not self.enabled:\n            self.logger.info(\"Tracing is disabled, cannot save trace.\")\n            return False\n\n        filepath = Path(filepath)\n        try:\n            filepath.parent.mkdir(parents=True, exist_ok=True)\n            trace_data_dict = self.get_trace_data()\n\n            with open(filepath, 'w', encoding='utf-8') as f:\n                json.dump(trace_data_dict, f, indent=2, cls=NumpyEncoder) # Use the encoder\n\n            self.logger.info(f\"Trace data saved successfully to {filepath}\")\n            return True\n        except TypeError as e: # This will be caught after the debug log in the encoder\n            self.logger.error(f\"Failed to serialize trace data to JSON for file '{filepath}': {e}\", exc_info=True)\n            return False\n        except Exception as e:\n            self.logger.error(f\"Error saving trace data to file '{filepath}': {str(e)}\", exc_info=True)\n            return False\n\n    def to_json(self) -&gt; str:\n        \"\"\"Convert the collected trace data to a JSON string.\n\n        Returns:\n            A JSON string representation of the trace data, or a JSON error object.\n        \"\"\"\n        if not self.enabled: return json.dumps({\"error\": \"Tracing disabled\"})\n        trace_data_dict = self.get_trace_data()\n        try:\n            return json.dumps(trace_data_dict, indent=2, cls=NumpyEncoder) # Use the encoder\n        except TypeError as e: # This will be caught after the debug log in the encoder\n            self.logger.error(f\"Error converting trace data to JSON string: {str(e)}\")\n            return json.dumps({\"error\": f\"Failed to serialize trace data: {str(e)}\"}, indent=2)\n        except Exception as e:\n            self.logger.error(f\"Unexpected error converting trace data to JSON string: {str(e)}\", exc_info=True)\n            return json.dumps({\"error\": f\"Unexpected error serializing trace data: {str(e)}\"}, indent=2)\n</code></pre>"},{"location":"api/utilities/#ai_ensemble_suite.utils.TraceCollector.__init__","title":"<code>__init__(enabled=True, logger_instance=None)</code>","text":"<p>Initialize the TraceCollector.</p> Source code in <code>src\\ai_ensemble_suite\\utils\\tracing.py</code> <pre><code>def __init__(self, enabled: bool = True, logger_instance=None) -&gt; None: # Add logger_instance parameter\n    \"\"\"Initialize the TraceCollector.\"\"\"\n    self.enabled = enabled\n    if logger_instance:\n        self.logger = logger_instance\n    else:\n        # Fallback to standard logger if none provided\n        self.logger = logging.getLogger(__name__)\n        # Optionally add a handler if no root config exists\n        if not logging.getLogger().hasHandlers():\n             handler = logging.StreamHandler()\n             formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n             handler.setFormatter(formatter)\n             self.logger.addHandler(handler)\n             self.logger.setLevel(logging.DEBUG) # Set a default level\n             self.logger.warning(\n                \"TraceCollector: No logger instance provided and root logger had no handlers. \"\n                \"Added default StreamHandler for '%s'.\", __name__\n             )\n        else:\n             self.logger.warning(\n                \"TraceCollector initialized without a specific logger instance. \"\n                \"Using standard logger '%s'. Ensure it's configured correctly.\", __name__\n             )\n\n\n    self.model_traces: Dict[str, List[ModelTrace]] = {}\n    self.phase_traces: Dict[str, PhaseTrace] = {}\n    self.aggregation_trace: Optional[AggregationTrace] = None\n    self.session_trace: Optional[SessionTrace] = None\n    self.start_time: Optional[float] = None\n    self.end_time: Optional[float] = None\n    self.session_id: str = f\"session_{int(time.time())}_{os.getpid()}\" # Unique session ID\n</code></pre>"},{"location":"api/utilities/#ai_ensemble_suite.utils.TraceCollector.add_aggregation_trace","title":"<code>add_aggregation_trace(strategy_name, inputs, output, execution_time, parameters, status='success', error_message=None)</code>","text":"<p>Add a trace record for the aggregation step.</p> Source code in <code>src\\ai_ensemble_suite\\utils\\tracing.py</code> <pre><code>def add_aggregation_trace(self, strategy_name: str, inputs: Dict[str, Any], output: Dict[str, Any], execution_time: float, parameters: Dict[str, Any], status: str = \"success\", error_message: Optional[str] = None) -&gt; None:\n    \"\"\"Add a trace record for the aggregation step.\"\"\"\n    if not self.enabled: return\n    trace = AggregationTrace(strategy_name=strategy_name, inputs=inputs, output=output, execution_time=execution_time, parameters=parameters, status=status, error_message=error_message)\n    self.aggregation_trace = trace;\n    log_message = f\"Trace added: Aggregation '{strategy_name}' (Status: {status})\"\n    if error_message: log_message += f\" Error: {error_message}\"\n    if status == \"failure\":\n        self.logger.error(log_message)\n    else:\n        self.logger.debug(log_message)\n</code></pre>"},{"location":"api/utilities/#ai_ensemble_suite.utils.TraceCollector.add_model_trace","title":"<code>add_model_trace(model_id, input_prompt, output, execution_time, parameters, status='success', error_message=None)</code>","text":"<p>Add a trace record for a model inference call.</p> Source code in <code>src\\ai_ensemble_suite\\utils\\tracing.py</code> <pre><code>def add_model_trace(self, model_id: str, input_prompt: str, output: Dict[str, Any], execution_time: float, parameters: Dict[str, Any], status: str = \"success\", error_message: Optional[str] = None) -&gt; None:\n    \"\"\"Add a trace record for a model inference call.\"\"\"\n    if not self.enabled: return\n    trace = ModelTrace(model_id=model_id, input_prompt=input_prompt, output=output, execution_time=execution_time, parameters=parameters, status=status, error_message=error_message)\n    if model_id not in self.model_traces: self.model_traces[model_id] = []\n    self.model_traces[model_id].append(trace);\n    log_message = f\"Trace added: Model '{model_id}' (Status: {status})\"\n    if error_message: log_message += f\" Error: {error_message}\"\n    if status == \"failure\":\n        self.logger.error(log_message)\n    else:\n        self.logger.debug(log_message)\n</code></pre>"},{"location":"api/utilities/#ai_ensemble_suite.utils.TraceCollector.add_phase_trace","title":"<code>add_phase_trace(phase_name, input_data, output_data, execution_time, phase_parameters, phase_type=None, status='success', error_message=None)</code>","text":"<p>Add a trace record for a collaboration phase execution.</p> Source code in <code>src\\ai_ensemble_suite\\utils\\tracing.py</code> <pre><code>def add_phase_trace(self, phase_name: str, input_data: Dict[str, Any], output_data: Dict[str, Any], execution_time: float, phase_parameters: Dict[str, Any], phase_type: Optional[str] = None, status: str = \"success\", error_message: Optional[str] = None) -&gt; None:\n    \"\"\"Add a trace record for a collaboration phase execution.\"\"\"\n    if not self.enabled: return\n    trace = PhaseTrace(phase_name=phase_name, phase_type=phase_type, input_data=input_data, output_data=output_data, execution_time=execution_time, phase_parameters=phase_parameters, status=status, error_message=error_message)\n    self.phase_traces[phase_name] = trace;\n    log_message = f\"Trace added: Phase '{phase_name}' (Status: {status})\"\n    if error_message: log_message += f\" Error: {error_message}\"\n    if status == \"failure\":\n        self.logger.error(log_message)\n    else:\n        self.logger.debug(log_message)\n</code></pre>"},{"location":"api/utilities/#ai_ensemble_suite.utils.TraceCollector.add_session_trace","title":"<code>add_session_trace(query, final_response, total_execution_time, configuration)</code>","text":"<p>Add the overall session summary trace record. Overwrites previous if called again.</p> Source code in <code>src\\ai_ensemble_suite\\utils\\tracing.py</code> <pre><code>def add_session_trace(self, query: str, final_response: str, total_execution_time: float, configuration: Dict[str, Any]) -&gt; None:\n    \"\"\"Add the overall session summary trace record. Overwrites previous if called again.\"\"\"\n    if not self.enabled: return\n    # Status/error updated via end_session\n    trace = SessionTrace(query=query, final_response=final_response, total_execution_time=total_execution_time, configuration=configuration)\n    self.session_trace = trace;\n    self.logger.debug(\"Session trace data updated.\")\n</code></pre>"},{"location":"api/utilities/#ai_ensemble_suite.utils.TraceCollector.calculate_statistics","title":"<code>calculate_statistics()</code>","text":"<p>Calculate summary statistics from the collected trace data.</p> Source code in <code>src\\ai_ensemble_suite\\utils\\tracing.py</code> <pre><code>def calculate_statistics(self) -&gt; Dict[str, Any]:\n    \"\"\"Calculate summary statistics from the collected trace data.\"\"\"\n    stats: Dict[str, Any] = {\n        \"model_count_configured\": 0, # Need config access for this realistically\n        \"phase_count_executed\": len(self.phase_traces),\n        \"total_model_calls\": sum(len(traces) for traces in self.model_traces.values()),\n        \"successful_model_calls\": sum(1 for traces in self.model_traces.values() for trace in traces if trace.status == \"success\"),\n        \"failed_model_calls\": sum(1 for traces in self.model_traces.values() for trace in traces if trace.status != \"success\"),\n        \"total_model_execution_time_ms\": 0,\n        \"average_model_execution_time_ms\": 0,\n        \"total_phase_execution_time_ms\": 0,\n        \"average_phase_execution_time_ms\": 0,\n        \"aggregation_execution_time_ms\": 0,\n        \"model_execution_details\": {},\n        \"phase_execution_times_ms\": {}\n    }\n\n    all_model_times_ms = []\n    for model_id, traces in self.model_traces.items():\n        times_ms = [trace.execution_time * 1000 for trace in traces if trace.execution_time is not None and trace.status == \"success\"]\n        total_ms = sum(times_ms) if times_ms else 0\n        if times_ms:\n            stats[\"model_execution_details\"][model_id] = {\n                \"total_ms\": round(total_ms),\n                \"average_ms\": round(total_ms / len(times_ms)),\n                \"min_ms\": round(min(times_ms)),\n                \"max_ms\": round(max(times_ms)),\n                \"success_calls\": len(times_ms),\n                \"total_calls\": len(traces) # Include failures in total call count for the model\n            }\n            all_model_times_ms.extend(times_ms)\n        elif traces: # If traces exist but none were successful/timed\n             stats[\"model_execution_details\"][model_id] = {\"total_ms\": 0, \"average_ms\": 0, \"success_calls\": 0, \"total_calls\": len(traces)}\n\n    if all_model_times_ms:\n        stats[\"total_model_execution_time_ms\"] = round(sum(all_model_times_ms))\n        stats[\"average_model_execution_time_ms\"] = round(sum(all_model_times_ms) / len(all_model_times_ms))\n\n    all_phase_times_ms = []\n    for phase_name, trace in self.phase_traces.items():\n        if trace.execution_time is not None and trace.status == \"success\":\n            time_ms = trace.execution_time * 1000\n            stats[\"phase_execution_times_ms\"][phase_name] = round(time_ms)\n            all_phase_times_ms.append(time_ms)\n        elif trace.status != \"success\": # Record failed/skipped phases as 0 time for stats\n             stats[\"phase_execution_times_ms\"][phase_name] = 0\n\n    if all_phase_times_ms:\n        stats[\"total_phase_execution_time_ms\"] = round(sum(all_phase_times_ms))\n        stats[\"average_phase_execution_time_ms\"] = round(sum(all_phase_times_ms) / len(all_phase_times_ms))\n\n    if self.aggregation_trace and self.aggregation_trace.execution_time is not None and self.aggregation_trace.status == \"success\":\n        stats[\"aggregation_execution_time_ms\"] = round(self.aggregation_trace.execution_time * 1000)\n\n    # Calculate perceived total time (sum of phase + aggregation times) vs actual session time\n    perceived_total_ms = stats[\"total_phase_execution_time_ms\"] + stats[\"aggregation_execution_time_ms\"]\n    stats[\"_perceived_total_execution_time_ms\"] = perceived_total_ms\n\n    # Add overall session time from session trace if available\n    if self.session_trace and self.session_trace.total_execution_time is not None:\n         stats[\"session_total_execution_time_ms\"] = round(self.session_trace.total_execution_time * 1000)\n    elif self.start_time and self.end_time: # Use collector's measured time if session trace missing total\n         stats[\"session_total_execution_time_ms\"] = round((self.end_time - self.start_time) * 1000)\n\n    return stats\n</code></pre>"},{"location":"api/utilities/#ai_ensemble_suite.utils.TraceCollector.clear","title":"<code>clear()</code>","text":"<p>Clear all collected trace data and reset timers.</p> Source code in <code>src\\ai_ensemble_suite\\utils\\tracing.py</code> <pre><code>def clear(self) -&gt; None:\n    \"\"\"Clear all collected trace data and reset timers.\"\"\"\n    self.model_traces = {}\n    self.phase_traces = {}\n    self.aggregation_trace = None\n    self.session_trace = None\n    self.start_time = None\n    self.end_time = None\n    # Keep session_id or generate new? Let's keep existing if any, start_session generates new one.\n    self.logger.debug(\"Trace data cleared.\")\n</code></pre>"},{"location":"api/utilities/#ai_ensemble_suite.utils.TraceCollector.end_session","title":"<code>end_session(status='success', final_error=None)</code>","text":"<p>End the current tracing session.</p> Source code in <code>src\\ai_ensemble_suite\\utils\\tracing.py</code> <pre><code>def end_session(self, status: str = \"success\", final_error: Optional[str] = None) -&gt; None:\n    \"\"\"End the current tracing session.\"\"\"\n    if not self.enabled or self.start_time is None: return\n    if self.end_time is None: # Ensure end_time is only set once\n         self.end_time = time.time()\n         duration = self.end_time - self.start_time\n         # Update session trace status if it exists\n         if self.session_trace:\n              self.session_trace.status = status\n              self.session_trace.final_error = final_error\n         self.logger.debug(f\"Trace session ended. ID: {self.session_id}. Duration: {duration:.3f}s. Status: {status}.\")\n</code></pre>"},{"location":"api/utilities/#ai_ensemble_suite.utils.TraceCollector.get_trace_data","title":"<code>get_trace_data()</code>","text":"<p>Retrieve the collected trace data as a dictionary.</p> Source code in <code>src\\ai_ensemble_suite\\utils\\tracing.py</code> <pre><code>def get_trace_data(self) -&gt; Dict[str, Any]:\n    \"\"\"Retrieve the collected trace data as a dictionary.\"\"\"\n    if not self.enabled: return {\"error\": \"Tracing disabled\"}\n    # Ensure session ends if not explicitly called, using current status\n    if self.start_time is not None and self.end_time is None:\n        self.logger.warning(\"get_trace_data called before end_session. Ending session implicitly.\")\n        self.end_session() # End with default status\n\n    result: Dict[str, Any] = {\n        \"session_id\": self.session_id,\n        \"session\": self.session_trace.to_dict() if self.session_trace else None,\n        \"phases\": {name: trace.to_dict() for name, trace in self.phase_traces.items()},\n        \"aggregation\": self.aggregation_trace.to_dict() if self.aggregation_trace else None,\n        \"models\": {model_id: [trace.to_dict() for trace in traces] for model_id, traces in self.model_traces.items()},\n        \"statistics\": self.calculate_statistics(),\n        \"trace_metadata\": {\n            \"start_time_unix\": self.start_time,\n            \"end_time_unix\": self.end_time,\n            \"collection_duration_ms\": round((self.end_time - self.start_time) * 1000) if self.start_time and self.end_time else None,\n        }\n    }\n    return result\n</code></pre>"},{"location":"api/utilities/#ai_ensemble_suite.utils.TraceCollector.save_to_file","title":"<code>save_to_file(filepath)</code>","text":"<p>Save the collected trace data to a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>Union[str, Path]</code> <p>The path to the file where the trace data should be saved.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if saving was successful, False otherwise.</p> Source code in <code>src\\ai_ensemble_suite\\utils\\tracing.py</code> <pre><code>def save_to_file(self, filepath: Union[str, Path]) -&gt; bool:\n    \"\"\"Save the collected trace data to a JSON file.\n\n    Args:\n        filepath: The path to the file where the trace data should be saved.\n\n    Returns:\n        True if saving was successful, False otherwise.\n    \"\"\"\n    if not self.enabled:\n        self.logger.info(\"Tracing is disabled, cannot save trace.\")\n        return False\n\n    filepath = Path(filepath)\n    try:\n        filepath.parent.mkdir(parents=True, exist_ok=True)\n        trace_data_dict = self.get_trace_data()\n\n        with open(filepath, 'w', encoding='utf-8') as f:\n            json.dump(trace_data_dict, f, indent=2, cls=NumpyEncoder) # Use the encoder\n\n        self.logger.info(f\"Trace data saved successfully to {filepath}\")\n        return True\n    except TypeError as e: # This will be caught after the debug log in the encoder\n        self.logger.error(f\"Failed to serialize trace data to JSON for file '{filepath}': {e}\", exc_info=True)\n        return False\n    except Exception as e:\n        self.logger.error(f\"Error saving trace data to file '{filepath}': {str(e)}\", exc_info=True)\n        return False\n</code></pre>"},{"location":"api/utilities/#ai_ensemble_suite.utils.TraceCollector.start_session","title":"<code>start_session()</code>","text":"<p>Start a new tracing session, clearing previous data.</p> Source code in <code>src\\ai_ensemble_suite\\utils\\tracing.py</code> <pre><code>def start_session(self) -&gt; None:\n    \"\"\"Start a new tracing session, clearing previous data.\"\"\"\n    if not self.enabled: return\n    self.clear() # Clear previous state\n    self.session_id = f\"session_{int(time.time())}_{os.getpid()}\" # Generate new ID\n    self.start_time = time.time()\n    self.logger.debug(f\"Trace session started. ID: {self.session_id}\")\n</code></pre>"},{"location":"api/utilities/#ai_ensemble_suite.utils.TraceCollector.to_json","title":"<code>to_json()</code>","text":"<p>Convert the collected trace data to a JSON string.</p> <p>Returns:</p> Type Description <code>str</code> <p>A JSON string representation of the trace data, or a JSON error object.</p> Source code in <code>src\\ai_ensemble_suite\\utils\\tracing.py</code> <pre><code>def to_json(self) -&gt; str:\n    \"\"\"Convert the collected trace data to a JSON string.\n\n    Returns:\n        A JSON string representation of the trace data, or a JSON error object.\n    \"\"\"\n    if not self.enabled: return json.dumps({\"error\": \"Tracing disabled\"})\n    trace_data_dict = self.get_trace_data()\n    try:\n        return json.dumps(trace_data_dict, indent=2, cls=NumpyEncoder) # Use the encoder\n    except TypeError as e: # This will be caught after the debug log in the encoder\n        self.logger.error(f\"Error converting trace data to JSON string: {str(e)}\")\n        return json.dumps({\"error\": f\"Failed to serialize trace data: {str(e)}\"}, indent=2)\n    except Exception as e:\n        self.logger.error(f\"Unexpected error converting trace data to JSON string: {str(e)}\", exc_info=True)\n        return json.dumps({\"error\": f\"Unexpected error serializing trace data: {str(e)}\"}, indent=2)\n</code></pre>"},{"location":"api/utilities/#ai_ensemble_suite.utils.create_assistant_message","title":"<code>create_assistant_message(assistant_response)</code>","text":"<p>Create an assistant message for model prompting.</p> <p>Parameters:</p> Name Type Description Default <code>assistant_response</code> <code>str</code> <p>The assistant response content.</p> required <p>Returns:</p> Type Description <code>Dict[str, str]</code> <p>Dictionary containing the assistant message.</p> Source code in <code>src\\ai_ensemble_suite\\utils\\prompt_utils.py</code> <pre><code>def create_assistant_message(assistant_response: str) -&gt; Dict[str, str]:\n    \"\"\"Create an assistant message for model prompting.\n\n    Args:\n        assistant_response: The assistant response content.\n\n    Returns:\n        Dictionary containing the assistant message.\n    \"\"\"\n    return {\"role\": \"assistant\", \"content\": assistant_response}\n</code></pre>"},{"location":"api/utilities/#ai_ensemble_suite.utils.create_chat_prompt","title":"<code>create_chat_prompt(system_prompt, messages)</code>","text":"<p>Create a chat prompt from system prompt and messages.</p> <p>Parameters:</p> Name Type Description Default <code>system_prompt</code> <code>Optional[str]</code> <p>Optional system prompt.</p> required <code>messages</code> <code>List[Dict[str, str]]</code> <p>List of message dictionaries.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Formatted chat prompt as string.</p> Source code in <code>src\\ai_ensemble_suite\\utils\\prompt_utils.py</code> <pre><code>def create_chat_prompt(\n    system_prompt: Optional[str],\n    messages: List[Dict[str, str]],\n) -&gt; str:\n    \"\"\"Create a chat prompt from system prompt and messages.\n\n    Args:\n        system_prompt: Optional system prompt.\n        messages: List of message dictionaries.\n\n    Returns:\n        Formatted chat prompt as string.\n    \"\"\"\n    formatted_prompt = \"\"\n\n    if system_prompt:\n        formatted_prompt += f\"&lt;|system|&gt;\\n{system_prompt}\\n\\n\"\n\n    for message in messages:\n        role = message.get(\"role\", \"\").lower()\n        content = message.get(\"content\", \"\")\n\n        if role == \"system\":\n            formatted_prompt += f\"&lt;|system|&gt;\\n{content}\\n\\n\"\n        elif role == \"user\":\n            formatted_prompt += f\"&lt;|user|&gt;\\n{content}\\n\\n\"\n        elif role == \"assistant\":\n            formatted_prompt += f\"&lt;|assistant|&gt;\\n{content}\\n\\n\"\n\n    # Add a final assistant marker to indicate where the model should generate\n    formatted_prompt += \"&lt;|assistant|&gt;\\n\"\n\n    return formatted_prompt\n</code></pre>"},{"location":"api/utilities/#ai_ensemble_suite.utils.create_system_message","title":"<code>create_system_message(system_prompt)</code>","text":"<p>Create a system message for model prompting.</p> <p>Parameters:</p> Name Type Description Default <code>system_prompt</code> <code>str</code> <p>The system prompt content.</p> required <p>Returns:</p> Type Description <code>Dict[str, str]</code> <p>Dictionary containing the system message.</p> Source code in <code>src\\ai_ensemble_suite\\utils\\prompt_utils.py</code> <pre><code>def create_system_message(system_prompt: str) -&gt; Dict[str, str]:\n    \"\"\"Create a system message for model prompting.\n\n    Args:\n        system_prompt: The system prompt content.\n\n    Returns:\n        Dictionary containing the system message.\n    \"\"\"\n    return {\"role\": \"system\", \"content\": system_prompt}\n</code></pre>"},{"location":"api/utilities/#ai_ensemble_suite.utils.create_user_message","title":"<code>create_user_message(user_prompt)</code>","text":"<p>Create a user message for model prompting.</p> <p>Parameters:</p> Name Type Description Default <code>user_prompt</code> <code>str</code> <p>The user prompt content.</p> required <p>Returns:</p> Type Description <code>Dict[str, str]</code> <p>Dictionary containing the user message.</p> Source code in <code>src\\ai_ensemble_suite\\utils\\prompt_utils.py</code> <pre><code>def create_user_message(user_prompt: str) -&gt; Dict[str, str]:\n    \"\"\"Create a user message for model prompting.\n\n    Args:\n        user_prompt: The user prompt content.\n\n    Returns:\n        Dictionary containing the user message.\n    \"\"\"\n    return {\"role\": \"user\", \"content\": user_prompt}\n</code></pre>"},{"location":"api/utilities/#ai_ensemble_suite.utils.format_prompt","title":"<code>format_prompt(template, strict=False, **kwargs)</code>","text":"<p>Format a prompt template with the provided values.</p> <p>Supports both standard Python string formatting and custom markers.</p> <p>Parameters:</p> Name Type Description Default <code>template</code> <code>str</code> <p>The template string to format.</p> required <code>strict</code> <code>bool</code> <p>If True, raises an error for missing values.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Values to format the template with.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The formatted prompt.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>If strict=True and a required value is missing.</p> Source code in <code>src\\ai_ensemble_suite\\utils\\prompt_utils.py</code> <pre><code>def format_prompt(\n    template: str, strict: bool = False, **kwargs: Any\n) -&gt; str:\n    \"\"\"Format a prompt template with the provided values.\n\n    Supports both standard Python string formatting and custom markers.\n\n    Args:\n        template: The template string to format.\n        strict: If True, raises an error for missing values.\n        **kwargs: Values to format the template with.\n\n    Returns:\n        The formatted prompt.\n\n    Raises:\n        ValidationError: If strict=True and a required value is missing.\n    \"\"\"\n    if not kwargs and not strict:\n        return template\n\n    # First, identify all required keys in the template\n    required_keys = set()\n    for match in re.finditer(r\"\\{([a-zA-Z0-9_]+)\\}\", template):\n        required_keys.add(match.group(1))\n\n    # Check if all required keys are provided when strict is True\n    if strict:\n        missing_keys = required_keys - set(kwargs.keys())\n        if missing_keys:\n            missing_keys_str = \", \".join(missing_keys)\n            raise ValidationError(f\"Missing required values: {missing_keys_str}\")\n\n    # Format using Python's string formatting\n    try:\n        formatted_prompt = template.format(**kwargs)\n        return formatted_prompt\n    except KeyError as e:\n        if strict:\n            raise ValidationError(f\"Missing required value: {str(e)}\")\n        # Fall back to partial formatting when not strict\n        return template\n</code></pre>"},{"location":"api/utilities/#ai_ensemble_suite.utils.gather_with_concurrency","title":"<code>gather_with_concurrency(limit, *tasks, return_exceptions=False)</code>  <code>async</code>","text":"<p>Run asyncio tasks with a concurrency limit.</p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <code>int</code> <p>The maximum number of tasks to run concurrently.</p> required <code>*tasks</code> <code>Coroutine[Any, Any, T]</code> <p>The coroutine tasks to run.</p> <code>()</code> <code>return_exceptions</code> <code>bool</code> <p>If True, exceptions are returned as results                instead of raising them. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>List[Union[T, BaseException]]</code> <p>A list containing the results or exceptions from the tasks, preserving</p> <code>List[Union[T, BaseException]]</code> <p>the original order.</p> Source code in <code>src\\ai_ensemble_suite\\utils\\async_utils.py</code> <pre><code>async def gather_with_concurrency(\n    limit: int,\n    *tasks: Coroutine[Any, Any, T],\n    return_exceptions: bool = False\n) -&gt; List[Union[T, BaseException]]:\n    \"\"\"Run asyncio tasks with a concurrency limit.\n\n    Args:\n        limit: The maximum number of tasks to run concurrently.\n        *tasks: The coroutine tasks to run.\n        return_exceptions: If True, exceptions are returned as results\n                           instead of raising them. Defaults to False.\n\n    Returns:\n        A list containing the results or exceptions from the tasks, preserving\n        the original order.\n    \"\"\"\n    if limit &lt;= 0:\n        raise ValueError(\"Concurrency limit must be greater than 0\")\n    if not tasks:\n        return []\n\n    semaphore = asyncio.Semaphore(limit)\n    results: List[Optional[Union[T, BaseException]]] = [None] * len(tasks) # Preallocate results list\n\n    async def sem_task(task_index: int, task: Coroutine[Any, Any, T]) -&gt; None:\n        \"\"\"Wrapper to run a task under semaphore control.\"\"\"\n        nonlocal results\n        async with semaphore:\n            try:\n                result = await task\n                results[task_index] = result # Store result at correct index\n            except Exception as e:\n                if return_exceptions:\n                    logger.debug(f\"Task {task_index} failed with exception (captured): {e}\")\n                    results[task_index] = e # Store exception at correct index\n                else:\n                    logger.error(f\"Task {task_index} failed with exception (raising): {e}\", exc_info=True)\n                    raise # Re-raise if return_exceptions is False\n\n    # Create wrapper tasks\n    wrapper_tasks = [\n        sem_task(i, task) for i, task in enumerate(tasks)\n    ]\n\n    # Run all wrapper tasks\n    task_run_results = await asyncio.gather(*wrapper_tasks, return_exceptions=True)\n\n    # Process potential errors from asyncio.gather itself or re-raised exceptions\n    final_ordered_results: List[Union[T, BaseException]] = []\n    for i, run_result in enumerate(task_run_results):\n        original_result = results[i]\n        if isinstance(run_result, Exception):\n             if return_exceptions:\n                  if original_result is None:\n                      logger.warning(f\"gather_with_concurrency: Gather reported exception for task {i}, but no result stored: {run_result}\")\n                      final_ordered_results.append(run_result)\n                  elif isinstance(original_result, BaseException):\n                      final_ordered_results.append(original_result)\n                  else:\n                      logger.error(f\"gather_with_concurrency: Mismatch state for task {i}. Gather reported exc {run_result}, result was {original_result}\")\n                      final_ordered_results.append(run_result)\n             else:\n                 logger.error(f\"gather_with_concurrency: Task {i} raised an exception (return_exceptions=False): {run_result}\")\n                 raise run_result\n        elif original_result is not None:\n              final_ordered_results.append(original_result)\n        else:\n             logger.error(f\"gather_with_concurrency: Inconsistent state for task {i}. run_result={run_result}, original_result=None. Using default.\")\n             final_ordered_results.append(None)\n\n    if len(final_ordered_results) != len(tasks):\n        logger.error(f\"gather_with_concurrency: Result list length mismatch ({len(final_ordered_results)} vs {len(tasks)}). Padding with Nones.\")\n        final_ordered_results.extend([None] * (len(tasks) - len(final_ordered_results)))\n\n    return final_ordered_results\n</code></pre>"},{"location":"api/utilities/#ai_ensemble_suite.utils.run_in_threadpool","title":"<code>run_in_threadpool(func, *args, _executor=None, **kwargs)</code>  <code>async</code>","text":"<p>Run a potentially blocking function in a separate thread using an executor.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable[..., Any]</code> <p>The synchronous function or callable object to run.</p> required <code>*args</code> <code>Any</code> <p>Positional arguments for func.</p> <code>()</code> <code>_executor</code> <code>Optional[Executor]</code> <p>The concurrent.futures.Executor to use. If None, uses the       default executor of the current event loop.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments for func.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The result of the function call.</p> Source code in <code>src\\ai_ensemble_suite\\utils\\async_utils.py</code> <pre><code>async def run_in_threadpool(\n    func: Callable[..., Any],\n    *args: Any,\n     # Use keyword-only argument for the executor to avoid name clashes\n    _executor: Optional[concurrent.futures.Executor] = None,\n    **kwargs: Any\n) -&gt; Any:\n    \"\"\"Run a potentially blocking function in a separate thread using an executor.\n\n    Args:\n        func: The synchronous function or callable object to run.\n        *args: Positional arguments for func.\n        _executor: The concurrent.futures.Executor to use. If None, uses the\n                  default executor of the current event loop.\n        **kwargs: Keyword arguments for func.\n\n    Returns:\n        The result of the function call.\n\n    Raises:\n        Any exception raised by func.\n    \"\"\"\n    loop = asyncio.get_running_loop()\n    func_to_run = functools.partial(func, *args, **kwargs)\n\n    try:\n        result = await loop.run_in_executor(\n            _executor,\n            func_to_run\n        )\n        return result\n    except Exception as e:\n        # --- MODIFIED Error Logging ---\n        # Get a descriptive name for the callable, falling back gracefully\n        func_name = getattr(func, '__name__', None) # Try standard __name__\n        if func_name is None:\n            # If no __name__, try getting the type name\n            func_name = type(func).__name__\n        # --- END MODIFICATION ---\n\n        logger.error(f\"Error in threadpool execution of '{func_name}': {e}\", exc_info=True)\n        raise # Re-raise the exception to be handled by the caller\n</code></pre>"},{"location":"api/utilities/#ai_ensemble_suite.utils.truncate_text","title":"<code>truncate_text(text, max_length, truncation_marker='...')</code>","text":"<p>Truncate text to a maximum length.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to truncate.</p> required <code>max_length</code> <code>int</code> <p>Maximum length of the text.</p> required <code>truncation_marker</code> <code>str</code> <p>Marker to append to truncated text.</p> <code>'...'</code> <p>Returns:</p> Type Description <code>str</code> <p>Truncated text.</p> Source code in <code>src\\ai_ensemble_suite\\utils\\prompt_utils.py</code> <pre><code>def truncate_text(text: str, max_length: int, truncation_marker: str = \"...\") -&gt; str:\n    \"\"\"Truncate text to a maximum length.\n\n    Args:\n        text: The text to truncate.\n        max_length: Maximum length of the text.\n        truncation_marker: Marker to append to truncated text.\n\n    Returns:\n        Truncated text.\n    \"\"\"\n    if len(text) &lt;= max_length:\n        return text\n\n    # Account for truncation marker length\n    available_length = max_length - len(truncation_marker)\n    if available_length &lt;= 0:\n        return truncation_marker\n\n    return text[:available_length] + truncation_marker\n</code></pre>"},{"location":"development/contributing/","title":"Contributing","text":"<p>Guidelines for contributing to AI Ensemble Suite. </p>"},{"location":"development/testing/","title":"Testing","text":"<p>Information about testing AI Ensemble Suite. </p>"},{"location":"examples/aggregation/","title":"Aggregation Strategies","text":"<p>Examples of different aggregation strategies. </p>"},{"location":"examples/basic/","title":"Basic Usage","text":"<p>Basic examples of using AI Ensemble Suite. </p>"},{"location":"examples/collaboration/","title":"Collaboration Patterns","text":"<p>Examples of collaboration patterns. </p>"},{"location":"examples/production/","title":"Production Configurations","text":"<p>Examples of production configurations. </p>"}]}